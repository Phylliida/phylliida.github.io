{"summary": "The model analyzed and summarized technical texts on backpropagation and neural networks, focusing on clarity and mathematical precision. It distilled complex concepts into concise summaries that emphasized both theoretical understanding and practical application", "name": "Summarized backpropagation and neural network texts with mathematical clarity", "children": [{"summary": "The model analyzed and summarized technical texts on backpropagation and neural networks, focusing on clarity and mathematical precision. It distilled complex concepts into concise summaries that emphasized both theoretical understanding and practical application", "name": "Summarized backpropagation and neural network texts with mathematical clarity", "children": [{"summary": "The user provided detailed academic texts on back-propagation, and the response involved processing and understanding complex technical content related to neural networks and computational graphs. The task required formulating concise summaries without direct copying, focusing on accurate interpretation of technical explanations", "name": "Analyzed backpropagation texts to summarize technical concepts", "children": [], "numConvs": 6, "conversations": [{"facetValue": "The user provided a detailed explanation about feedforward neural networks, and I processed the information to generate a summary. The task involved understanding technical content and formulating a concise response without copying sentences directly.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed explanation about feedforward neural networks, and I processed the information to generate a summary. The task involved understanding technical content and formulating a concise response without copying sentences directly."}], "conversation": [{"role": "user", "content": "\"Deep feedforward networks, also often called feedforward neural networks,\nor multilayer perceptrons (MLPs), are the quintessential deep learning models.\nThe goal of a feedforward network is to approximate some function f \u2217. For example,\nfor a classifier, y = f \u2217(x) maps an input x to a category y. A feedforward network\ndefines a mapping y = f (x; \u03b8) and learns the value of the parameters \u03b8 that result\nin the best function approximation.\nThese models are called feedforward because information flows through the\nfunction being evaluated from x, through the intermediate computations used to\ndefine f , and finally to the output y. There are no feedback connections in which\noutputs of the model are fed back into itself. When feedforward neural networks\nare extended to include feedback connections, they are called recurrent neural\nnetworks, presented in chapter 10.\nFeedforward networks are of extreme importance to machine learning practitioners. They form the basis of many important commercial applications. For\nexample, the convolutional networks used for object recognition from photos are a\nspecialized kind of feedforward network. Feedforward networks are a conceptual\nstepping stone on the path to recurrent networks, which power many natural\nlanguage applications.\nFeedforward neural networks are called networks because they are typically\nrepresented by composing together many different functions. The model is associated with a directed acyclic graph describing how the functions are composed\ntogether. For example, we might have three functions f (1), f (2), and f (3) connected\nin a chain, to form f(x) = f(3)(f (2)(f(1) (x))). These chain structures are the most\ncommonly used structures of neural networks. In this case, f (1) is called the first\nlayer of the network, f (2) is called the second layer, and so on. The overall\n168\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nlength of the chain gives the depth of the model. It is from this terminology that\nthe name \u201cdeep learning\u201d arises. The final layer of a feedforward network is called\nthe output layer. During neural network training, we drive f(x) to match f\u2217(x).\nThe training data provides us with noisy, approximate examples of f \u2217(x) evaluated\nat different training points. Each example x is accompanied by a label y \u2248 f \u2217(x).\nThe training examples specify directly what the output layer must do at each point\nx; it must produce a value that is close to y. The behavior of the other layers is\nnot directly specified by the training data. The learning algorithm must decide\nhow to use those layers to produce the desired output, but the training data does\nnot say what each individual layer should do. Instead, the learning algorithm must\ndecide how to use these layers to best implement an approximation of f\u2217. Because\nthe training data does not show the desired output for each of these layers, these\nlayers are called hidden layers.\nFinally, these networks are called neural because they are loosely inspired by\nneuroscience. Each hidden layer of the network is typically vector-valued. The\ndimensionality of these hidden layers determines the width of the model. Each\nelement of the vector may be interpreted as playing a role analogous to a neuron.\nRather than thinking of the layer as representing a single vector-to-vector function,\nwe can also think of the layer as consisting of many units that act in parallel,\neach representing a vector-to-scalar function. Each unit resembles a neuron in\nthe sense that it receives input from many other units and computes its own\nactivation value. The idea of using many layers of vector-valued representation\nis drawn from neuroscience. The choice of the functions f(i)\n(x) used to compute\nthese representations is also loosely guided by neuroscientific observations about\nthe functions that biological neurons compute. However, modern neural network\nresearch is guided by many mathematical and engineering disciplines, and the\ngoal of neural networks is not to perfectly model the brain. It is best to think of\nfeedforward networks as function approximation machines that are designed to\nachieve statistical generalization, occasionally drawing some insights from what we\nknow about the brain, rather than as models of brain function.\nOne way to understand feedforward networks is to begin with linear models\nand consider how to overcome their limitations. Linear models, such as logistic\nregression and linear regression, are appealing because they may be fit efficiently\nand reliably, either in closed form or with convex optimization. Linear models also\nhave the obvious defect that the model capacity is limited to linear functions, so\nthe model cannot understand the interaction between any two input variables.\nTo extend linear models to represent nonlinear functions of x, we can apply\nthe linear model not to x itself but to a transformed input \u03c6(x), where \u03c6 is a\n169\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nnonlinear transformation. Equivalently, we can apply the kernel trick described in\nsection 5.7.2, to obtain a nonlinear learning algorithm based on implicitly applying\nthe \u03c6 mapping. We can think of \u03c6 as providing a set of features describing x, or\nas providing a new representation for x.\nThe question is then how to choose the mapping \u03c6.\n1. One option is to use a very generic \u03c6, such as the infinite-dimensional \u03c6 that\nis implicitly used by kernel machines based on the RBF kernel. If \u03c6(x) is\nof high enough dimension, we can always have enough capacity to fit the\ntraining set, but generalization to the test set often remains poor. Very\ngeneric feature mappings are usually based only on the principle of local\nsmoothness and do not encode enough prior information to solve advanced\nproblems.\n2. Another option is to manually engineer \u03c6. Until the advent of deep learning,\nthis was the dominant approach. This approach requires decades of human\neffort for each separate task, with practitioners specializing in different\ndomains such as speech recognition or computer vision, and with little\ntransfer between domains.\n3. The strategy of deep learning is to learn \u03c6. In this approach, we have a model\ny = f (x; \u03b8, w) = \u03c6(x; \u03b8)\ue03ew. We now have parameters \u03b8 that we use to learn\n\u03c6 from a broad class of functions, and parameters w that map from \u03c6(x) to\nthe desired output. This is an example of a deep feedforward network, with\n\u03c6 defining a hidden layer. This approach is the only one of the three that\ngives up on the convexity of the training problem, but the benefits outweigh\nthe harms. In this approach, we parametrize the representation as \u03c6(x; \u03b8)\nand use the optimization algorithm to find the \u03b8 that corresponds to a good\nrepresentation. If we wish, this approach can capture the benefit of the first\napproach by being highly generic\u2014we do so by using a very broad family\n\u03c6(x;\u03b8). This approach can also capture the benefit of the second approach.\nHuman practitioners can encode their knowledge to help generalization by\ndesigning families \u03c6(x; \u03b8) that they expect will perform well. The advantage\nis that the human designer only needs to find the right general function\nfamily rather than finding precisely the right function.\nThis general principle of improving models by learning features extends beyond\nthe feedforward networks described in this chapter. It is a recurring theme of deep\nlearning that applies to all of the kinds of models described throughout this book.\nFeedforward networks are the application of this principle to learning deterministic\n170\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmappings from x to y that lack feedback connections. Other models presented\nlater will apply these principles to learning stochastic mappings, learning functions\nwith feedback, and learning probability distributions over a single vector.\nWe begin this chapter with a simple example of a feedforward network. Next,\nwe address each of the design decisions needed to deploy a feedforward network.\nFirst, training a feedforward network requires making many of the same design\ndecisions as are necessary for a linear model: choosing the optimizer, the cost\nfunction, and the form of the output units. We review these basics of gradient-based\nlearning, then proceed to confront some of the design decisions that are unique\nto feedforward networks. Feedforward networks have introduced the concept of a\nhidden layer, and this requires us to choose the activation functions that will\nbe used to compute the hidden layer values. We must also design the architecture\nof the network, including how many layers the network should contain, how these\nlayers should be connected to each other, and how many units should be in\neach layer. Learning in deep neural networks requires computing the gradients\nof complicated functions. We present the back-propagation algorithm and its\nmodern generalizations, which can be used to efficiently compute these gradients.\nFinally, we close with some historical perspective.\n6.1 Example: Learning XOR\nTo make the idea of a feedforward network more concrete, we begin with an\nexample of a fully functioning feedforward network on a very simple task: learning\nthe XOR function.\nThe XOR function (\u201cexclusive or\u201d) is an operation on two binary values, x1\nand x2. When exactly one of these binary values is equal to 1, the XOR function\nreturns 1. Otherwise, it returns 0. The XOR function provides the target function\ny = f\u2217(x) that we want to learn. Our model provides a function y = f(x;\u03b8) and\nour learning algorithm will adapt the parameters \u03b8 to make f as similar as possible\nto f\u2217.\nIn this simple example, we will not be concerned with statistical generalization.\nWe want our network to perform correctly on the four points X = {[0, 0]\ue03e, [0,1]\ue03e,\n[1, 0]\ue03e, and [1, 1]\ue03e}. We will train the network on all four of these points. The\nonly challenge is to fit the training set.\nWe can treat this problem as a regression problem and use a mean squared\nerror loss function. We choose this loss function to simplify the math for this\nexample as much as possible. In practical applications, MSE is usually not an\n171\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nappropriate cost function for modeling binary data. More appropriate approaches\nare described in section 6.2.2.2.\nEvaluated on our whole training set, the MSE loss function is\nJ(\u03b8) = 1\n4\n\ue058\nx\u2208X\n(f\u2217(x) \u2212 f(x; \u03b8))2 . (6.1)\nNow we must choose the form of our model, f (x;\u03b8). Suppose that we choose\na linear model, with \u03b8 consisting of w and b. Our model is defined to be\nf(x; w, b) = x\ue03ew + b. (6.2)\nWe can minimize J(\u03b8) in closed form with respect to w and b using the normal\nequations.\nAfter solving the normal equations, we obtain w = 0 and b = 1\n2. The linear\nmodel simply outputs 0.5 everywhere. Why does this happen? Figure 6.1 shows\nhow a linear model is not able to represent the XOR function. One way to solve\nthis problem is to use a model that learns a different feature space in which a\nlinear model is able to represent the solution.\nSpecifically, we will introduce a very simple feedforward network with one\nhidden layer containing two hidden units. See figure 6.2 for an illustration of\nthis model. This feedforward network has a vector of hidden units h that are\ncomputed by a function f (1)(x; W, c). The values of these hidden units are then\nused as the input for a second layer. The second layer is the output layer of the\nnetwork. The output layer is still just a linear regression model, but now it is\napplied to h rather than to x. The network now contains two functions chained\ntogether: h = f(1)(x;W, c) and y = f (2)(h; w, b), with the complete model being\nf(x;W, c, w, b) = f(2)(f (1)(x)).\nWhat function should f (1) compute? Linear models have served us well so far,\nand it may be tempting to make f (1) be linear as well. Unfortunately, if f(1) were\nlinear, then the feedforward network as a whole would remain a linear function of\nits input. Ignoring the intercept terms for the moment, suppose f (1)(x) = W\ue03ex\nand f(2)(h) = h\ue03ew. Then f (x) = w\ue03eW \ue03ex. We could represent this function as\nf(x) = x\ue03e w\ue030 where w\ue030\n= Ww.\nClearly, we must use a nonlinear function to describe the features. Most neural\nnetworks do so using an affine transformation controlled by learned parameters,\nfollowed by a fixed, nonlinear function called an activation function. We use that\nstrategy here, by defining h = g(W\ue03ex + c), where W provides the weights of a\nlinear transformation and c the biases. Previously, to describe a linear regression\n172\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n0 1\nx1\n0\n1\nx2\nOriginal x space\n0 1 2\nh1\n0\n1\nh2\nLearned h space\nFigure 6.1: Solving the XOR problem by learning a representation. The bold numbers\nprinted on the plot indicate the value that the learned function must output at each point.\n(Left)A linear model applied directly to the original input cannot implement the XOR\nfunction. When x1 = 0, the model\u2019s output must increase as x2 increases. When x1 = 1,\nthe model\u2019s output must decrease as x2 increases. A linear model must apply a fixed\ncoefficient w2 to x2. The linear model therefore cannot use the value of x1 to change\nthe coefficient on x2 and cannot solve this problem. (Right)In the transformed space\nrepresented by the features extracted by a neural network, a linear model can now solve\nthe problem. In our example solution, the two points that must have output 1 have been\ncollapsed into a single point in feature space. In other words, the nonlinear features have\nmapped both x = [1, 0] \ue03e and x = [0,1]\ue03e to a single point in feature space, h = [1 ,0] \ue03e.\nThe linear model can now describe the function as increasing in h1 and decreasing in h2.\nIn this example, the motivation for learning the feature space is only to make the model\ncapacity greater so that it can fit the training set. In more realistic applications, learned\nrepresentations can also help the model to generalize.\n173\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ny\nh\nx\nW\nw\ny\nh1\nx1\nh2\nx2\nFigure 6.2: An example of a feedforward network, drawn in two different styles. Specifically,\nthis is the feedforward network we use to solve the XOR example. It has a single hidden\nlayer containing two units. (Left)In this style, we draw every unit as a node in the graph.\nThis style is very explicit and unambiguous but for networks larger than this example\nit can consume too much space. (Right)In this style, we draw a node in the graph for\neach entire vector representing a layer\u2019s activations. This style is much more compact.\nSometimes we annotate the edges in this graph with the name of the parameters that\ndescribe the relationship between two layers. Here, we indicate that a matrixW describes\nthe mapping from x to h, and a vector w describes the mapping from h to y. We\ntypically omit the intercept parameters associated with each layer when labeling this kind\nof drawing.\nmodel, we used a vector of weights and a scalar bias parameter to describe an\naffine transformation from an input vector to an output scalar. Now, we describe\nan affine transformation from a vector x to a vector h, so an entire vector of bias\nparameters is needed. The activation function g is typically chosen to be a function\nthat is applied element-wise, with hi = g(x\ue03eW:,i+c i). In modern neural networks,\nthe default recommendation is to use the rectified linear unit or ReLU (Jarrett\net al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a) defined by the activation\nfunction g(z) = max{0, z} depicted in figure 6.3.\nWe can now specify our complete network as\nf(x;W, c, w, b) = w\n\ue03e max{0,W \ue03ex + c} + b. (6.3)\nWe can now specify a solution to the XOR problem. Let\nW =\n\ue014 1 1\n1 1\n\ue015\n, (6.4)\nc =\n\ue014 0\n\u22121\n\ue015\n, (6.5)\n174\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n0\nz\n0\ng z( ) = max 0{ , z}\nFigure 6.3: The rectified linear activation function. This activation function is the default\nactivation function recommended for use with most feedforward neural networks. Applying\nthis function to the output of a linear transformation yields a nonlinear transformation.\nHowever, the function remains very close to linear, in the sense that is a piecewise linear\nfunction with two linear pieces. Because rectified linear units are nearly linear, they\npreserve many of the properties that make linear models easy to optimize with gradientbased methods. They also preserve many of the properties that make linear models\ngeneralize well. A common principle throughout computer science is that we can build\ncomplicated systems from minimal components. Much as a Turing machine\u2019s memory\nneeds only to be able to store 0 or 1 states, we can build a universal function approximator\nfrom rectified linear functions.\n175\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nw =\n\ue014 1\n\u22122\n\ue015\n, (6.6)\nand b = 0.\nWe can now walk through the way that the model processes a batch of inputs.\nLet X be the design matrix containing all four points in the binary input space,\nwith one example per row:\nX =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8f0\n0 0\n0 1\n1 0\n1 1\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fb. (6.7)\nThe first step in the neural network is to multiply the input matrix by the first\nlayer\u2019s weight matrix:\nXW =\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8f0\n0 0\n1 1\n1 1\n2 2\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fb . (6.8)\nNext, we add the bias vector c, to obtain\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8f0\n0 \u22121\n1 0\n1 0\n2 1\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fb. (6.9)\nIn this space, all of the examples lie along a line with slope 1. As we move along\nthis line, the output needs to begin at 0, then rise to 1, then drop back down to 0.\nA linear model cannot implement such a function. To finish computing the value\nof h for each example, we apply the rectified linear transformation:\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8f0\n0 0\n1 0\n1 0\n2 1\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fb . (6.10)\nThis transformation has changed the relationship between the examples. They no\nlonger lie on a single line. As shown in figure 6.1, they now lie in a space where a\nlinear model can solve the problem.\nWe finish by multiplying by the weight vector w:\n\uf8ee\n\uf8ef\n\uf8ef\n\uf8f0\n0\n1\n1\n0\n\uf8f9\n\uf8fa\n\uf8fa\n\uf8fb. (6.11)\n176\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nThe neural network has obtained the correct answer for every example in the batch.\nIn this example, we simply specified the solution, then showed that it obtained\nzero error. In a real situation, there might be billions of model parameters and\nbillions of training examples, so one cannot simply guess the solution as we did\nhere. Instead, a gradient-based optimization algorithm can find parameters that\nproduce very little error. The solution we described to the XOR problem is at a\nglobal minimum of the loss function, so gradient descent could converge to this\npoint. There are other equivalent solutions to the XOR problem that gradient\ndescent could also find. The convergence point of gradient descent depends on the\ninitial values of the parameters. In practice, gradient descent would usually not\nfind clean, easily understood, integer-valued solutions like the one we presented\n\"\nSummarize the above basic principle of feed forward networks with equations, try to use your own words, do not directly copy the sentence from the above paper."}, {"role": "assistant", "content": "Feedforward neural networks, also known as deep feedforward networks, multilayer perceptrons (MLPs), or just feedforward networks, are foundational models in deep learning designed to approximate a target function \\(f^*\\). For instance, in classification tasks, the network aims to map an input \\(x\\) to a category \\(y\\) through an approximation function \\(y = f(x; \\theta)\\), where \\(\\theta\\) represents the parameters of the model that are learned during training. The essence of feedforward networks is to create a function that processes input data \\(x\\) through several layers of computation to produce the output \\(y\\), without any cycles or feedback loops in the network architecture, which distinguishes them from recurrent neural networks (RNNs).\n\nThese networks are constructed by chaining together multiple functions, resembling a directed acyclic graph where each function represents a layer in the network. Typically, a feedforward network involves an input layer, several hidden layers, and an output layer. The term \"deep\" in deep learning refers to the presence of multiple hidden layers that enable the network to learn complex representations at different levels of abstraction.\n\nIn essence, a feedforward network operates by applying a series of transformations to the input data. The first step involves an affine transformation (a linear mapping followed by a translation) at each layer, which is then passed through a non-linear activation function. A commonly used activation function is the Rectified Linear Unit (ReLU), defined as \\(g(z) = \\max(0, z)\\). This combination of linear and non-linear operations allows the network to learn complex mappings from inputs to outputs.\n\nMathematically, the operation of a feedforward network with one hidden layer can be described by:\n- The computation of the hidden layer \\(h\\) as \\(h = g(W^T x + c)\\), where \\(W\\) represents the weight matrix of the connections between the input layer and the hidden layer, \\(c\\) is the bias vector for the hidden layer, and \\(g\\) is the activation function.\n- The output \\(y\\) is then computed as \\(y = f(h; w, b) = w^T h + b\\), where \\(w\\) and \\(b\\) are the weight vector and bias for the connections between the hidden layer and the output layer, respectively.\n\nThe learning in feedforward networks involves adjusting the parameters \\(W\\), \\(c\\), \\(w\\), and \\(b\\) to minimize a loss function, which measures the discrepancy between the network's predictions and the true target values. This process typically relies on gradient-based optimization techniques.\n\nA classic example of the utility of feedforward networks is in learning the XOR function, which cannot be learned by simple linear models due to its non-linearity. By introducing a hidden layer and non-linear activation functions, feedforward networks can effectively learn to separate the XOR data into classes that a linear output layer can then correctly classify. This demonstrates the network's ability to capture complex patterns through its learned representations in the hidden layers."}]}, {"facetValue": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding technical content related to neural networks and computational graphs.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding technical content related to neural networks and computational graphs."}], "conversation": [{"role": "user", "content": "\"6.5 Back-Propagation and Other Differentiation Algorithms\nWhen we use a feedforward neural network to accept an input x and produce an\noutput y\u02c6, information flows forward through the network. The inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces y\u02c6. This is called forward propagation. During training,\nforward propagation can continue onward until it produces a scalar cost J(\u03b8).\nThe back-propagation algorithm (Rumelhart et al., 1986a), often simply called\nbackprop, allows the information from the cost to then flow backwards through\nthe network, in order to compute the gradient.\nComputing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. The\nback-propagation algorithm does so using a simple and inexpensive procedure.\nThe term back-propagation is often misunderstood as meaning the whole\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nFurthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). Specifically, we will describe how to compute the gradient\n\u2207xf(x, y) for an arbitrary function f , wherex is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\nbut whose derivatives are not required. In learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters,\n\u2207\u03b8 J(\u03b8). Many machine learning tasks involve computing other derivatives, either\nas part of the learning process, or to analyze the learned model. The backpropagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. The\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the Jacobian of a function\nf with multiple outputs. We restrict our description here to the most commonly\nused case where f has a single output.\n204\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.1 Computational Graphs\nSo far we have discussed neural networks with a relatively informal graph language.\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\nMany ways of formalizing computation as graphs are possible.\nHere, we use each node in the graph to indicate a variable. The variable may\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\nTo formalize our graphs, we also need to introduce the idea of an operation.\nAn operation is a simple function of one or more variables. Our graph language\nis accompanied by a set of allowable operations. Functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\nWithout loss of generality, we define an operation to return only a single\noutput variable. This does not lose generality because the output variable can have\nmultiple entries, such as a vector. Software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\nIf a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y. We sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context.\nExamples of computational graphs are shown in figure 6.8.\n6.5.2 Chain Rule of Calculus\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. Back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient.\nLet x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. Suppose that y = g(x) and z = f (g(x)) = f (y). Then\nthe chain rule states that\ndz\ndx = dz\ndy\ndy\ndx . (6.44)\nWe can generalize this beyond the scalar case. Suppose that x \u2208 Rm, y \u2208 Rn,\n205\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx y\n(a)\n\u00d7\nx w\n(b)\nu(1)\ndot\nb\nu(2)\n+\ny\u02c6\n\u03c3\n(c)\nX W\nU (1)\nmatmul\nb\nU (2)\n+\nH\nrelu\nx w\n(d)\ny\u02c6\ndot\n\u03bb\nu(1)\nsqr\nu(2)\nsum\nu(3)\n\u00d7\nFigure 6.8: Examples of computational graphs. (a)The graph using the\u00d7 operation to\ncompute z = xy. (b)The graph for the logistic regression prediction y\u02c6 = \u03c3\n\ue000\nx\ue03ew + b\n\ue001\n.\nSome of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. We simply name the i-th such variable u(i)\n. (c)The\ncomputational graph for the expression H = max{0,XW + b}, which computes a design\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\nof inputs X . (d)Examples a\u2013c applied at most one operation to each variable, but it\nis possible to apply more than one operation. Here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. The\nweights are used to make both the prediction y\u02c6 and the weight decay penalty \u03bb\n\ue050\ni w2\ni .\n206\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ng maps from Rm to Rn, and f maps from Rn to R. If y = g(x) and z = f(y), then\n\u2202z\n\u2202xi\n= \ue058\nj\n\u2202z\n\u2202yj\n\u2202yj\n\u2202xi\n. (6.45)\nIn vector notation, this may be equivalently written as\n\u2207xz =\n\ue012 \u2202y\n\u2202x\n\ue013\ue03e\n\u2207y z, (6.46)\nwhere \u2202y\n\u2202x is the n \u00d7 m Jacobian matrix of g.\nFrom this we see that the gradient of a variable x can be obtained by multiplying\na Jacobian matrix \u2202y\n\u2202x by a gradient \u2207yz. The back-propagation algorithm consists\nof performing such a Jacobian-gradient product for each operation in the graph.\nUsually we do not apply the back-propagation algorithm merely to vectors,\nbut rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\nsame as back-propagation with vectors. The only difference is how the numbers\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor. In this rearranged view,\nback-propagation is still just multiplying Jacobians by gradients.\nTo denote the gradient of a value z with respect to a tensor X, we write \u2207Xz,\njust as if X were a vector. The indices into X now have multiple coordinates\u2014for\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\nby using a single variable i to represent the complete tuple of indices. For all\npossible index tuples i, (\u2207Xz)i gives \u2202z\n\u2202Xi\n. This is exactly the same as how for all\npossible integer indices i into a vector, (\u2207x z)i gives \u2202z\n\u2202xi\n. Using this notation, we\ncan write the chain rule as it applies to tensors. If Y = g(X) and z = f(Y), then\n\u2207X z =\ue058\nj\n(\u2207XYj ) \u2202z\n\u2202Yj\n. (6.47)\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\nUsing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. However, actually evaluating that expression in a computer\nintroduces some extra considerations.\nSpecifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. Any procedure that computes the gradient\n207\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwill need to choose whether to store these subexpressions or to recompute them\nseveral times. An example of how these repeated subexpressions arise is given in\nfigure 6.9. In some cases, computing the same subexpression twice would simply\nbe wasteful. For complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nIn other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\nWe first begin by a version of the back-propagation algorithm that specifies the\nactual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the\nassociated forward computation), in the order it will actually be done and according\nto the recursive application of chain rule. One could either directly perform these\ncomputations or view the description of the algorithm as a symbolic specification\nof the computational graph for computing the back-propagation. However, this\nformulation does not make explicit the manipulation and the construction of the\nsymbolic graph that performs the gradient computation. Such a formulation is\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\nnodes that contain arbitrary tensors.\nFirst consider a computational graph describing how to compute a single scalar\nu(n) (say the loss on a training example). This scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u(1) to u(ni). In\nother words we wish to compute \u2202u(n)\n\u2202u(i) for all i \u2208 {1, 2, . . . , ni}. In the application\nof back-propagation to computing gradients for gradient descent over parameters,\nu(n) will be the cost associated with an example or a minibatch, while u(1) to u(ni)\ncorrespond to the parameters of the model.\nWe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u(ni+1) and\ngoing up to u(n)\n. As defined in algorithm 6.1, each node u(i) is associated with an\noperation f(i) and is computed by evaluating the function\nu(i) = f(A(i)\n) (6.48)\nwhere A(i) is the set of all nodes that are parents of u(i)\n.\nThat algorithm specifies the forward propagation computation, which we could\nput in a graph G. In order to perform back-propagation, we can construct a\ncomputational graph that depends onG and adds to it an extra set of nodes. These\nform a subgraph B with one node per node of G. Computation in B proceeds in\nexactly the reverse of the order of computation in G, and each node of B computes\nthe derivative \u2202u(n)\n\u2202u(i) associated with the forward graph node u(i)\n. This is done\n208\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.1 A procedure that performs the computations mapping ni inputs\nu(1) to u(ni) to an output u(n)\n. This defines a computational graph where each node\ncomputes numerical value u(i) by applying a function f (i) to the set of arguments\nA(i) that comprises the values of previous nodes u(j)\n, j < i, with j \u2208 Pa(u(i)\n). The\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu(1) to u(ni ) . The output of the computational graph is read off the last (output)\nnode u(n)\n.\nfor i = 1, . . . , ni do\nu(i) \u2190 xi\nend for\nfor i = ni + 1, . . . , n do\nA(i) \u2190 {u(j) | j \u2208 Pa(u(i) )}\nu(i) \u2190 f (i)\n(A(i)\n)\nend for\nreturn u(n)\nusing the chain rule with respect to scalar output u(n)\n:\n\u2202u(n)\n\u2202u(j) = \ue058\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u(i)\n\u2202u(j) (6.49)\nas specified by algorithm 6.2. The subgraph B contains exactly one edge for each\nedge from node u(j) to node u(i) of G. The edge from u(j) to u(i) is associated with\nthe computation of \u2202u(i)\n\u2202u(j) . In addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u(i) that are children\nof u(j) and the vector containing the partial derivatives \u2202u(i)\n\u2202u(j) for the same children\nnodes u(i) . To summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in G, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations.\nThe back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. Specifically, it performs on the order\nof one Jacobian product per node in the graph. This can be seen from the fact\nthat backprop (algorithm 6.2) visits each edge from node u(j) to node u(i) of\nthe graph exactly once in order to obtain the associated partial derivative \u2202u(i)\n\u2202u(j) .\n209\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.2 Simplified version of the back-propagation algorithm for computing\nthe derivatives of u(n) with respect to the variables in the graph. This example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u(1), . . . , u(ni )\n.\nThis simplified version computes the derivatives of all nodes in the graph. The\ncomputational cost of this algorithm is proportional to the number of edges in\nthe graph, assuming that the partial derivative associated with each edge requires\na constant time. This is of the same order as the number of computations for\nthe forward propagation. Each \u2202u(i)\n\u2202u(j) is a function of the parents u(j) of u(i)\n, thus\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\nRun forward propagation (algorithm 6.1 for this example) to obtain the activations of the network\nInitialize grad_table, a data structure that will store the derivatives that have\nbeen computed. The entry grad_table[u(i)\n] will store the computed value of\n\u2202u(n)\n\u2202u(i) .\ngrad_table[u(n)\n] \u2190 1\nfor j = n \u2212 1 down to 1 do\nThe next line computes \u2202u(n)\n\u2202u(j) = \ue050\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u (i)\n\u2202u(j) using stored values:\ngrad_table[u(j) ] \u2190 \ue050\ni:j\u2208Pa(u(i)) grad_table[u(i) ] \u2202u(i)\n\u2202u(j)\nend for\nreturn {grad_table[u(i)\n] | i = 1, . . . , ni}\nBack-propagation thus avoids the exponential explosion in repeated subexpressions.\nHowever, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. We will revisit these ideas\nafter describing the back-propagation algorithm itself.\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\nTo clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer MLP.\nAlgorithm 6.3 first shows the forward propagation, which maps parameters to\nthe supervised loss L(y\u02c6, y) associated with a single (input,target) training example\n(x, y), with \u02c6y the output of the neural network when x is provided in input.\nAlgorithm 6.4 then shows the corresponding computation to be done for\n210\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nFigure 6.9: A computational graph that results in repeated subexpressions when computing\nthe gradient. Let w \u2208 R be the input to the graph. We use the same function f : R \u2192 R\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f( y).\nTo compute \u2202z\n\u2202w , we apply equation 6.44 and obtain:\n\u2202z\n\u2202w (6.50)\n= \u2202z\n\u2202y\n\u2202y\n\u2202x\n\u2202x\n\u2202w (6.51)\n=f\ue030\n(y)f\ue030\n(x)f\ue030\n(w) (6.52)\n=f\ue030\n(f(f(w)))f \ue030\n(f(w))f\ue030\n(w) (6.53)\nEquation 6.52 suggests an implementation in which we compute the value off (w) only\nonce and store it in the variable x. This is the approach taken by the back-propagation\nalgorithm. An alternative approach is suggested by equation 6.53, where the subexpression\nf(w) appears more than once. In the alternative approach, f(w) is recomputed each time\nit is needed. When the memory required to store the value of these expressions is low, the\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\nuseful when memory is limited.\n211\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napplying the back-propagation algorithm to this graph.\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\nstraightforward to understand. However, they are specialized to one specific\nproblem.\nModern software implementations are based on the generalized form of backpropagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\nAlgorithm 6.3 Forward propagation through a typical deep neural network and\nthe computation of the cost function. The loss L(y\u02c6, y) depends on the output\ny\u02c6 and on the target y (see section 6.2.1.1 for examples of loss functions). To\nobtain the total cost J, the loss may be added to a regularizer \u2126(\u03b8), where \u03b8\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\ncompute gradients of J with respect to parameters W and b. For simplicity, this\ndemonstration uses only a single input example x. Practical applications should\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\nRequire: Network depth, l\nRequire: W(i)\n,i \u2208 {1, . . . , l}, the weight matrices of the model\nRequire: b(i) ,i \u2208 {1, . . . , l}, the bias parameters of the model\nRequire: x, the input to process\nRequire: y, the target output\nh(0) = x\nfor k = 1, . . . , l do\na(k) = b(k) + W(k)h(k\u22121)\nh(k) = f(a(k)\n)\nend for\ny\u02c6 = h(l)\nJ = L(y\u02c6, y) + \u03bb\u2126(\u03b8)\n6.5.5 Symbol-to-Symbol Derivatives\nAlgebraic expressions and computational graphs both operate on symbols, or\nvariables that do not have specific values. These algebraic and graph-based\nrepresentations are called symbolic representations. When we actually use or\ntrain a neural network, we must assign specific values to these symbols. We\nreplace a symbolic input to the network x with a specific numeric value, such as\n[1.2, 3.765, \u22121.8]\ue03e.\n212\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation\nyields the gradients on the activations a(k) for each layer k, starting from the\noutput layer and going backwards to the first hidden layer. From these gradients,\nwhich can be interpreted as an indication of how each layer\u2019s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. The\ngradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods.\nAfter the forward computation, compute the gradient on the output layer:\ng \u2190 \u2207y\u02c6J = \u2207y\u02c6L(y\u02c6, y)\nfor k = l, l \u2212 1, . . . , 1 do\nConvert the gradient on the layer\u2019s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\ng \u2190 \u2207a(k)J = g \ue00c f\ue030\n(a(k)\n)\nCompute gradients on weights and biases (including the regularization term,\nwhere needed):\n\u2207b(k)J = g + \u03bb\u2207b(k)\u2126(\u03b8)\n\u2207W (k)J = g h(k\u22121)\ue03e + \u03bb\u2207W (k)\u2126(\u03b8)\nPropagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations:\ng \u2190 \u2207h(k\u22121) J = W(k)\ue03e g\nend for\n213\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nz\nx\ny\nw\nf\nf\nf\ndz\ndy\nf \ue021\ndy\ndx\nf \ue021\ndz\ndx\n\u00d7\ndx\ndw\nf \ue021\ndz\ndw\n\u00d7\nFigure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\nto compute these derivatives. A generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\nrepresenting z = f (f(f (w))). (Right)We run the back-propagation algorithm, instructing\nit to construct the graph for the expression corresponding to dz\ndw . In this example, we do\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\nSome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. We call this approach \u201csymbolto-number\u201d differentiation. This is the approach used by libraries such as Torch\n(Collobert et al., 2011b) and Caffe (Jia, 2013).\nAnother approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. This\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\nis illustrated in figure 6.10. The primary advantage of this approach is that\nthe derivatives are described in the same language as the original expression.\nBecause the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. Computation of higher-order derivatives is described in section 6.5.10.\nWe will use the latter approach and describe the back-propagation algorithm in\n214\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nterms of constructing a computational graph for the derivatives. Any subset of the\ngraph may then be evaluated using specific numerical values at a later time. This\nallows us to avoid specifying exactly when each operation should be computed.\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents\u2019 values are available.\nThe description of the symbol-to-symbol based approach subsumes the symbolto-number approach. The symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\napproach does not expose the graph.\n6.5.6 General Back-Propagation\nThe back-propagation algorithm is very simple. To compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by dz\ndz = 1. We can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the Jacobian of the operation that produced z. We continue\nmultiplying by Jacobians traveling backwards through the graph in this way until\nwe reach x. For any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\nMore formally, each node in the graph G corresponds to a variable. To achieve\nmaximum generality, we describe this variable as being a tensor V. Tensor can\nin general have any number of dimensions. They subsume scalars, vectors, and\nmatrices.\nWe assume that each variable V is associated with the following subroutines:\n\u2022 get_operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,\nthere may be a Python or C++ class representing the matrix multiplication\noperation, and the get_operation function. Suppose we have a variable that\nis created by matrix multiplication, C = AB. Then get_operation(V)\nreturns a pointer to an instance of the corresponding C++ class.\n\u2022 get_consumers(V, G): This returns the list of variables that are children of\nV in the computational graph G.\n\u2022 get_inputs(V, G): This returns the list of variables that are parents of V\nin the computational graph G.\n215\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nEach operation op is also associated with a bprop operation. This bprop\noperation can compute a Jacobian-vector product as described by equation 6.47.\nThis is how the back-propagation algorithm is able to achieve great generality.\nEach operation is responsible for knowing how to back-propagate through the\nedges in the graph that it participates in. For example, we might use a matrix\nmultiplication operation to create a variable C = AB. Suppose that the gradient\nof a scalar z with respect to C is given by G. The matrix multiplication operation\nis responsible for defining two back-propagation rules, one for each of its input\narguments. If we call the bprop method to request the gradient with respect to\nA given that the gradient on the output is G, then the bprop method of the\nmatrix multiplication operation must state that the gradient with respect to A\nis given by GB\ue03e. Likewise, if we call the bprop method to request the gradient\nwith respect to B, then the matrix operation is responsible for implementing the\nbprop method and specifying that the desired gradient is given by A\ue03eG. The\nback-propagation algorithm itself does not need to know any differentiation rules. It\nonly needs to call each operation\u2019s bprop rules with the right arguments. Formally,\nop.bprop(inputs, X, G) must return\n\ue058\ni\n(\u2207 Xop.f(inputs)i) Gi, (6.54)\nwhich is just an implementation of the chain rule as expressed in equation 6.47.\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements,X is the input whose gradient\nwe wish to compute, and G is the gradient on the output of the operation.\nThe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. For example, if the mul operator is passed\ntwo copies of x to compute x2, the op.bprop method should still return x as the\nderivative with respect to both inputs. The back-propagation algorithm will later\nadd both of these arguments together to obtain 2x, which is the correct total\nderivative on x.\nSoftware implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually.\nThe back-propagation algorithm is formally described in algorithm 6.5.\n216\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\nportion does simple setup and cleanup work. Most of the important work happens\nin the build_grad subroutine of algorithm 6.6 .\nRequire: T, the target set of variables whose gradients must be computed.\nRequire: G, the computational graph\nRequire: z, the variable to be differentiated\nLet G\ue030 be G pruned to contain only nodes that are ancestors of z and descendents\nof nodes in T.\nInitialize grad_table, a data structure associating tensors to their gradients\ngrad_table[z] \u2190 1\nfor V in T do\nbuild_grad(V, G, G\ue030 , grad_table)\nend for\nReturn grad_table restricted to T\nIn section 6.5.2, we explained that back-propagation was developed in order to\navoid computing the same subexpression in the chain rule multiple times. The naive\nalgorithm could have exponential runtime due to these repeated subexpressions.\nNow that we have specified the back-propagation algorithm, we can understand its\ncomputational cost. If we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed. Keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\nmultiplication as a single operation). Computing a gradient in a graph with n nodes\nwill never execute more than O(n2) operations or store the output of more than\nO(n2) operations. Here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. For example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. We can see that computing the gradient requires as\nmost O(n2) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). The back-propagation algorithm\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\nedge in the original graph. Because the computational graph is a directed acyclic\ngraph it has at most O(n2 ) edges. For the kinds of graphs that are commonly used\nin practice, the situation is even better. Most neural network cost functions are\n217\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.6 The inner loop subroutine build_grad(V, G, G\ue030\n, grad_table) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm 6.5.\nRequire: V, the variable whose gradient should be added to G and grad_table.\nRequire: G, the graph to modify.\nRequire: G\ue030\n, the restriction of G to nodes that participate in the gradient.\nRequire: grad_table, a data structure mapping nodes to their gradients\nif V is in grad_table then\nReturn grad_table[V]\nend if\ni \u2190 1\nfor C in get_consumers(V, G\ue030\n) do\nop \u2190 get_operation(C)\nD \u2190 build_grad(C, G,G\ue030\n, grad_table)\nG(i) \u2190 op.bprop(get_inputs(C, G\ue030\n), V, D)\ni \u2190 i + 1\nend for\nG \u2190\ue050\ni G(i)\ngrad_table[V] = G\nInsert G and the operations creating it into G\nReturn G\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. This potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (equation 6.49) non-recursively:\n\u2202u(n)\n\u2202u(j) = \ue058\npath (u(\u03c01),u(\u03c02) ,...,u(\u03c0t) ),\nfrom \u03c01=j to \u03c0t=n\n\ue059\nt\nk=2\n\u2202u(\u03c0k)\n\u2202u(\u03c0k\u22121 )\n. (6.55)\nSince the number of paths from node j to node n can grow exponentially in the\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. This large cost would be incurred because the same computation for\n\u2202u(i)\n\u2202u(j) would be redone many times. To avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results \u2202u(n)\n\u2202u(i) . Each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. By filling in these table entries in order,\n218\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nback-propagation avoids repeating many common subexpressions. This table-filling\nstrategy is sometimes called dynamic programming.\n6.5.7 Example: Back-Propagation for MLP Training\nAs an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\nHere we develop a very simple multilayer perception with a single hidden\nlayer. To train this model, we will use minibatch stochastic gradient descent.\nThe back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. Specifically, we use a minibatch of examples from the training\nset formatted as a design matrix X and a vector of associated class labels y.\nThe network computes a layer of hidden features H = max{0, XW(1)}. To\nsimplify the presentation we do not use biases in this model. We assume that our\ngraph language includes a relu operation that can compute max{0, Z} elementwise. The predictions of the unnormalized log probabilities over classes are then\ngiven by HW (2). We assume that our graph language includes a cross_entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. The resulting crossentropy defines the cost JMLE. Minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. However, to make this example more realistic,\nwe also include a regularization term. The total cost\nJ = JMLE + \u03bb\n\uf8eb\n\uf8ed\ue058\ni,j\n\ue010\nW (1)\ni,j \ue0112\n+ \ue058\ni,j\n\ue010\nW(2)\ni,j \ue0112\n\uf8f6\n\uf8f8 (6.56)\nconsists of the cross-entropy and a weight decay term with coefficient \u03bb. The\ncomputational graph is illustrated in figure 6.11.\nThe computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. This demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually.\nWe can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in figure 6.11. To train, we wish\nto compute both \u2207W (1)J and \u2207W (2)J. There are two different paths leading\nbackward from J to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\nalways contribute 2\u03bbW(i) to the gradient on W(i) .\n219\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nX W (1)\nU (1)\nmatmul\nH\nrelu\nU (3)\nsqr u(4)\nsum\nW (2) u(7) \u03bb\nU (2)\nmatmul\ny\nJMLE\ncross_entropy\nU (5)\nsqr u(6)\nsum\nu(8)\nJ\n+\n\u00d7\n+\nFigure 6.11: The computational graph used to compute the cost used to train our example\nof a single-layer MLP using the cross-entropy loss and weight decay.\nThe other path through the cross-entropy cost is slightly more complicated.\nLet G be the gradient on the unnormalized log probabilities U(2) provided by\nthe cross_entropy operation. The back-propagation algorithm now needs to\nexplore two different branches. On the shorter branch, it adds H \ue03eG to the\ngradient on W(2) , using the back-propagation rule for the second argument to\nthe matrix multiplication operation. The other branch corresponds to the longer\nchain descending further along the network. First, the back-propagation algorithm\ncomputes \u2207HJ = GW(2)\ue03e using the back-propagation rule for the first argument\nto the matrix multiplication operation. Next, the relu operation uses its backpropagation rule to zero out components of the gradient corresponding to entries\nof U(1) that were less than 0. Let the result be called G\ue030 . The last step of the\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the matmul operation to add X\ue03eG\ue030 to the gradient on W(1).\nAfter these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters.\nFor the MLP, the computational cost is dominated by the cost of matrix\nmultiplication. During the forward propagation stage, we multiply by each weight\n220\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost. The main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\nThis value is stored from the time it is computed until the backward pass has\nreturned to the same point. The memory cost is thus O(mnh), where m is the\nnumber of examples in the minibatch and nh is the number of hidden units.\n\"\nSummarize the above basic principle of back-propagation for neural network training, try to use your own words, do not directly copy the sentence from the above paper.\n"}, {"role": "assistant", "content": "Back-propagation is a fundamental algorithm in the training of feedforward neural networks. It operates by first sending input data (x) through the network in a process called forward propagation, where it travels through hidden layers (if any) to produce an output (y\u0302). This output is then used to compute a cost (J(\u03b8)), a measure of the difference between the predicted output and the actual output.\n\nThe essence of back-propagation lies in its ability to calculate the gradient of the cost function with respect to each parameter (weight and bias) in the network. This is achieved by allowing the error information from the cost to flow backward through the network, thus the name \"back-propagation.\" The backward journey starts by assigning an initial gradient of 1 to the cost itself, since the derivative of the cost with respect to itself is 1.\n\nTo compute these gradients efficiently, back-propagation applies the chain rule of calculus. In essence, for each parameter, it multiplies the gradients flowing back to it from the subsequent layer by the gradient of the cost with respect to its effect on the next layer's input. This process is repeated layer by layer, moving from the output layer back to the input layer.\n\nA crucial aspect of back-propagation is its efficiency in computing these gradients. Instead of calculating redundant operations multiple times, it uses a structured approach to ensure that each partial derivative is computed just once and then re-used, making the algorithm scalable and practical for networks with a large number of parameters.\n\nBack-propagation itself only computes the gradients; it doesn't perform the actual updating of the parameters. Typically, another algorithm, often stochastic gradient descent (SGD), uses the computed gradients to update the parameters of the model in such a way that the cost is reduced.\n\nBy iteratively applying back-propagation and the parameter update step (like in SGD), the network learns to minimize the cost function, thereby improving its prediction accuracy. The algorithm can be applied not just to multi-layer neural networks but to computing derivatives of various functions, making back-propagation a highly versatile technique in machine learning tasks."}]}, {"facetValue": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs."}], "conversation": [{"role": "user", "content": "\u201c6.5 Back-Propagation and Other Differentiation Algorithms\nWhen we use a feedforward neural network to accept an input x and produce an\noutput y\u02c6, information flows forward through the network. The inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces y\u02c6. This is called forward propagation. During training,\nforward propagation can continue onward until it produces a scalar cost J(\u03b8).\nThe back-propagation algorithm (Rumelhart et al., 1986a), often simply called\nbackprop, allows the information from the cost to then flow backwards through\nthe network, in order to compute the gradient.\nComputing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. The\nback-propagation algorithm does so using a simple and inexpensive procedure.\nThe term back-propagation is often misunderstood as meaning the whole\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nFurthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). Specifically, we will describe how to compute the gradient\n\u2207xf(x, y) for an arbitrary function f , wherex is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\nbut whose derivatives are not required. In learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters,\n\u2207\u03b8 J(\u03b8). Many machine learning tasks involve computing other derivatives, either\nas part of the learning process, or to analyze the learned model. The backpropagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. The\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the Jacobian of a function\nf with multiple outputs. We restrict our description here to the most commonly\nused case where f has a single output.\n204\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.1 Computational Graphs\nSo far we have discussed neural networks with a relatively informal graph language.\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\nMany ways of formalizing computation as graphs are possible.\nHere, we use each node in the graph to indicate a variable. The variable may\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\nTo formalize our graphs, we also need to introduce the idea of an operation.\nAn operation is a simple function of one or more variables. Our graph language\nis accompanied by a set of allowable operations. Functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\nWithout loss of generality, we define an operation to return only a single\noutput variable. This does not lose generality because the output variable can have\nmultiple entries, such as a vector. Software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\nIf a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y. We sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context.\nExamples of computational graphs are shown in figure 6.8.\n6.5.2 Chain Rule of Calculus\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. Back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient.\nLet x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. Suppose that y = g(x) and z = f (g(x)) = f (y). Then\nthe chain rule states that\ndz\ndx = dz\ndy\ndy\ndx . (6.44)\nWe can generalize this beyond the scalar case. Suppose that x \u2208 Rm, y \u2208 Rn,\n205\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx y\n(a)\n\u00d7\nx w\n(b)\nu(1)\ndot\nb\nu(2)\n+\ny\u02c6\n\u03c3\n\u00a9\nX W\nU (1)\nmatmul\nb\nU (2)\n+\nH\nrelu\nx w\n(d)\ny\u02c6\ndot\n\u03bb\nu(1)\nsqr\nu(2)\nsum\nu(3)\n\u00d7\nFigure 6.8: Examples of computational graphs. (a)The graph using the\u00d7 operation to\ncompute z = xy. (b)The graph for the logistic regression prediction y\u02c6 = \u03c3\n\ue000\nx\ue03ew + b\n\ue001\n.\nSome of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. We simply name the i-th such variable u(i)\n. \u00a9The\ncomputational graph for the expression H = max{0,XW + b}, which computes a design\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\nof inputs X . (d)Examples a\u2013c applied at most one operation to each variable, but it\nis possible to apply more than one operation. Here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. The\nweights are used to make both the prediction y\u02c6 and the weight decay penalty \u03bb\n\ue050\ni w2\ni .\n206\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ng maps from Rm to Rn, and f maps from Rn to R. If y = g(x) and z = f(y), then\n\u2202z\n\u2202xi\n= \ue058\nj\n\u2202z\n\u2202yj\n\u2202yj\n\u2202xi\n. (6.45)\nIn vector notation, this may be equivalently written as\n\u2207xz =\n\ue012 \u2202y\n\u2202x\n\ue013\ue03e\n\u2207y z, (6.46)\nwhere \u2202y\n\u2202x is the n \u00d7 m Jacobian matrix of g.\nFrom this we see that the gradient of a variable x can be obtained by multiplying\na Jacobian matrix \u2202y\n\u2202x by a gradient \u2207yz. The back-propagation algorithm consists\nof performing such a Jacobian-gradient product for each operation in the graph.\nUsually we do not apply the back-propagation algorithm merely to vectors,\nbut rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\nsame as back-propagation with vectors. The only difference is how the numbers\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor. In this rearranged view,\nback-propagation is still just multiplying Jacobians by gradients.\nTo denote the gradient of a value z with respect to a tensor X, we write \u2207Xz,\njust as if X were a vector. The indices into X now have multiple coordinates\u2014for\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\nby using a single variable i to represent the complete tuple of indices. For all\npossible index tuples i, (\u2207Xz)i gives \u2202z\n\u2202Xi\n. This is exactly the same as how for all\npossible integer indices i into a vector, (\u2207x z)i gives \u2202z\n\u2202xi\n. Using this notation, we\ncan write the chain rule as it applies to tensors. If Y = g(X) and z = f(Y), then\n\u2207X z =\ue058\nj\n(\u2207XYj ) \u2202z\n\u2202Yj\n. (6.47)\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\nUsing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. However, actually evaluating that expression in a computer\nintroduces some extra considerations.\nSpecifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. Any procedure that computes the gradient\n207\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwill need to choose whether to store these subexpressions or to recompute them\nseveral times. An example of how these repeated subexpressions arise is given in\nfigure 6.9. In some cases, computing the same subexpression twice would simply\nbe wasteful. For complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nIn other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\nWe first begin by a version of the back-propagation algorithm that specifies the\nactual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the\nassociated forward computation), in the order it will actually be done and according\nto the recursive application of chain rule. One could either directly perform these\ncomputations or view the description of the algorithm as a symbolic specification\nof the computational graph for computing the back-propagation. However, this\nformulation does not make explicit the manipulation and the construction of the\nsymbolic graph that performs the gradient computation. Such a formulation is\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\nnodes that contain arbitrary tensors.\nFirst consider a computational graph describing how to compute a single scalar\nu(n) (say the loss on a training example). This scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u(1) to u(ni). In\nother words we wish to compute \u2202u(n)\n\u2202u(i) for all i \u2208 {1, 2, . . . , ni}. In the application\nof back-propagation to computing gradients for gradient descent over parameters,\nu(n) will be the cost associated with an example or a minibatch, while u(1) to u(ni)\ncorrespond to the parameters of the model.\nWe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u(ni+1) and\ngoing up to u(n)\n. As defined in algorithm 6.1, each node u(i) is associated with an\noperation f(i) and is computed by evaluating the function\nu(i) = f(A(i)\n) (6.48)\nwhere A(i) is the set of all nodes that are parents of u(i)\n.\nThat algorithm specifies the forward propagation computation, which we could\nput in a graph G. In order to perform back-propagation, we can construct a\ncomputational graph that depends onG and adds to it an extra set of nodes. These\nform a subgraph B with one node per node of G. Computation in B proceeds in\nexactly the reverse of the order of computation in G, and each node of B computes\nthe derivative \u2202u(n)\n\u2202u(i) associated with the forward graph node u(i)\n. This is done\n208\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.1 A procedure that performs the computations mapping ni inputs\nu(1) to u(ni) to an output u(n)\n. This defines a computational graph where each node\ncomputes numerical value u(i) by applying a function f (i) to the set of arguments\nA(i) that comprises the values of previous nodes u(j)\n, j < i, with j \u2208 Pa(u(i)\n). The\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu(1) to u(ni ) . The output of the computational graph is read off the last (output)\nnode u(n)\n.\nfor i = 1, . . . , ni do\nu(i) \u2190 xi\nend for\nfor i = ni + 1, . . . , n do\nA(i) \u2190 {u(j) | j \u2208 Pa(u(i) )}\nu(i) \u2190 f (i)\n(A(i)\n)\nend for\nreturn u(n)\nusing the chain rule with respect to scalar output u(n)\n:\n\u2202u(n)\n\u2202u(j) = \ue058\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u(i)\n\u2202u(j) (6.49)\nas specified by algorithm 6.2. The subgraph B contains exactly one edge for each\nedge from node u(j) to node u(i) of G. The edge from u(j) to u(i) is associated with\nthe computation of \u2202u(i)\n\u2202u(j) . In addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u(i) that are children\nof u(j) and the vector containing the partial derivatives \u2202u(i)\n\u2202u(j) for the same children\nnodes u(i) . To summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in G, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations.\nThe back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. Specifically, it performs on the order\nof one Jacobian product per node in the graph. This can be seen from the fact\nthat backprop (algorithm 6.2) visits each edge from node u(j) to node u(i) of\nthe graph exactly once in order to obtain the associated partial derivative \u2202u(i)\n\u2202u(j) .\n209\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.2 Simplified version of the back-propagation algorithm for computing\nthe derivatives of u(n) with respect to the variables in the graph. This example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u(1), . . . , u(ni )\n.\nThis simplified version computes the derivatives of all nodes in the graph. The\ncomputational cost of this algorithm is proportional to the number of edges in\nthe graph, assuming that the partial derivative associated with each edge requires\na constant time. This is of the same order as the number of computations for\nthe forward propagation. Each \u2202u(i)\n\u2202u(j) is a function of the parents u(j) of u(i)\n, thus\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\nRun forward propagation (algorithm 6.1 for this example) to obtain the activations of the network\nInitialize grad_table, a data structure that will store the derivatives that have\nbeen computed. The entry grad_table[u(i)\n] will store the computed value of\n\u2202u(n)\n\u2202u(i) .\ngrad_table[u(n)\n] \u2190 1\nfor j = n \u2212 1 down to 1 do\nThe next line computes \u2202u(n)\n\u2202u(j) = \ue050\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u (i)\n\u2202u(j) using stored values:\ngrad_table[u(j) ] \u2190 \ue050\ni:j\u2208Pa(u(i)) grad_table[u(i) ] \u2202u(i)\n\u2202u(j)\nend for\nreturn {grad_table[u(i)\n] | i = 1, . . . , ni}\nBack-propagation thus avoids the exponential explosion in repeated subexpressions.\nHowever, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. We will revisit these ideas\nafter describing the back-propagation algorithm itself.\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\nTo clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer MLP.\nAlgorithm 6.3 first shows the forward propagation, which maps parameters to\nthe supervised loss L(y\u02c6, y) associated with a single (input,target) training example\n(x, y), with \u02c6y the output of the neural network when x is provided in input.\nAlgorithm 6.4 then shows the corresponding computation to be done for\n210\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nFigure 6.9: A computational graph that results in repeated subexpressions when computing\nthe gradient. Let w \u2208 R be the input to the graph. We use the same function f : R \u2192 R\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f( y).\nTo compute \u2202z\n\u2202w , we apply equation 6.44 and obtain:\n\u2202z\n\u2202w (6.50)\n= \u2202z\n\u2202y\n\u2202y\n\u2202x\n\u2202x\n\u2202w (6.51)\n=f\ue030\n(y)f\ue030\n(x)f\ue030\n(w) (6.52)\n=f\ue030\n(f(f(w)))f \ue030\n(f(w))f\ue030\n(w) (6.53)\nEquation 6.52 suggests an implementation in which we compute the value off (w) only\nonce and store it in the variable x. This is the approach taken by the back-propagation\nalgorithm. An alternative approach is suggested by equation 6.53, where the subexpression\nf(w) appears more than once. In the alternative approach, f(w) is recomputed each time\nit is needed. When the memory required to store the value of these expressions is low, the\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\nuseful when memory is limited.\n211\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napplying the back-propagation algorithm to this graph.\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\nstraightforward to understand. However, they are specialized to one specific\nproblem.\nModern software implementations are based on the generalized form of backpropagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\nAlgorithm 6.3 Forward propagation through a typical deep neural network and\nthe computation of the cost function. The loss L(y\u02c6, y) depends on the output\ny\u02c6 and on the target y (see section 6.2.1.1 for examples of loss functions). To\nobtain the total cost J, the loss may be added to a regularizer \u2126(\u03b8), where \u03b8\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\ncompute gradients of J with respect to parameters W and b. For simplicity, this\ndemonstration uses only a single input example x. Practical applications should\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\nRequire: Network depth, l\nRequire: W(i)\n,i \u2208 {1, . . . , l}, the weight matrices of the model\nRequire: b(i) ,i \u2208 {1, . . . , l}, the bias parameters of the model\nRequire: x, the input to process\nRequire: y, the target output\nh(0) = x\nfor k = 1, . . . , l do\na(k) = b(k) + W(k)h(k\u22121)\nh(k) = f(a(k)\n)\nend for\ny\u02c6 = h(l)\nJ = L(y\u02c6, y) + \u03bb\u2126(\u03b8)\n6.5.5 Symbol-to-Symbol Derivatives\nAlgebraic expressions and computational graphs both operate on symbols, or\nvariables that do not have specific values. These algebraic and graph-based\nrepresentations are called symbolic representations. When we actually use or\ntrain a neural network, we must assign specific values to these symbols. We\nreplace a symbolic input to the network x with a specific numeric value, such as\n[1.2, 3.765, \u22121.8]\ue03e.\n212\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation\nyields the gradients on the activations a(k) for each layer k, starting from the\noutput layer and going backwards to the first hidden layer. From these gradients,\nwhich can be interpreted as an indication of how each layer\u2019s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. The\ngradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods.\nAfter the forward computation, compute the gradient on the output layer:\ng \u2190 \u2207y\u02c6J = \u2207y\u02c6L(y\u02c6, y)\nfor k = l, l \u2212 1, . . . , 1 do\nConvert the gradient on the layer\u2019s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\ng \u2190 \u2207a(k)J = g \ue00c f\ue030\n(a(k)\n)\nCompute gradients on weights and biases (including the regularization term,\nwhere needed):\n\u2207b(k)J = g + \u03bb\u2207b(k)\u2126(\u03b8)\n\u2207W (k)J = g h(k\u22121)\ue03e + \u03bb\u2207W (k)\u2126(\u03b8)\nPropagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations:\ng \u2190 \u2207h(k\u22121) J = W(k)\ue03e g\nend for\n213\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nz\nx\ny\nw\nf\nf\nf\ndz\ndy\nf \ue021\ndy\ndx\nf \ue021\ndz\ndx\n\u00d7\ndx\ndw\nf \ue021\ndz\ndw\n\u00d7\nFigure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\nto compute these derivatives. A generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\nrepresenting z = f (f(f (w))). (Right)We run the back-propagation algorithm, instructing\nit to construct the graph for the expression corresponding to dz\ndw . In this example, we do\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\nSome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. We call this approach \u201csymbolto-number\u201d differentiation. This is the approach used by libraries such as Torch\n(Collobert et al., 2011b) and Caffe (Jia, 2013).\nAnother approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. This\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\nis illustrated in figure 6.10. The primary advantage of this approach is that\nthe derivatives are described in the same language as the original expression.\nBecause the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. Computation of higher-order derivatives is described in section 6.5.10.\nWe will use the latter approach and describe the back-propagation algorithm in\n214\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nterms of constructing a computational graph for the derivatives. Any subset of the\ngraph may then be evaluated using specific numerical values at a later time. This\nallows us to avoid specifying exactly when each operation should be computed.\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents\u2019 values are available.\nThe description of the symbol-to-symbol based approach subsumes the symbolto-number approach. The symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\napproach does not expose the graph.\n6.5.6 General Back-Propagation\nThe back-propagation algorithm is very simple. To compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by dz\ndz = 1. We can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the Jacobian of the operation that produced z. We continue\nmultiplying by Jacobians traveling backwards through the graph in this way until\nwe reach x. For any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\nMore formally, each node in the graph G corresponds to a variable. To achieve\nmaximum generality, we describe this variable as being a tensor V. Tensor can\nin general have any number of dimensions. They subsume scalars, vectors, and\nmatrices.\nWe assume that each variable V is associated with the following subroutines:\n\u2022 get_operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,\nthere may be a Python or C++ class representing the matrix multiplication\noperation, and the get_operation function. Suppose we have a variable that\nis created by matrix multiplication, C = AB. Then get_operation(V)\nreturns a pointer to an instance of the corresponding C++ class.\n\u2022 get_consumers(V, G): This returns the list of variables that are children of\nV in the computational graph G.\n\u2022 get_inputs(V, G): This returns the list of variables that are parents of V\nin the computational graph G.\n215\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nEach operation op is also associated with a bprop operation. This bprop\noperation can compute a Jacobian-vector product as described by equation 6.47.\nThis is how the back-propagation algorithm is able to achieve great generality.\nEach operation is responsible for knowing how to back-propagate through the\nedges in the graph that it participates in. For example, we might use a matrix\nmultiplication operation to create a variable C = AB. Suppose that the gradient\nof a scalar z with respect to C is given by G. The matrix multiplication operation\nis responsible for defining two back-propagation rules, one for each of its input\narguments. If we call the bprop method to request the gradient with respect to\nA given that the gradient on the output is G, then the bprop method of the\nmatrix multiplication operation must state that the gradient with respect to A\nis given by GB\ue03e. Likewise, if we call the bprop method to request the gradient\nwith respect to B, then the matrix operation is responsible for implementing the\nbprop method and specifying that the desired gradient is given by A\ue03eG. The\nback-propagation algorithm itself does not need to know any differentiation rules. It\nonly needs to call each operation\u2019s bprop rules with the right arguments. Formally,\nop.bprop(inputs, X, G) must return\n\ue058\ni\n(\u2207 Xop.f(inputs)i) Gi, (6.54)\nwhich is just an implementation of the chain rule as expressed in equation 6.47.\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements,X is the input whose gradient\nwe wish to compute, and G is the gradient on the output of the operation.\nThe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. For example, if the mul operator is passed\ntwo copies of x to compute x2, the op.bprop method should still return x as the\nderivative with respect to both inputs. The back-propagation algorithm will later\nadd both of these arguments together to obtain 2x, which is the correct total\nderivative on x.\nSoftware implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually.\nThe back-propagation algorithm is formally described in algorithm 6.5.\n216\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\nportion does simple setup and cleanup work. Most of the important work happens\nin the build_grad subroutine of algorithm 6.6 .\nRequire: T, the target set of variables whose gradients must be computed.\nRequire: G, the computational graph\nRequire: z, the variable to be differentiated\nLet G\ue030 be G pruned to contain only nodes that are ancestors of z and descendents\nof nodes in T.\nInitialize grad_table, a data structure associating tensors to their gradients\ngrad_table[z] \u2190 1\nfor V in T do\nbuild_grad(V, G, G\ue030 , grad_table)\nend for\nReturn grad_table restricted to T\nIn section 6.5.2, we explained that back-propagation was developed in order to\navoid computing the same subexpression in the chain rule multiple times. The naive\nalgorithm could have exponential runtime due to these repeated subexpressions.\nNow that we have specified the back-propagation algorithm, we can understand its\ncomputational cost. If we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed. Keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\nmultiplication as a single operation). Computing a gradient in a graph with n nodes\nwill never execute more than O(n2) operations or store the output of more than\nO(n2) operations. Here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. For example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. We can see that computing the gradient requires as\nmost O(n2) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). The back-propagation algorithm\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\nedge in the original graph. Because the computational graph is a directed acyclic\ngraph it has at most O(n2 ) edges. For the kinds of graphs that are commonly used\nin practice, the situation is even better. Most neural network cost functions are\n217\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.6 The inner loop subroutine build_grad(V, G, G\ue030\n, grad_table) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm 6.5.\nRequire: V, the variable whose gradient should be added to G and grad_table.\nRequire: G, the graph to modify.\nRequire: G\ue030\n, the restriction of G to nodes that participate in the gradient.\nRequire: grad_table, a data structure mapping nodes to their gradients\nif V is in grad_table then\nReturn grad_table[V]\nend if\ni \u2190 1\nfor C in get_consumers(V, G\ue030\n) do\nop \u2190 get_operation\u00a9\nD \u2190 build_grad(C, G,G\ue030\n, grad_table)\nG(i) \u2190 op.bprop(get_inputs(C, G\ue030\n), V, D)\ni \u2190 i + 1\nend for\nG \u2190\ue050\ni G(i)\ngrad_table[V] = G\nInsert G and the operations creating it into G\nReturn G\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. This potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (equation 6.49) non-recursively:\n\u2202u(n)\n\u2202u(j) = \ue058\npath (u(\u03c01),u(\u03c02) ,\u2026,u(\u03c0t) ),\nfrom \u03c01=j to \u03c0t=n\n\ue059\nt\nk=2\n\u2202u(\u03c0k)\n\u2202u(\u03c0k\u22121 )\n. (6.55)\nSince the number of paths from node j to node n can grow exponentially in the\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. This large cost would be incurred because the same computation for\n\u2202u(i)\n\u2202u(j) would be redone many times. To avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results \u2202u(n)\n\u2202u(i) . Each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. By filling in these table entries in order,\n218\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nback-propagation avoids repeating many common subexpressions. This table-filling\nstrategy is sometimes called dynamic programming.\n6.5.7 Example: Back-Propagation for MLP Training\nAs an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\nHere we develop a very simple multilayer perception with a single hidden\nlayer. To train this model, we will use minibatch stochastic gradient descent.\nThe back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. Specifically, we use a minibatch of examples from the training\nset formatted as a design matrix X and a vector of associated class labels y.\nThe network computes a layer of hidden features H = max{0, XW(1)}. To\nsimplify the presentation we do not use biases in this model. We assume that our\ngraph language includes a relu operation that can compute max{0, Z} elementwise. The predictions of the unnormalized log probabilities over classes are then\ngiven by HW (2). We assume that our graph language includes a cross_entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. The resulting crossentropy defines the cost JMLE. Minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. However, to make this example more realistic,\nwe also include a regularization term. The total cost\nJ = JMLE + \u03bb\n\uf8eb\n\uf8ed\ue058\ni,j\n\ue010\nW (1)\ni,j \ue0112\n+ \ue058\ni,j\n\ue010\nW(2)\ni,j \ue0112\n\uf8f6\n\uf8f8 (6.56)\nconsists of the cross-entropy and a weight decay term with coefficient \u03bb. The\ncomputational graph is illustrated in figure 6.11.\nThe computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. This demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually.\nWe can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in figure 6.11. To train, we wish\nto compute both \u2207W (1)J and \u2207W (2)J. There are two different paths leading\nbackward from J to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\nalways contribute 2\u03bbW(i) to the gradient on W(i) .\n219\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nX W (1)\nU (1)\nmatmul\nH\nrelu\nU (3)\nsqr u(4)\nsum\nW (2) u(7) \u03bb\nU (2)\nmatmul\ny\nJMLE\ncross_entropy\nU (5)\nsqr u(6)\nsum\nu(8)\nJ\n+\n\u00d7\n+\nFigure 6.11: The computational graph used to compute the cost used to train our example\nof a single-layer MLP using the cross-entropy loss and weight decay.\nThe other path through the cross-entropy cost is slightly more complicated.\nLet G be the gradient on the unnormalized log probabilities U(2) provided by\nthe cross_entropy operation. The back-propagation algorithm now needs to\nexplore two different branches. On the shorter branch, it adds H \ue03eG to the\ngradient on W(2) , using the back-propagation rule for the second argument to\nthe matrix multiplication operation. The other branch corresponds to the longer\nchain descending further along the network. First, the back-propagation algorithm\ncomputes \u2207HJ = GW(2)\ue03e using the back-propagation rule for the first argument\nto the matrix multiplication operation. Next, the relu operation uses its backpropagation rule to zero out components of the gradient corresponding to entries\nof U(1) that were less than 0. Let the result be called G\ue030 . The last step of the\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the matmul operation to add X\ue03eG\ue030 to the gradient on W(1).\nAfter these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters.\nFor the MLP, the computational cost is dominated by the cost of matrix\nmultiplication. During the forward propagation stage, we multiply by each weight\n220\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost. The main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\nThis value is stored from the time it is computed until the backward pass has\nreturned to the same point. The memory cost is thus O(mnh), where m is the\nnumber of examples in the minibatch and nh is the number of hidden units.\n\u201d\nSummarize the above basic principle of back-propagation for neural network training with equations, try to use your own words, do not directly copy the sentence from the above paper."}, {"role": "assistant", "content": "Back-propagation is a fundamental algorithm used for training neural networks, particularly in calculating the gradient of a loss function with respect to the network's parameters (weights). At its core, back-propagation leverages the chain rule from calculus to efficiently compute these gradients, which are then used to update the network's weights in a direction that minimally reduces the error between the network's predictions and the actual target values. \n\nLet's break down the essential steps and principles underlying the back-propagation algorithm, without directly quoting the provided text:\n\n1. **Forward Pass**: The algorithm starts with a forward pass, where input data is passed through the neural network layer by layer, from the input layer to the hidden layers, and finally to the output layer. This process computes the activations at each layer until the final prediction output \\(y^{(\\hat)}\\) is produced.\n\n2. **Computation of the Loss**: Once we have the output from the forward pass, we calculate the loss (or cost) \\(J(\\theta)\\) using a loss function. This function measures the difference between the predicted output \\(y^{(\\hat)}\\) and the actual target values \\(y\\). Common loss functions include Mean Squared Error for regression tasks and Cross-Entropy Loss for classification tasks.\n\n3. **Backward Pass**: The core of back-propagation happens in the backward pass. Starting from the computed loss, we trace our steps back through the network to calculate the gradients of the loss function with respect to each parameter (weight) of the network. This is done by applying the chain rule of calculus. For a given weight \\(w\\), the chain rule helps us understand how the loss \\(J(\\theta)\\) changes with a slight change in \\(w\\).\n\n4. **Gradient Computation**: For each layer starting from the output back to the input, we compute the gradient of the loss with respect to the weights of that layer. This tells us how we need to adjust the weights to decrease the loss. If \\(w\\) significantly influences the increase in loss, its gradient will be large, and consequently, \\(w\\) needs a substantial adjustment. \n\n5. **Parameter Update**: After calculating the gradients, the network's parameters (weights and biases) are updated using an optimization algorithm like Stochastic Gradient Descent (SGD). The update rule typically looks like \\(w = w - \\eta \\nabla J(\\theta)\\), where \\(\\eta\\) is the learning rate, a small positive scalar determining the step size of the updates.\n\n6. **Iteration**: Steps 1-5 are repeated for many iterations (epochs) over the training dataset. With each iteration, the weights of the network are adjusted to minimize the loss, and the model gets progressively better at making predictions.\n\nBy iteratively applying these steps, back-propagation enables neural networks to learn complex mappings from inputs to outputs, making it a cornerstone algorithm in the field of deep learning."}]}, {"facetValue": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs."}], "conversation": [{"role": "user", "content": "\u201c6.5 Back-Propagation and Other Differentiation Algorithms\nWhen we use a feedforward neural network to accept an input x and produce an\noutput y\u02c6, information flows forward through the network. The inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces y\u02c6. This is called forward propagation. During training,\nforward propagation can continue onward until it produces a scalar cost J(\u03b8).\nThe back-propagation algorithm (Rumelhart et al., 1986a), often simply called\nbackprop, allows the information from the cost to then flow backwards through\nthe network, in order to compute the gradient.\nComputing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. The\nback-propagation algorithm does so using a simple and inexpensive procedure.\nThe term back-propagation is often misunderstood as meaning the whole\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nFurthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). Specifically, we will describe how to compute the gradient\n\u2207xf(x, y) for an arbitrary function f , wherex is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\nbut whose derivatives are not required. In learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters,\n\u2207\u03b8 J(\u03b8). Many machine learning tasks involve computing other derivatives, either\nas part of the learning process, or to analyze the learned model. The backpropagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. The\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the Jacobian of a function\nf with multiple outputs. We restrict our description here to the most commonly\nused case where f has a single output.\n204\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.1 Computational Graphs\nSo far we have discussed neural networks with a relatively informal graph language.\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\nMany ways of formalizing computation as graphs are possible.\nHere, we use each node in the graph to indicate a variable. The variable may\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\nTo formalize our graphs, we also need to introduce the idea of an operation.\nAn operation is a simple function of one or more variables. Our graph language\nis accompanied by a set of allowable operations. Functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\nWithout loss of generality, we define an operation to return only a single\noutput variable. This does not lose generality because the output variable can have\nmultiple entries, such as a vector. Software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\nIf a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y. We sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context.\nExamples of computational graphs are shown in figure 6.8.\n6.5.2 Chain Rule of Calculus\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. Back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient.\nLet x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. Suppose that y = g(x) and z = f (g(x)) = f (y). Then\nthe chain rule states that\ndz\ndx = dz\ndy\ndy\ndx . (6.44)\nWe can generalize this beyond the scalar case. Suppose that x \u2208 Rm, y \u2208 Rn,\n205\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx y\n(a)\n\u00d7\nx w\n(b)\nu(1)\ndot\nb\nu(2)\n+\ny\u02c6\n\u03c3\n\u00a9\nX W\nU (1)\nmatmul\nb\nU (2)\n+\nH\nrelu\nx w\n(d)\ny\u02c6\ndot\n\u03bb\nu(1)\nsqr\nu(2)\nsum\nu(3)\n\u00d7\nFigure 6.8: Examples of computational graphs. (a)The graph using the\u00d7 operation to\ncompute z = xy. (b)The graph for the logistic regression prediction y\u02c6 = \u03c3\n\ue000\nx\ue03ew + b\n\ue001\n.\nSome of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. We simply name the i-th such variable u(i)\n. \u00a9The\ncomputational graph for the expression H = max{0,XW + b}, which computes a design\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\nof inputs X . (d)Examples a\u2013c applied at most one operation to each variable, but it\nis possible to apply more than one operation. Here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. The\nweights are used to make both the prediction y\u02c6 and the weight decay penalty \u03bb\n\ue050\ni w2\ni .\n206\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ng maps from Rm to Rn, and f maps from Rn to R. If y = g(x) and z = f(y), then\n\u2202z\n\u2202xi\n= \ue058\nj\n\u2202z\n\u2202yj\n\u2202yj\n\u2202xi\n. (6.45)\nIn vector notation, this may be equivalently written as\n\u2207xz =\n\ue012 \u2202y\n\u2202x\n\ue013\ue03e\n\u2207y z, (6.46)\nwhere \u2202y\n\u2202x is the n \u00d7 m Jacobian matrix of g.\nFrom this we see that the gradient of a variable x can be obtained by multiplying\na Jacobian matrix \u2202y\n\u2202x by a gradient \u2207yz. The back-propagation algorithm consists\nof performing such a Jacobian-gradient product for each operation in the graph.\nUsually we do not apply the back-propagation algorithm merely to vectors,\nbut rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\nsame as back-propagation with vectors. The only difference is how the numbers\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor. In this rearranged view,\nback-propagation is still just multiplying Jacobians by gradients.\nTo denote the gradient of a value z with respect to a tensor X, we write \u2207Xz,\njust as if X were a vector. The indices into X now have multiple coordinates\u2014for\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\nby using a single variable i to represent the complete tuple of indices. For all\npossible index tuples i, (\u2207Xz)i gives \u2202z\n\u2202Xi\n. This is exactly the same as how for all\npossible integer indices i into a vector, (\u2207x z)i gives \u2202z\n\u2202xi\n. Using this notation, we\ncan write the chain rule as it applies to tensors. If Y = g(X) and z = f(Y), then\n\u2207X z =\ue058\nj\n(\u2207XYj ) \u2202z\n\u2202Yj\n. (6.47)\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\nUsing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. However, actually evaluating that expression in a computer\nintroduces some extra considerations.\nSpecifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. Any procedure that computes the gradient\n207\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwill need to choose whether to store these subexpressions or to recompute them\nseveral times. An example of how these repeated subexpressions arise is given in\nfigure 6.9. In some cases, computing the same subexpression twice would simply\nbe wasteful. For complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nIn other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\nWe first begin by a version of the back-propagation algorithm that specifies the\nactual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the\nassociated forward computation), in the order it will actually be done and according\nto the recursive application of chain rule. One could either directly perform these\ncomputations or view the description of the algorithm as a symbolic specification\nof the computational graph for computing the back-propagation. However, this\nformulation does not make explicit the manipulation and the construction of the\nsymbolic graph that performs the gradient computation. Such a formulation is\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\nnodes that contain arbitrary tensors.\nFirst consider a computational graph describing how to compute a single scalar\nu(n) (say the loss on a training example). This scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u(1) to u(ni). In\nother words we wish to compute \u2202u(n)\n\u2202u(i) for all i \u2208 {1, 2, . . . , ni}. In the application\nof back-propagation to computing gradients for gradient descent over parameters,\nu(n) will be the cost associated with an example or a minibatch, while u(1) to u(ni)\ncorrespond to the parameters of the model.\nWe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u(ni+1) and\ngoing up to u(n)\n. As defined in algorithm 6.1, each node u(i) is associated with an\noperation f(i) and is computed by evaluating the function\nu(i) = f(A(i)\n) (6.48)\nwhere A(i) is the set of all nodes that are parents of u(i)\n.\nThat algorithm specifies the forward propagation computation, which we could\nput in a graph G. In order to perform back-propagation, we can construct a\ncomputational graph that depends onG and adds to it an extra set of nodes. These\nform a subgraph B with one node per node of G. Computation in B proceeds in\nexactly the reverse of the order of computation in G, and each node of B computes\nthe derivative \u2202u(n)\n\u2202u(i) associated with the forward graph node u(i)\n. This is done\n208\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.1 A procedure that performs the computations mapping ni inputs\nu(1) to u(ni) to an output u(n)\n. This defines a computational graph where each node\ncomputes numerical value u(i) by applying a function f (i) to the set of arguments\nA(i) that comprises the values of previous nodes u(j)\n, j < i, with j \u2208 Pa(u(i)\n). The\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu(1) to u(ni ) . The output of the computational graph is read off the last (output)\nnode u(n)\n.\nfor i = 1, . . . , ni do\nu(i) \u2190 xi\nend for\nfor i = ni + 1, . . . , n do\nA(i) \u2190 {u(j) | j \u2208 Pa(u(i) )}\nu(i) \u2190 f (i)\n(A(i)\n)\nend for\nreturn u(n)\nusing the chain rule with respect to scalar output u(n)\n:\n\u2202u(n)\n\u2202u(j) = \ue058\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u(i)\n\u2202u(j) (6.49)\nas specified by algorithm 6.2. The subgraph B contains exactly one edge for each\nedge from node u(j) to node u(i) of G. The edge from u(j) to u(i) is associated with\nthe computation of \u2202u(i)\n\u2202u(j) . In addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u(i) that are children\nof u(j) and the vector containing the partial derivatives \u2202u(i)\n\u2202u(j) for the same children\nnodes u(i) . To summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in G, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations.\nThe back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. Specifically, it performs on the order\nof one Jacobian product per node in the graph. This can be seen from the fact\nthat backprop (algorithm 6.2) visits each edge from node u(j) to node u(i) of\nthe graph exactly once in order to obtain the associated partial derivative \u2202u(i)\n\u2202u(j) .\n209\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.2 Simplified version of the back-propagation algorithm for computing\nthe derivatives of u(n) with respect to the variables in the graph. This example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u(1), . . . , u(ni )\n.\nThis simplified version computes the derivatives of all nodes in the graph. The\ncomputational cost of this algorithm is proportional to the number of edges in\nthe graph, assuming that the partial derivative associated with each edge requires\na constant time. This is of the same order as the number of computations for\nthe forward propagation. Each \u2202u(i)\n\u2202u(j) is a function of the parents u(j) of u(i)\n, thus\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\nRun forward propagation (algorithm 6.1 for this example) to obtain the activations of the network\nInitialize grad_table, a data structure that will store the derivatives that have\nbeen computed. The entry grad_table[u(i)\n] will store the computed value of\n\u2202u(n)\n\u2202u(i) .\ngrad_table[u(n)\n] \u2190 1\nfor j = n \u2212 1 down to 1 do\nThe next line computes \u2202u(n)\n\u2202u(j) = \ue050\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u (i)\n\u2202u(j) using stored values:\ngrad_table[u(j) ] \u2190 \ue050\ni:j\u2208Pa(u(i)) grad_table[u(i) ] \u2202u(i)\n\u2202u(j)\nend for\nreturn {grad_table[u(i)\n] | i = 1, . . . , ni}\nBack-propagation thus avoids the exponential explosion in repeated subexpressions.\nHowever, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. We will revisit these ideas\nafter describing the back-propagation algorithm itself.\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\nTo clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer MLP.\nAlgorithm 6.3 first shows the forward propagation, which maps parameters to\nthe supervised loss L(y\u02c6, y) associated with a single (input,target) training example\n(x, y), with \u02c6y the output of the neural network when x is provided in input.\nAlgorithm 6.4 then shows the corresponding computation to be done for\n210\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nFigure 6.9: A computational graph that results in repeated subexpressions when computing\nthe gradient. Let w \u2208 R be the input to the graph. We use the same function f : R \u2192 R\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f( y).\nTo compute \u2202z\n\u2202w , we apply equation 6.44 and obtain:\n\u2202z\n\u2202w (6.50)\n= \u2202z\n\u2202y\n\u2202y\n\u2202x\n\u2202x\n\u2202w (6.51)\n=f\ue030\n(y)f\ue030\n(x)f\ue030\n(w) (6.52)\n=f\ue030\n(f(f(w)))f \ue030\n(f(w))f\ue030\n(w) (6.53)\nEquation 6.52 suggests an implementation in which we compute the value off (w) only\nonce and store it in the variable x. This is the approach taken by the back-propagation\nalgorithm. An alternative approach is suggested by equation 6.53, where the subexpression\nf(w) appears more than once. In the alternative approach, f(w) is recomputed each time\nit is needed. When the memory required to store the value of these expressions is low, the\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\nuseful when memory is limited.\n211\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napplying the back-propagation algorithm to this graph.\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\nstraightforward to understand. However, they are specialized to one specific\nproblem.\nModern software implementations are based on the generalized form of backpropagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\nAlgorithm 6.3 Forward propagation through a typical deep neural network and\nthe computation of the cost function. The loss L(y\u02c6, y) depends on the output\ny\u02c6 and on the target y (see section 6.2.1.1 for examples of loss functions). To\nobtain the total cost J, the loss may be added to a regularizer \u2126(\u03b8), where \u03b8\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\ncompute gradients of J with respect to parameters W and b. For simplicity, this\ndemonstration uses only a single input example x. Practical applications should\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\nRequire: Network depth, l\nRequire: W(i)\n,i \u2208 {1, . . . , l}, the weight matrices of the model\nRequire: b(i) ,i \u2208 {1, . . . , l}, the bias parameters of the model\nRequire: x, the input to process\nRequire: y, the target output\nh(0) = x\nfor k = 1, . . . , l do\na(k) = b(k) + W(k)h(k\u22121)\nh(k) = f(a(k)\n)\nend for\ny\u02c6 = h(l)\nJ = L(y\u02c6, y) + \u03bb\u2126(\u03b8)\n6.5.5 Symbol-to-Symbol Derivatives\nAlgebraic expressions and computational graphs both operate on symbols, or\nvariables that do not have specific values. These algebraic and graph-based\nrepresentations are called symbolic representations. When we actually use or\ntrain a neural network, we must assign specific values to these symbols. We\nreplace a symbolic input to the network x with a specific numeric value, such as\n[1.2, 3.765, \u22121.8]\ue03e.\n212\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation\nyields the gradients on the activations a(k) for each layer k, starting from the\noutput layer and going backwards to the first hidden layer. From these gradients,\nwhich can be interpreted as an indication of how each layer\u2019s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. The\ngradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods.\nAfter the forward computation, compute the gradient on the output layer:\ng \u2190 \u2207y\u02c6J = \u2207y\u02c6L(y\u02c6, y)\nfor k = l, l \u2212 1, . . . , 1 do\nConvert the gradient on the layer\u2019s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\ng \u2190 \u2207a(k)J = g \ue00c f\ue030\n(a(k)\n)\nCompute gradients on weights and biases (including the regularization term,\nwhere needed):\n\u2207b(k)J = g + \u03bb\u2207b(k)\u2126(\u03b8)\n\u2207W (k)J = g h(k\u22121)\ue03e + \u03bb\u2207W (k)\u2126(\u03b8)\nPropagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations:\ng \u2190 \u2207h(k\u22121) J = W(k)\ue03e g\nend for\n213\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nz\nx\ny\nw\nf\nf\nf\ndz\ndy\nf \ue021\ndy\ndx\nf \ue021\ndz\ndx\n\u00d7\ndx\ndw\nf \ue021\ndz\ndw\n\u00d7\nFigure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\nto compute these derivatives. A generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\nrepresenting z = f (f(f (w))). (Right)We run the back-propagation algorithm, instructing\nit to construct the graph for the expression corresponding to dz\ndw . In this example, we do\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\nSome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. We call this approach \u201csymbolto-number\u201d differentiation. This is the approach used by libraries such as Torch\n(Collobert et al., 2011b) and Caffe (Jia, 2013).\nAnother approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. This\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\nis illustrated in figure 6.10. The primary advantage of this approach is that\nthe derivatives are described in the same language as the original expression.\nBecause the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. Computation of higher-order derivatives is described in section 6.5.10.\nWe will use the latter approach and describe the back-propagation algorithm in\n214\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nterms of constructing a computational graph for the derivatives. Any subset of the\ngraph may then be evaluated using specific numerical values at a later time. This\nallows us to avoid specifying exactly when each operation should be computed.\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents\u2019 values are available.\nThe description of the symbol-to-symbol based approach subsumes the symbolto-number approach. The symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\napproach does not expose the graph.\n6.5.6 General Back-Propagation\nThe back-propagation algorithm is very simple. To compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by dz\ndz = 1. We can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the Jacobian of the operation that produced z. We continue\nmultiplying by Jacobians traveling backwards through the graph in this way until\nwe reach x. For any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\nMore formally, each node in the graph G corresponds to a variable. To achieve\nmaximum generality, we describe this variable as being a tensor V. Tensor can\nin general have any number of dimensions. They subsume scalars, vectors, and\nmatrices.\nWe assume that each variable V is associated with the following subroutines:\n\u2022 get_operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,\nthere may be a Python or C++ class representing the matrix multiplication\noperation, and the get_operation function. Suppose we have a variable that\nis created by matrix multiplication, C = AB. Then get_operation(V)\nreturns a pointer to an instance of the corresponding C++ class.\n\u2022 get_consumers(V, G): This returns the list of variables that are children of\nV in the computational graph G.\n\u2022 get_inputs(V, G): This returns the list of variables that are parents of V\nin the computational graph G.\n215\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nEach operation op is also associated with a bprop operation. This bprop\noperation can compute a Jacobian-vector product as described by equation 6.47.\nThis is how the back-propagation algorithm is able to achieve great generality.\nEach operation is responsible for knowing how to back-propagate through the\nedges in the graph that it participates in. For example, we might use a matrix\nmultiplication operation to create a variable C = AB. Suppose that the gradient\nof a scalar z with respect to C is given by G. The matrix multiplication operation\nis responsible for defining two back-propagation rules, one for each of its input\narguments. If we call the bprop method to request the gradient with respect to\nA given that the gradient on the output is G, then the bprop method of the\nmatrix multiplication operation must state that the gradient with respect to A\nis given by GB\ue03e. Likewise, if we call the bprop method to request the gradient\nwith respect to B, then the matrix operation is responsible for implementing the\nbprop method and specifying that the desired gradient is given by A\ue03eG. The\nback-propagation algorithm itself does not need to know any differentiation rules. It\nonly needs to call each operation\u2019s bprop rules with the right arguments. Formally,\nop.bprop(inputs, X, G) must return\n\ue058\ni\n(\u2207 Xop.f(inputs)i) Gi, (6.54)\nwhich is just an implementation of the chain rule as expressed in equation 6.47.\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements,X is the input whose gradient\nwe wish to compute, and G is the gradient on the output of the operation.\nThe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. For example, if the mul operator is passed\ntwo copies of x to compute x2, the op.bprop method should still return x as the\nderivative with respect to both inputs. The back-propagation algorithm will later\nadd both of these arguments together to obtain 2x, which is the correct total\nderivative on x.\nSoftware implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually.\nThe back-propagation algorithm is formally described in algorithm 6.5.\n216\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\nportion does simple setup and cleanup work. Most of the important work happens\nin the build_grad subroutine of algorithm 6.6 .\nRequire: T, the target set of variables whose gradients must be computed.\nRequire: G, the computational graph\nRequire: z, the variable to be differentiated\nLet G\ue030 be G pruned to contain only nodes that are ancestors of z and descendents\nof nodes in T.\nInitialize grad_table, a data structure associating tensors to their gradients\ngrad_table[z] \u2190 1\nfor V in T do\nbuild_grad(V, G, G\ue030 , grad_table)\nend for\nReturn grad_table restricted to T\nIn section 6.5.2, we explained that back-propagation was developed in order to\navoid computing the same subexpression in the chain rule multiple times. The naive\nalgorithm could have exponential runtime due to these repeated subexpressions.\nNow that we have specified the back-propagation algorithm, we can understand its\ncomputational cost. If we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed. Keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\nmultiplication as a single operation). Computing a gradient in a graph with n nodes\nwill never execute more than O(n2) operations or store the output of more than\nO(n2) operations. Here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. For example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. We can see that computing the gradient requires as\nmost O(n2) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). The back-propagation algorithm\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\nedge in the original graph. Because the computational graph is a directed acyclic\ngraph it has at most O(n2 ) edges. For the kinds of graphs that are commonly used\nin practice, the situation is even better. Most neural network cost functions are\n217\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.6 The inner loop subroutine build_grad(V, G, G\ue030\n, grad_table) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm 6.5.\nRequire: V, the variable whose gradient should be added to G and grad_table.\nRequire: G, the graph to modify.\nRequire: G\ue030\n, the restriction of G to nodes that participate in the gradient.\nRequire: grad_table, a data structure mapping nodes to their gradients\nif V is in grad_table then\nReturn grad_table[V]\nend if\ni \u2190 1\nfor C in get_consumers(V, G\ue030\n) do\nop \u2190 get_operation\u00a9\nD \u2190 build_grad(C, G,G\ue030\n, grad_table)\nG(i) \u2190 op.bprop(get_inputs(C, G\ue030\n), V, D)\ni \u2190 i + 1\nend for\nG \u2190\ue050\ni G(i)\ngrad_table[V] = G\nInsert G and the operations creating it into G\nReturn G\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. This potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (equation 6.49) non-recursively:\n\u2202u(n)\n\u2202u(j) = \ue058\npath (u(\u03c01),u(\u03c02) ,\u2026,u(\u03c0t) ),\nfrom \u03c01=j to \u03c0t=n\n\ue059\nt\nk=2\n\u2202u(\u03c0k)\n\u2202u(\u03c0k\u22121 )\n. (6.55)\nSince the number of paths from node j to node n can grow exponentially in the\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. This large cost would be incurred because the same computation for\n\u2202u(i)\n\u2202u(j) would be redone many times. To avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results \u2202u(n)\n\u2202u(i) . Each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. By filling in these table entries in order,\n218\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nback-propagation avoids repeating many common subexpressions. This table-filling\nstrategy is sometimes called dynamic programming.\n6.5.7 Example: Back-Propagation for MLP Training\nAs an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\nHere we develop a very simple multilayer perception with a single hidden\nlayer. To train this model, we will use minibatch stochastic gradient descent.\nThe back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. Specifically, we use a minibatch of examples from the training\nset formatted as a design matrix X and a vector of associated class labels y.\nThe network computes a layer of hidden features H = max{0, XW(1)}. To\nsimplify the presentation we do not use biases in this model. We assume that our\ngraph language includes a relu operation that can compute max{0, Z} elementwise. The predictions of the unnormalized log probabilities over classes are then\ngiven by HW (2). We assume that our graph language includes a cross_entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. The resulting crossentropy defines the cost JMLE. Minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. However, to make this example more realistic,\nwe also include a regularization term. The total cost\nJ = JMLE + \u03bb\n\uf8eb\n\uf8ed\ue058\ni,j\n\ue010\nW (1)\ni,j \ue0112\n+ \ue058\ni,j\n\ue010\nW(2)\ni,j \ue0112\n\uf8f6\n\uf8f8 (6.56)\nconsists of the cross-entropy and a weight decay term with coefficient \u03bb. The\ncomputational graph is illustrated in figure 6.11.\nThe computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. This demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually.\nWe can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in figure 6.11. To train, we wish\nto compute both \u2207W (1)J and \u2207W (2)J. There are two different paths leading\nbackward from J to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\nalways contribute 2\u03bbW(i) to the gradient on W(i) .\n219\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nX W (1)\nU (1)\nmatmul\nH\nrelu\nU (3)\nsqr u(4)\nsum\nW (2) u(7) \u03bb\nU (2)\nmatmul\ny\nJMLE\ncross_entropy\nU (5)\nsqr u(6)\nsum\nu(8)\nJ\n+\n\u00d7\n+\nFigure 6.11: The computational graph used to compute the cost used to train our example\nof a single-layer MLP using the cross-entropy loss and weight decay.\nThe other path through the cross-entropy cost is slightly more complicated.\nLet G be the gradient on the unnormalized log probabilities U(2) provided by\nthe cross_entropy operation. The back-propagation algorithm now needs to\nexplore two different branches. On the shorter branch, it adds H \ue03eG to the\ngradient on W(2) , using the back-propagation rule for the second argument to\nthe matrix multiplication operation. The other branch corresponds to the longer\nchain descending further along the network. First, the back-propagation algorithm\ncomputes \u2207HJ = GW(2)\ue03e using the back-propagation rule for the first argument\nto the matrix multiplication operation. Next, the relu operation uses its backpropagation rule to zero out components of the gradient corresponding to entries\nof U(1) that were less than 0. Let the result be called G\ue030 . The last step of the\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the matmul operation to add X\ue03eG\ue030 to the gradient on W(1).\nAfter these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters.\nFor the MLP, the computational cost is dominated by the cost of matrix\nmultiplication. During the forward propagation stage, we multiply by each weight\n220\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost. The main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\nThis value is stored from the time it is computed until the backward pass has\nreturned to the same point. The memory cost is thus O(mnh), where m is the\nnumber of examples in the minibatch and nh is the number of hidden units.\n\u201d\nSummarize the above basic principle of back-propagation for neural network training with equations mathematically, try to use your own words, do not directly copy the sentence from the above paper."}, {"role": "assistant", "content": "Back-propagation is a computational algorithm commonly employed for training feedforward neural networks. The key principle hinges on the efficient computation of gradients of a loss function (denoted as J) with respect to the model parameters (denoted as \u03b8) by propagating error information backward through the network. \n\nIn a typical feedforward neural network, input data \\(x\\) flows forward through the network's layers to produce an output \\(\\hat{y}\\), through a process known as forward propagation. During training, after computing the loss \\(\\text{J}(\\theta)\\) (a measure of discrepancy between the predicted output \\(\\hat{y}\\) and the actual target \\(y\\)), back-propagation is used to calculate the gradients of J with respect to each parameter in the network. \n\nThe back-propagation algorithm utilizes the chain rule of calculus, a fundamental tool for computing derivatives of composite functions, to systematically compute these gradients. For a function \\(f(g(x))\\), the chain rule states \\(dz/dx = dz/dy \\cdot dy/dx\\), where \\(z = f(g(x))\\) and \\(y = g(x)\\). This principle gets extended in neural networks to compute the gradient of the loss function with respect to weights in each layer.\n\nThe back-propagation algorithm proceeds in two main steps after the forward pass:\n1. **Local Gradients Calculation:** For each node (unit) in the network, starting from the output layer and moving backward, compute the gradient of the loss function with respect to the activation of that node. This is achieved by applying the chain rule to decompose the derivative into a product of simpler derivatives (local gradients).\n\n2. **Gradients Accumulation:** Using these local gradients, compute the gradients of the loss with respect to the weights. This involves the multiplication of the local gradient by the gradient of the node's activation with respect to its input weight. These computations are straightforward and rely on the previously computed activations during the forward pass, thus allowing for an efficient computation of gradients.\n\nThe term \"back-propagation\" specifically refers to this computational strategy of using the chain rule from calculus in a backward manner, hence avoiding direct calculation of complex derivatives and efficiently reusing computed derivatives. It is critical to understand that back-propagation itself doesn't perform the parameter updates. Instead, it computes gradients that can then be used by optimization algorithms (like stochastic gradient descent) to adjust the parameters in a direction that minimizes the loss.\n\nMathematically, if we consider a model function \\(f(x, w)\\) parameterized by weights \\(w\\), evaluating at an input \\(x\\), and a loss function \\(\\text{J}(f(x, w), y)\\), where \\(y\\) is the true label or value, the objective of back-propagation is to compute \\(\\nabla_w \\text{J}\\), the gradient of \\(\\text{J}\\) with respect to \\(w\\). Through systematic application of the chain rule, back-propagation decomposes the process into manageable steps, such as computing the gradient of the loss with respect to each layer's output and then using these to find the gradients with respect to the weights.\n\nThis methodical computation ensures that regardless of the size or complexity of the neural network, the gradient of the loss with respect to any parameter can be calculated efficiently, which is essential for training deep learning models."}]}, {"facetValue": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs."}], "conversation": [{"role": "user", "content": "\u201c6.5 Back-Propagation and Other Differentiation Algorithms\nWhen we use a feedforward neural network to accept an input x and produce an\noutput y\u02c6, information flows forward through the network. The inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces y\u02c6. This is called forward propagation. During training,\nforward propagation can continue onward until it produces a scalar cost J(\u03b8).\nThe back-propagation algorithm (Rumelhart et al., 1986a), often simply called\nbackprop, allows the information from the cost to then flow backwards through\nthe network, in order to compute the gradient.\nComputing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. The\nback-propagation algorithm does so using a simple and inexpensive procedure.\nThe term back-propagation is often misunderstood as meaning the whole\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nFurthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). Specifically, we will describe how to compute the gradient\n\u2207xf(x, y) for an arbitrary function f , wherex is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\nbut whose derivatives are not required. In learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters,\n\u2207\u03b8 J(\u03b8). Many machine learning tasks involve computing other derivatives, either\nas part of the learning process, or to analyze the learned model. The backpropagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. The\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the Jacobian of a function\nf with multiple outputs. We restrict our description here to the most commonly\nused case where f has a single output.\n204\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.1 Computational Graphs\nSo far we have discussed neural networks with a relatively informal graph language.\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\nMany ways of formalizing computation as graphs are possible.\nHere, we use each node in the graph to indicate a variable. The variable may\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\nTo formalize our graphs, we also need to introduce the idea of an operation.\nAn operation is a simple function of one or more variables. Our graph language\nis accompanied by a set of allowable operations. Functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\nWithout loss of generality, we define an operation to return only a single\noutput variable. This does not lose generality because the output variable can have\nmultiple entries, such as a vector. Software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\nIf a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y. We sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context.\nExamples of computational graphs are shown in figure 6.8.\n6.5.2 Chain Rule of Calculus\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. Back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient.\nLet x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. Suppose that y = g(x) and z = f (g(x)) = f (y). Then\nthe chain rule states that\ndz\ndx = dz\ndy\ndy\ndx . (6.44)\nWe can generalize this beyond the scalar case. Suppose that x \u2208 Rm, y \u2208 Rn,\n205\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx y\n(a)\n\u00d7\nx w\n(b)\nu(1)\ndot\nb\nu(2)\n+\ny\u02c6\n\u03c3\n\u00a9\nX W\nU (1)\nmatmul\nb\nU (2)\n+\nH\nrelu\nx w\n(d)\ny\u02c6\ndot\n\u03bb\nu(1)\nsqr\nu(2)\nsum\nu(3)\n\u00d7\nFigure 6.8: Examples of computational graphs. (a)The graph using the\u00d7 operation to\ncompute z = xy. (b)The graph for the logistic regression prediction y\u02c6 = \u03c3\n\ue000\nx\ue03ew + b\n\ue001\n.\nSome of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. We simply name the i-th such variable u(i)\n. \u00a9The\ncomputational graph for the expression H = max{0,XW + b}, which computes a design\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\nof inputs X . (d)Examples a\u2013c applied at most one operation to each variable, but it\nis possible to apply more than one operation. Here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. The\nweights are used to make both the prediction y\u02c6 and the weight decay penalty \u03bb\n\ue050\ni w2\ni .\n206\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ng maps from Rm to Rn, and f maps from Rn to R. If y = g(x) and z = f(y), then\n\u2202z\n\u2202xi\n= \ue058\nj\n\u2202z\n\u2202yj\n\u2202yj\n\u2202xi\n. (6.45)\nIn vector notation, this may be equivalently written as\n\u2207xz =\n\ue012 \u2202y\n\u2202x\n\ue013\ue03e\n\u2207y z, (6.46)\nwhere \u2202y\n\u2202x is the n \u00d7 m Jacobian matrix of g.\nFrom this we see that the gradient of a variable x can be obtained by multiplying\na Jacobian matrix \u2202y\n\u2202x by a gradient \u2207yz. The back-propagation algorithm consists\nof performing such a Jacobian-gradient product for each operation in the graph.\nUsually we do not apply the back-propagation algorithm merely to vectors,\nbut rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\nsame as back-propagation with vectors. The only difference is how the numbers\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor. In this rearranged view,\nback-propagation is still just multiplying Jacobians by gradients.\nTo denote the gradient of a value z with respect to a tensor X, we write \u2207Xz,\njust as if X were a vector. The indices into X now have multiple coordinates\u2014for\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\nby using a single variable i to represent the complete tuple of indices. For all\npossible index tuples i, (\u2207Xz)i gives \u2202z\n\u2202Xi\n. This is exactly the same as how for all\npossible integer indices i into a vector, (\u2207x z)i gives \u2202z\n\u2202xi\n. Using this notation, we\ncan write the chain rule as it applies to tensors. If Y = g(X) and z = f(Y), then\n\u2207X z =\ue058\nj\n(\u2207XYj ) \u2202z\n\u2202Yj\n. (6.47)\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\nUsing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. However, actually evaluating that expression in a computer\nintroduces some extra considerations.\nSpecifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. Any procedure that computes the gradient\n207\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwill need to choose whether to store these subexpressions or to recompute them\nseveral times. An example of how these repeated subexpressions arise is given in\nfigure 6.9. In some cases, computing the same subexpression twice would simply\nbe wasteful. For complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nIn other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\nWe first begin by a version of the back-propagation algorithm that specifies the\nactual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the\nassociated forward computation), in the order it will actually be done and according\nto the recursive application of chain rule. One could either directly perform these\ncomputations or view the description of the algorithm as a symbolic specification\nof the computational graph for computing the back-propagation. However, this\nformulation does not make explicit the manipulation and the construction of the\nsymbolic graph that performs the gradient computation. Such a formulation is\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\nnodes that contain arbitrary tensors.\nFirst consider a computational graph describing how to compute a single scalar\nu(n) (say the loss on a training example). This scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u(1) to u(ni). In\nother words we wish to compute \u2202u(n)\n\u2202u(i) for all i \u2208 {1, 2, . . . , ni}. In the application\nof back-propagation to computing gradients for gradient descent over parameters,\nu(n) will be the cost associated with an example or a minibatch, while u(1) to u(ni)\ncorrespond to the parameters of the model.\nWe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u(ni+1) and\ngoing up to u(n)\n. As defined in algorithm 6.1, each node u(i) is associated with an\noperation f(i) and is computed by evaluating the function\nu(i) = f(A(i)\n) (6.48)\nwhere A(i) is the set of all nodes that are parents of u(i)\n.\nThat algorithm specifies the forward propagation computation, which we could\nput in a graph G. In order to perform back-propagation, we can construct a\ncomputational graph that depends onG and adds to it an extra set of nodes. These\nform a subgraph B with one node per node of G. Computation in B proceeds in\nexactly the reverse of the order of computation in G, and each node of B computes\nthe derivative \u2202u(n)\n\u2202u(i) associated with the forward graph node u(i)\n. This is done\n208\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.1 A procedure that performs the computations mapping ni inputs\nu(1) to u(ni) to an output u(n)\n. This defines a computational graph where each node\ncomputes numerical value u(i) by applying a function f (i) to the set of arguments\nA(i) that comprises the values of previous nodes u(j)\n, j < i, with j \u2208 Pa(u(i)\n). The\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu(1) to u(ni ) . The output of the computational graph is read off the last (output)\nnode u(n)\n.\nfor i = 1, . . . , ni do\nu(i) \u2190 xi\nend for\nfor i = ni + 1, . . . , n do\nA(i) \u2190 {u(j) | j \u2208 Pa(u(i) )}\nu(i) \u2190 f (i)\n(A(i)\n)\nend for\nreturn u(n)\nusing the chain rule with respect to scalar output u(n)\n:\n\u2202u(n)\n\u2202u(j) = \ue058\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u(i)\n\u2202u(j) (6.49)\nas specified by algorithm 6.2. The subgraph B contains exactly one edge for each\nedge from node u(j) to node u(i) of G. The edge from u(j) to u(i) is associated with\nthe computation of \u2202u(i)\n\u2202u(j) . In addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u(i) that are children\nof u(j) and the vector containing the partial derivatives \u2202u(i)\n\u2202u(j) for the same children\nnodes u(i) . To summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in G, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations.\nThe back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. Specifically, it performs on the order\nof one Jacobian product per node in the graph. This can be seen from the fact\nthat backprop (algorithm 6.2) visits each edge from node u(j) to node u(i) of\nthe graph exactly once in order to obtain the associated partial derivative \u2202u(i)\n\u2202u(j) .\n209\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.2 Simplified version of the back-propagation algorithm for computing\nthe derivatives of u(n) with respect to the variables in the graph. This example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u(1), . . . , u(ni )\n.\nThis simplified version computes the derivatives of all nodes in the graph. The\ncomputational cost of this algorithm is proportional to the number of edges in\nthe graph, assuming that the partial derivative associated with each edge requires\na constant time. This is of the same order as the number of computations for\nthe forward propagation. Each \u2202u(i)\n\u2202u(j) is a function of the parents u(j) of u(i)\n, thus\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\nRun forward propagation (algorithm 6.1 for this example) to obtain the activations of the network\nInitialize grad_table, a data structure that will store the derivatives that have\nbeen computed. The entry grad_table[u(i)\n] will store the computed value of\n\u2202u(n)\n\u2202u(i) .\ngrad_table[u(n)\n] \u2190 1\nfor j = n \u2212 1 down to 1 do\nThe next line computes \u2202u(n)\n\u2202u(j) = \ue050\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u (i)\n\u2202u(j) using stored values:\ngrad_table[u(j) ] \u2190 \ue050\ni:j\u2208Pa(u(i)) grad_table[u(i) ] \u2202u(i)\n\u2202u(j)\nend for\nreturn {grad_table[u(i)\n] | i = 1, . . . , ni}\nBack-propagation thus avoids the exponential explosion in repeated subexpressions.\nHowever, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. We will revisit these ideas\nafter describing the back-propagation algorithm itself.\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\nTo clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer MLP.\nAlgorithm 6.3 first shows the forward propagation, which maps parameters to\nthe supervised loss L(y\u02c6, y) associated with a single (input,target) training example\n(x, y), with \u02c6y the output of the neural network when x is provided in input.\nAlgorithm 6.4 then shows the corresponding computation to be done for\n210\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nFigure 6.9: A computational graph that results in repeated subexpressions when computing\nthe gradient. Let w \u2208 R be the input to the graph. We use the same function f : R \u2192 R\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f( y).\nTo compute \u2202z\n\u2202w , we apply equation 6.44 and obtain:\n\u2202z\n\u2202w (6.50)\n= \u2202z\n\u2202y\n\u2202y\n\u2202x\n\u2202x\n\u2202w (6.51)\n=f\ue030\n(y)f\ue030\n(x)f\ue030\n(w) (6.52)\n=f\ue030\n(f(f(w)))f \ue030\n(f(w))f\ue030\n(w) (6.53)\nEquation 6.52 suggests an implementation in which we compute the value off (w) only\nonce and store it in the variable x. This is the approach taken by the back-propagation\nalgorithm. An alternative approach is suggested by equation 6.53, where the subexpression\nf(w) appears more than once. In the alternative approach, f(w) is recomputed each time\nit is needed. When the memory required to store the value of these expressions is low, the\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\nuseful when memory is limited.\n211\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napplying the back-propagation algorithm to this graph.\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\nstraightforward to understand. However, they are specialized to one specific\nproblem.\nModern software implementations are based on the generalized form of backpropagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\nAlgorithm 6.3 Forward propagation through a typical deep neural network and\nthe computation of the cost function. The loss L(y\u02c6, y) depends on the output\ny\u02c6 and on the target y (see section 6.2.1.1 for examples of loss functions). To\nobtain the total cost J, the loss may be added to a regularizer \u2126(\u03b8), where \u03b8\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\ncompute gradients of J with respect to parameters W and b. For simplicity, this\ndemonstration uses only a single input example x. Practical applications should\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\nRequire: Network depth, l\nRequire: W(i)\n,i \u2208 {1, . . . , l}, the weight matrices of the model\nRequire: b(i) ,i \u2208 {1, . . . , l}, the bias parameters of the model\nRequire: x, the input to process\nRequire: y, the target output\nh(0) = x\nfor k = 1, . . . , l do\na(k) = b(k) + W(k)h(k\u22121)\nh(k) = f(a(k)\n)\nend for\ny\u02c6 = h(l)\nJ = L(y\u02c6, y) + \u03bb\u2126(\u03b8)\n6.5.5 Symbol-to-Symbol Derivatives\nAlgebraic expressions and computational graphs both operate on symbols, or\nvariables that do not have specific values. These algebraic and graph-based\nrepresentations are called symbolic representations. When we actually use or\ntrain a neural network, we must assign specific values to these symbols. We\nreplace a symbolic input to the network x with a specific numeric value, such as\n[1.2, 3.765, \u22121.8]\ue03e.\n212\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation\nyields the gradients on the activations a(k) for each layer k, starting from the\noutput layer and going backwards to the first hidden layer. From these gradients,\nwhich can be interpreted as an indication of how each layer\u2019s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. The\ngradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods.\nAfter the forward computation, compute the gradient on the output layer:\ng \u2190 \u2207y\u02c6J = \u2207y\u02c6L(y\u02c6, y)\nfor k = l, l \u2212 1, . . . , 1 do\nConvert the gradient on the layer\u2019s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\ng \u2190 \u2207a(k)J = g \ue00c f\ue030\n(a(k)\n)\nCompute gradients on weights and biases (including the regularization term,\nwhere needed):\n\u2207b(k)J = g + \u03bb\u2207b(k)\u2126(\u03b8)\n\u2207W (k)J = g h(k\u22121)\ue03e + \u03bb\u2207W (k)\u2126(\u03b8)\nPropagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations:\ng \u2190 \u2207h(k\u22121) J = W(k)\ue03e g\nend for\n213\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nz\nx\ny\nw\nf\nf\nf\ndz\ndy\nf \ue021\ndy\ndx\nf \ue021\ndz\ndx\n\u00d7\ndx\ndw\nf \ue021\ndz\ndw\n\u00d7\nFigure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\nto compute these derivatives. A generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\nrepresenting z = f (f(f (w))). (Right)We run the back-propagation algorithm, instructing\nit to construct the graph for the expression corresponding to dz\ndw . In this example, we do\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\nSome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. We call this approach \u201csymbolto-number\u201d differentiation. This is the approach used by libraries such as Torch\n(Collobert et al., 2011b) and Caffe (Jia, 2013).\nAnother approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. This\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\nis illustrated in figure 6.10. The primary advantage of this approach is that\nthe derivatives are described in the same language as the original expression.\nBecause the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. Computation of higher-order derivatives is described in section 6.5.10.\nWe will use the latter approach and describe the back-propagation algorithm in\n214\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nterms of constructing a computational graph for the derivatives. Any subset of the\ngraph may then be evaluated using specific numerical values at a later time. This\nallows us to avoid specifying exactly when each operation should be computed.\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents\u2019 values are available.\nThe description of the symbol-to-symbol based approach subsumes the symbolto-number approach. The symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\napproach does not expose the graph.\n6.5.6 General Back-Propagation\nThe back-propagation algorithm is very simple. To compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by dz\ndz = 1. We can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the Jacobian of the operation that produced z. We continue\nmultiplying by Jacobians traveling backwards through the graph in this way until\nwe reach x. For any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\nMore formally, each node in the graph G corresponds to a variable. To achieve\nmaximum generality, we describe this variable as being a tensor V. Tensor can\nin general have any number of dimensions. They subsume scalars, vectors, and\nmatrices.\nWe assume that each variable V is associated with the following subroutines:\n\u2022 get_operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,\nthere may be a Python or C++ class representing the matrix multiplication\noperation, and the get_operation function. Suppose we have a variable that\nis created by matrix multiplication, C = AB. Then get_operation(V)\nreturns a pointer to an instance of the corresponding C++ class.\n\u2022 get_consumers(V, G): This returns the list of variables that are children of\nV in the computational graph G.\n\u2022 get_inputs(V, G): This returns the list of variables that are parents of V\nin the computational graph G.\n215\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nEach operation op is also associated with a bprop operation. This bprop\noperation can compute a Jacobian-vector product as described by equation 6.47.\nThis is how the back-propagation algorithm is able to achieve great generality.\nEach operation is responsible for knowing how to back-propagate through the\nedges in the graph that it participates in. For example, we might use a matrix\nmultiplication operation to create a variable C = AB. Suppose that the gradient\nof a scalar z with respect to C is given by G. The matrix multiplication operation\nis responsible for defining two back-propagation rules, one for each of its input\narguments. If we call the bprop method to request the gradient with respect to\nA given that the gradient on the output is G, then the bprop method of the\nmatrix multiplication operation must state that the gradient with respect to A\nis given by GB\ue03e. Likewise, if we call the bprop method to request the gradient\nwith respect to B, then the matrix operation is responsible for implementing the\nbprop method and specifying that the desired gradient is given by A\ue03eG. The\nback-propagation algorithm itself does not need to know any differentiation rules. It\nonly needs to call each operation\u2019s bprop rules with the right arguments. Formally,\nop.bprop(inputs, X, G) must return\n\ue058\ni\n(\u2207 Xop.f(inputs)i) Gi, (6.54)\nwhich is just an implementation of the chain rule as expressed in equation 6.47.\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements,X is the input whose gradient\nwe wish to compute, and G is the gradient on the output of the operation.\nThe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. For example, if the mul operator is passed\ntwo copies of x to compute x2, the op.bprop method should still return x as the\nderivative with respect to both inputs. The back-propagation algorithm will later\nadd both of these arguments together to obtain 2x, which is the correct total\nderivative on x.\nSoftware implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually.\nThe back-propagation algorithm is formally described in algorithm 6.5.\n216\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\nportion does simple setup and cleanup work. Most of the important work happens\nin the build_grad subroutine of algorithm 6.6 .\nRequire: T, the target set of variables whose gradients must be computed.\nRequire: G, the computational graph\nRequire: z, the variable to be differentiated\nLet G\ue030 be G pruned to contain only nodes that are ancestors of z and descendents\nof nodes in T.\nInitialize grad_table, a data structure associating tensors to their gradients\ngrad_table[z] \u2190 1\nfor V in T do\nbuild_grad(V, G, G\ue030 , grad_table)\nend for\nReturn grad_table restricted to T\nIn section 6.5.2, we explained that back-propagation was developed in order to\navoid computing the same subexpression in the chain rule multiple times. The naive\nalgorithm could have exponential runtime due to these repeated subexpressions.\nNow that we have specified the back-propagation algorithm, we can understand its\ncomputational cost. If we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed. Keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\nmultiplication as a single operation). Computing a gradient in a graph with n nodes\nwill never execute more than O(n2) operations or store the output of more than\nO(n2) operations. Here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. For example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. We can see that computing the gradient requires as\nmost O(n2) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). The back-propagation algorithm\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\nedge in the original graph. Because the computational graph is a directed acyclic\ngraph it has at most O(n2 ) edges. For the kinds of graphs that are commonly used\nin practice, the situation is even better. Most neural network cost functions are\n217\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.6 The inner loop subroutine build_grad(V, G, G\ue030\n, grad_table) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm 6.5.\nRequire: V, the variable whose gradient should be added to G and grad_table.\nRequire: G, the graph to modify.\nRequire: G\ue030\n, the restriction of G to nodes that participate in the gradient.\nRequire: grad_table, a data structure mapping nodes to their gradients\nif V is in grad_table then\nReturn grad_table[V]\nend if\ni \u2190 1\nfor C in get_consumers(V, G\ue030\n) do\nop \u2190 get_operation\u00a9\nD \u2190 build_grad(C, G,G\ue030\n, grad_table)\nG(i) \u2190 op.bprop(get_inputs(C, G\ue030\n), V, D)\ni \u2190 i + 1\nend for\nG \u2190\ue050\ni G(i)\ngrad_table[V] = G\nInsert G and the operations creating it into G\nReturn G\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. This potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (equation 6.49) non-recursively:\n\u2202u(n)\n\u2202u(j) = \ue058\npath (u(\u03c01),u(\u03c02) ,\u2026,u(\u03c0t) ),\nfrom \u03c01=j to \u03c0t=n\n\ue059\nt\nk=2\n\u2202u(\u03c0k)\n\u2202u(\u03c0k\u22121 )\n. (6.55)\nSince the number of paths from node j to node n can grow exponentially in the\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. This large cost would be incurred because the same computation for\n\u2202u(i)\n\u2202u(j) would be redone many times. To avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results \u2202u(n)\n\u2202u(i) . Each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. By filling in these table entries in order,\n218\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nback-propagation avoids repeating many common subexpressions. This table-filling\nstrategy is sometimes called dynamic programming.\n6.5.7 Example: Back-Propagation for MLP Training\nAs an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\nHere we develop a very simple multilayer perception with a single hidden\nlayer. To train this model, we will use minibatch stochastic gradient descent.\nThe back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. Specifically, we use a minibatch of examples from the training\nset formatted as a design matrix X and a vector of associated class labels y.\nThe network computes a layer of hidden features H = max{0, XW(1)}. To\nsimplify the presentation we do not use biases in this model. We assume that our\ngraph language includes a relu operation that can compute max{0, Z} elementwise. The predictions of the unnormalized log probabilities over classes are then\ngiven by HW (2). We assume that our graph language includes a cross_entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. The resulting crossentropy defines the cost JMLE. Minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. However, to make this example more realistic,\nwe also include a regularization term. The total cost\nJ = JMLE + \u03bb\n\uf8eb\n\uf8ed\ue058\ni,j\n\ue010\nW (1)\ni,j \ue0112\n+ \ue058\ni,j\n\ue010\nW(2)\ni,j \ue0112\n\uf8f6\n\uf8f8 (6.56)\nconsists of the cross-entropy and a weight decay term with coefficient \u03bb. The\ncomputational graph is illustrated in figure 6.11.\nThe computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. This demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually.\nWe can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in figure 6.11. To train, we wish\nto compute both \u2207W (1)J and \u2207W (2)J. There are two different paths leading\nbackward from J to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\nalways contribute 2\u03bbW(i) to the gradient on W(i) .\n219\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nX W (1)\nU (1)\nmatmul\nH\nrelu\nU (3)\nsqr u(4)\nsum\nW (2) u(7) \u03bb\nU (2)\nmatmul\ny\nJMLE\ncross_entropy\nU (5)\nsqr u(6)\nsum\nu(8)\nJ\n+\n\u00d7\n+\nFigure 6.11: The computational graph used to compute the cost used to train our example\nof a single-layer MLP using the cross-entropy loss and weight decay.\nThe other path through the cross-entropy cost is slightly more complicated.\nLet G be the gradient on the unnormalized log probabilities U(2) provided by\nthe cross_entropy operation. The back-propagation algorithm now needs to\nexplore two different branches. On the shorter branch, it adds H \ue03eG to the\ngradient on W(2) , using the back-propagation rule for the second argument to\nthe matrix multiplication operation. The other branch corresponds to the longer\nchain descending further along the network. First, the back-propagation algorithm\ncomputes \u2207HJ = GW(2)\ue03e using the back-propagation rule for the first argument\nto the matrix multiplication operation. Next, the relu operation uses its backpropagation rule to zero out components of the gradient corresponding to entries\nof U(1) that were less than 0. Let the result be called G\ue030 . The last step of the\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the matmul operation to add X\ue03eG\ue030 to the gradient on W(1).\nAfter these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters.\nFor the MLP, the computational cost is dominated by the cost of matrix\nmultiplication. During the forward propagation stage, we multiply by each weight\n220\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost. The main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\nThis value is stored from the time it is computed until the backward pass has\nreturned to the same point. The memory cost is thus O(mnh), where m is the\nnumber of examples in the minibatch and nh is the number of hidden units.\n\u201d\nMathematically explain the above basic principle of back-propagation for neural network training with step by step equations , try to use your own words, do not directly copy the sentence from the above paper."}, {"role": "assistant", "content": "Back-propagation is a foundational algorithm in training neural networks and understanding how it computes gradients step by step is essential for grasping how learning occurs. Let\u2019s break it down into simpler terms and equations, focusing on a basic feedforward neural network for clarity.\n\n1. **Forward Propagation**:\n   First, data flows forward through the network. For a network with a single hidden layer, the process involves computing the hidden layer activations (H) and then the output (Y\u0302) predictions.\n\n   Given an input `X` and parameters (weights `W` and biases `b`), the activations `a` for the hidden layer are computed as:\n   \\[a^{[1]} = XW^{[1]} + b^{[1]}\\]\n   The hidden layer outputs `H` are then obtained by applying a non-linearity (`f`, e.g., ReLU) to these activations:\n   \\[H = f(a^{[1]})\\]\n   Then we calculate the output layer's pre-activation features:\n   \\[a^{[2]} = HW^{[2]} + b^{[2]}\\]\n   And, apply a suitable activation function (e.g., softmax for classification) to get the final prediction `Y\u0302`:\n   \\[Y\u0302 = f(a^{[2]})\\]\n\n2. **Cost Function Calculation**:\n   A cost function `J(\u03b8)` measures how far off our predictions are from the true targets. For classification problems, a common choice is the cross-entropy loss.\n\n3. **Backward Propagation**:\n   The crux of back-propagation is to calculate how the cost function changes with respect to the parameters of the network (`W` and `b`). This is where the chain rule of calculus plays a crucial role.\n\n   For a scalar cost `J` and layers numbered `[l]`, we compute gradients starting from the output back to the inputs. The process unfolds in reverse to forward propagation:\n   \n   - **Output to Last Hidden Layer**: Compute the gradient of the cost with respect to the output layer\u2019s activation's pre-activation value `a^{[2]}`:\n     \\[\\frac{\\partial J}{\\partial a^{[2]}} = \\frac{\\partial J}{\\partial Y\u0302} \\cdot \\frac{\\partial Y\u0302}{\\partial a^{[2]}}\\]\n   - **Gradients of Weights and Biases (Last layer)**: Then, translate this gradient through the layer to its parameters:\n     \\[\\frac{\\partial J}{\\partial W^{[2]}} = H^T\\frac{\\partial J}{\\partial a^{[2]}}\\]\n     \\[\\frac{\\partial J}{\\partial b^{[2]}} = \\frac{\\partial J}{\\partial a^{[2]}}\\]\n   Here, we use the notion that the gradient w.r.t. the weights can be seen as the outer product of the input activations and the gradient of the loss w.r.t. the layer outputs.\n\n   - **Hidden Layer Back to Inputs**: We then propagate the gradient back to the hidden layer outputs `H` and further back compute gradients w.r.t. its pre-activation `a^{[1]}`:\n     \\[\\frac{\\partial J}{\\partial H} = \\frac{\\partial J}{\\partial a^{[2]}}(W^{[2]})^T\\]\n     And apply the derivative of the non-linearity:\n     \\[\\frac{\\partial J}{\\partial a^{[1]}} = \\frac{\\partial J}{\\partial H} \\cdot f'(a^{[1]})\\]\n   - **Gradients of Weights and Biases (First layer)**: Finally, determine how much the parameters of the first layer contribute to the loss:\n     \\[\\frac{\\partial J}{\\partial W^{[1]}} = X^T\\frac{\\partial J}{\\partial a^{[1]}}\\]\n     \\[\\frac{\\partial J}{\\partial b^{[1]}} = \\frac{\\partial J}{\\partial a^{[1]}}\\]\n\n4. **Gradient Descent Update**:\n   With the gradients computed, we adjust the parameters in the direction that most reduces the loss, typically using an update rule like:\n   \\[W = W - \\alpha\\frac{\\partial J}{\\partial W}\\]\n   \\[b = b - \\alpha\\frac{\\partial J}{\\partial b}\\]\n   Where `\\alpha` is the learning rate, a small positive scalar determining the step size.\n\nThis process is iterated across many epochs, gradually lowering the cost function, and improving the model's performance in predicting the target from the inputs. The beauty of back-propagation lies in its general applicability. Regardless of the network size or complexity, the same fundamental principles allow for the efficient computation of gradients across potentially deep and intricate neural network architectures."}]}, {"facetValue": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed academic text about back-propagation and related concepts. The response involved processing and understanding complex technical content related to neural networks and computational graphs."}], "conversation": [{"role": "user", "content": "\u201c6.5 Back-Propagation and Other Differentiation Algorithms\nWhen we use a feedforward neural network to accept an input x and produce an\noutput y\u02c6, information flows forward through the network. The inputs x provide\nthe initial information that then propagates up to the hidden units at each layer\nand finally produces y\u02c6. This is called forward propagation. During training,\nforward propagation can continue onward until it produces a scalar cost J(\u03b8).\nThe back-propagation algorithm (Rumelhart et al., 1986a), often simply called\nbackprop, allows the information from the cost to then flow backwards through\nthe network, in order to compute the gradient.\nComputing an analytical expression for the gradient is straightforward, but\nnumerically evaluating such an expression can be computationally expensive. The\nback-propagation algorithm does so using a simple and inexpensive procedure.\nThe term back-propagation is often misunderstood as meaning the whole\nlearning algorithm for multi-layer neural networks. Actually, back-propagation\nrefers only to the method for computing the gradient, while another algorithm,\nsuch as stochastic gradient descent, is used to perform learning using this gradient.\nFurthermore, back-propagation is often misunderstood as being specific to multilayer neural networks, but in principle it can compute derivatives of any function\n(for some functions, the correct response is to report that the derivative of the\nfunction is undefined). Specifically, we will describe how to compute the gradient\n\u2207xf(x, y) for an arbitrary function f , wherex is a set of variables whose derivatives\nare desired, and y is an additional set of variables that are inputs to the function\nbut whose derivatives are not required. In learning algorithms, the gradient we most\noften require is the gradient of the cost function with respect to the parameters,\n\u2207\u03b8 J(\u03b8). Many machine learning tasks involve computing other derivatives, either\nas part of the learning process, or to analyze the learned model. The backpropagation algorithm can be applied to these tasks as well, and is not restricted\nto computing the gradient of the cost function with respect to the parameters. The\nidea of computing derivatives by propagating information through a network is\nvery general, and can be used to compute values such as the Jacobian of a function\nf with multiple outputs. We restrict our description here to the most commonly\nused case where f has a single output.\n204\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.5.1 Computational Graphs\nSo far we have discussed neural networks with a relatively informal graph language.\nTo describe the back-propagation algorithm more precisely, it is helpful to have a\nmore precise computational graph language.\nMany ways of formalizing computation as graphs are possible.\nHere, we use each node in the graph to indicate a variable. The variable may\nbe a scalar, vector, matrix, tensor, or even a variable of another type.\nTo formalize our graphs, we also need to introduce the idea of an operation.\nAn operation is a simple function of one or more variables. Our graph language\nis accompanied by a set of allowable operations. Functions more complicated\nthan the operations in this set may be described by composing many operations\ntogether.\nWithout loss of generality, we define an operation to return only a single\noutput variable. This does not lose generality because the output variable can have\nmultiple entries, such as a vector. Software implementations of back-propagation\nusually support operations with multiple outputs, but we avoid this case in our\ndescription because it introduces many extra details that are not important to\nconceptual understanding.\nIf a variable y is computed by applying an operation to a variable x, then\nwe draw a directed edge from x to y. We sometimes annotate the output node\nwith the name of the operation applied, and other times omit this label when the\noperation is clear from context.\nExamples of computational graphs are shown in figure 6.8.\n6.5.2 Chain Rule of Calculus\nThe chain rule of calculus (not to be confused with the chain rule of probability) is\nused to compute the derivatives of functions formed by composing other functions\nwhose derivatives are known. Back-propagation is an algorithm that computes the\nchain rule, with a specific order of operations that is highly efficient.\nLet x be a real number, and let f and g both be functions mapping from a real\nnumber to a real number. Suppose that y = g(x) and z = f (g(x)) = f (y). Then\nthe chain rule states that\ndz\ndx = dz\ndy\ndy\ndx . (6.44)\nWe can generalize this beyond the scalar case. Suppose that x \u2208 Rm, y \u2208 Rn,\n205\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx y\n(a)\n\u00d7\nx w\n(b)\nu(1)\ndot\nb\nu(2)\n+\ny\u02c6\n\u03c3\n\u00a9\nX W\nU (1)\nmatmul\nb\nU (2)\n+\nH\nrelu\nx w\n(d)\ny\u02c6\ndot\n\u03bb\nu(1)\nsqr\nu(2)\nsum\nu(3)\n\u00d7\nFigure 6.8: Examples of computational graphs. (a)The graph using the\u00d7 operation to\ncompute z = xy. (b)The graph for the logistic regression prediction y\u02c6 = \u03c3\n\ue000\nx\ue03ew + b\n\ue001\n.\nSome of the intermediate expressions do not have names in the algebraic expression\nbut need names in the graph. We simply name the i-th such variable u(i)\n. \u00a9The\ncomputational graph for the expression H = max{0,XW + b}, which computes a design\nmatrix of rectified linear unit activations H given a design matrix containing a minibatch\nof inputs X . (d)Examples a\u2013c applied at most one operation to each variable, but it\nis possible to apply more than one operation. Here we show a computation graph that\napplies more than one operation to the weights w of a linear regression model. The\nweights are used to make both the prediction y\u02c6 and the weight decay penalty \u03bb\n\ue050\ni w2\ni .\n206\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ng maps from Rm to Rn, and f maps from Rn to R. If y = g(x) and z = f(y), then\n\u2202z\n\u2202xi\n= \ue058\nj\n\u2202z\n\u2202yj\n\u2202yj\n\u2202xi\n. (6.45)\nIn vector notation, this may be equivalently written as\n\u2207xz =\n\ue012 \u2202y\n\u2202x\n\ue013\ue03e\n\u2207y z, (6.46)\nwhere \u2202y\n\u2202x is the n \u00d7 m Jacobian matrix of g.\nFrom this we see that the gradient of a variable x can be obtained by multiplying\na Jacobian matrix \u2202y\n\u2202x by a gradient \u2207yz. The back-propagation algorithm consists\nof performing such a Jacobian-gradient product for each operation in the graph.\nUsually we do not apply the back-propagation algorithm merely to vectors,\nbut rather to tensors of arbitrary dimensionality. Conceptually, this is exactly the\nsame as back-propagation with vectors. The only difference is how the numbers\nare arranged in a grid to form a tensor. We could imagine flattening each tensor\ninto a vector before we run back-propagation, computing a vector-valued gradient,\nand then reshaping the gradient back into a tensor. In this rearranged view,\nback-propagation is still just multiplying Jacobians by gradients.\nTo denote the gradient of a value z with respect to a tensor X, we write \u2207Xz,\njust as if X were a vector. The indices into X now have multiple coordinates\u2014for\nexample, a 3-D tensor is indexed by three coordinates. We can abstract this away\nby using a single variable i to represent the complete tuple of indices. For all\npossible index tuples i, (\u2207Xz)i gives \u2202z\n\u2202Xi\n. This is exactly the same as how for all\npossible integer indices i into a vector, (\u2207x z)i gives \u2202z\n\u2202xi\n. Using this notation, we\ncan write the chain rule as it applies to tensors. If Y = g(X) and z = f(Y), then\n\u2207X z =\ue058\nj\n(\u2207XYj ) \u2202z\n\u2202Yj\n. (6.47)\n6.5.3 Recursively Applying the Chain Rule to Obtain Backprop\nUsing the chain rule, it is straightforward to write down an algebraic expression for\nthe gradient of a scalar with respect to any node in the computational graph that\nproduced that scalar. However, actually evaluating that expression in a computer\nintroduces some extra considerations.\nSpecifically, many subexpressions may be repeated several times within the\noverall expression for the gradient. Any procedure that computes the gradient\n207\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nwill need to choose whether to store these subexpressions or to recompute them\nseveral times. An example of how these repeated subexpressions arise is given in\nfigure 6.9. In some cases, computing the same subexpression twice would simply\nbe wasteful. For complicated graphs, there can be exponentially many of these\nwasted computations, making a naive implementation of the chain rule infeasible.\nIn other cases, computing the same subexpression twice could be a valid way to\nreduce memory consumption at the cost of higher runtime.\nWe first begin by a version of the back-propagation algorithm that specifies the\nactual gradient computation directly (algorithm 6.2 along with algorithm 6.1 for the\nassociated forward computation), in the order it will actually be done and according\nto the recursive application of chain rule. One could either directly perform these\ncomputations or view the description of the algorithm as a symbolic specification\nof the computational graph for computing the back-propagation. However, this\nformulation does not make explicit the manipulation and the construction of the\nsymbolic graph that performs the gradient computation. Such a formulation is\npresented below in section 6.5.6, with algorithm 6.5, where we also generalize to\nnodes that contain arbitrary tensors.\nFirst consider a computational graph describing how to compute a single scalar\nu(n) (say the loss on a training example). This scalar is the quantity whose\ngradient we want to obtain, with respect to the ni input nodes u(1) to u(ni). In\nother words we wish to compute \u2202u(n)\n\u2202u(i) for all i \u2208 {1, 2, . . . , ni}. In the application\nof back-propagation to computing gradients for gradient descent over parameters,\nu(n) will be the cost associated with an example or a minibatch, while u(1) to u(ni)\ncorrespond to the parameters of the model.\nWe will assume that the nodes of the graph have been ordered in such a way\nthat we can compute their output one after the other, starting at u(ni+1) and\ngoing up to u(n)\n. As defined in algorithm 6.1, each node u(i) is associated with an\noperation f(i) and is computed by evaluating the function\nu(i) = f(A(i)\n) (6.48)\nwhere A(i) is the set of all nodes that are parents of u(i)\n.\nThat algorithm specifies the forward propagation computation, which we could\nput in a graph G. In order to perform back-propagation, we can construct a\ncomputational graph that depends onG and adds to it an extra set of nodes. These\nform a subgraph B with one node per node of G. Computation in B proceeds in\nexactly the reverse of the order of computation in G, and each node of B computes\nthe derivative \u2202u(n)\n\u2202u(i) associated with the forward graph node u(i)\n. This is done\n208\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.1 A procedure that performs the computations mapping ni inputs\nu(1) to u(ni) to an output u(n)\n. This defines a computational graph where each node\ncomputes numerical value u(i) by applying a function f (i) to the set of arguments\nA(i) that comprises the values of previous nodes u(j)\n, j < i, with j \u2208 Pa(u(i)\n). The\ninput to the computational graph is the vector x, and is set into the first ni nodes\nu(1) to u(ni ) . The output of the computational graph is read off the last (output)\nnode u(n)\n.\nfor i = 1, . . . , ni do\nu(i) \u2190 xi\nend for\nfor i = ni + 1, . . . , n do\nA(i) \u2190 {u(j) | j \u2208 Pa(u(i) )}\nu(i) \u2190 f (i)\n(A(i)\n)\nend for\nreturn u(n)\nusing the chain rule with respect to scalar output u(n)\n:\n\u2202u(n)\n\u2202u(j) = \ue058\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u(i)\n\u2202u(j) (6.49)\nas specified by algorithm 6.2. The subgraph B contains exactly one edge for each\nedge from node u(j) to node u(i) of G. The edge from u(j) to u(i) is associated with\nthe computation of \u2202u(i)\n\u2202u(j) . In addition, a dot product is performed for each node,\nbetween the gradient already computed with respect to nodes u(i) that are children\nof u(j) and the vector containing the partial derivatives \u2202u(i)\n\u2202u(j) for the same children\nnodes u(i) . To summarize, the amount of computation required for performing\nthe back-propagation scales linearly with the number of edges in G, where the\ncomputation for each edge corresponds to computing a partial derivative (of one\nnode with respect to one of its parents) as well as performing one multiplication\nand one addition. Below, we generalize this analysis to tensor-valued nodes, which\nis just a way to group multiple scalar values in the same node and enable more\nefficient implementations.\nThe back-propagation algorithm is designed to reduce the number of common\nsubexpressions without regard to memory. Specifically, it performs on the order\nof one Jacobian product per node in the graph. This can be seen from the fact\nthat backprop (algorithm 6.2) visits each edge from node u(j) to node u(i) of\nthe graph exactly once in order to obtain the associated partial derivative \u2202u(i)\n\u2202u(j) .\n209\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.2 Simplified version of the back-propagation algorithm for computing\nthe derivatives of u(n) with respect to the variables in the graph. This example is\nintended to further understanding by showing a simplified case where all variables\nare scalars, and we wish to compute the derivatives with respect to u(1), . . . , u(ni )\n.\nThis simplified version computes the derivatives of all nodes in the graph. The\ncomputational cost of this algorithm is proportional to the number of edges in\nthe graph, assuming that the partial derivative associated with each edge requires\na constant time. This is of the same order as the number of computations for\nthe forward propagation. Each \u2202u(i)\n\u2202u(j) is a function of the parents u(j) of u(i)\n, thus\nlinking the nodes of the forward graph to those added for the back-propagation\ngraph.\nRun forward propagation (algorithm 6.1 for this example) to obtain the activations of the network\nInitialize grad_table, a data structure that will store the derivatives that have\nbeen computed. The entry grad_table[u(i)\n] will store the computed value of\n\u2202u(n)\n\u2202u(i) .\ngrad_table[u(n)\n] \u2190 1\nfor j = n \u2212 1 down to 1 do\nThe next line computes \u2202u(n)\n\u2202u(j) = \ue050\ni:j\u2208Pa(u(i) )\n\u2202u(n)\n\u2202u(i)\n\u2202u (i)\n\u2202u(j) using stored values:\ngrad_table[u(j) ] \u2190 \ue050\ni:j\u2208Pa(u(i)) grad_table[u(i) ] \u2202u(i)\n\u2202u(j)\nend for\nreturn {grad_table[u(i)\n] | i = 1, . . . , ni}\nBack-propagation thus avoids the exponential explosion in repeated subexpressions.\nHowever, other algorithms may be able to avoid more subexpressions by performing\nsimplifications on the computational graph, or may be able to conserve memory by\nrecomputing rather than storing some subexpressions. We will revisit these ideas\nafter describing the back-propagation algorithm itself.\n6.5.4 Back-Propagation Computation in Fully-Connected MLP\nTo clarify the above definition of the back-propagation computation, let us consider\nthe specific graph associated with a fully-connected multi-layer MLP.\nAlgorithm 6.3 first shows the forward propagation, which maps parameters to\nthe supervised loss L(y\u02c6, y) associated with a single (input,target) training example\n(x, y), with \u02c6y the output of the neural network when x is provided in input.\nAlgorithm 6.4 then shows the corresponding computation to be done for\n210\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nFigure 6.9: A computational graph that results in repeated subexpressions when computing\nthe gradient. Let w \u2208 R be the input to the graph. We use the same function f : R \u2192 R\nas the operation that we apply at every step of a chain: x = f(w), y = f(x), z = f( y).\nTo compute \u2202z\n\u2202w , we apply equation 6.44 and obtain:\n\u2202z\n\u2202w (6.50)\n= \u2202z\n\u2202y\n\u2202y\n\u2202x\n\u2202x\n\u2202w (6.51)\n=f\ue030\n(y)f\ue030\n(x)f\ue030\n(w) (6.52)\n=f\ue030\n(f(f(w)))f \ue030\n(f(w))f\ue030\n(w) (6.53)\nEquation 6.52 suggests an implementation in which we compute the value off (w) only\nonce and store it in the variable x. This is the approach taken by the back-propagation\nalgorithm. An alternative approach is suggested by equation 6.53, where the subexpression\nf(w) appears more than once. In the alternative approach, f(w) is recomputed each time\nit is needed. When the memory required to store the value of these expressions is low, the\nback-propagation approach of equation 6.52 is clearly preferable because of its reduced\nruntime. However, equation 6.53 is also a valid implementation of the chain rule, and is\nuseful when memory is limited.\n211\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\napplying the back-propagation algorithm to this graph.\nAlgorithms 6.3 and 6.4 are demonstrations that are chosen to be simple and\nstraightforward to understand. However, they are specialized to one specific\nproblem.\nModern software implementations are based on the generalized form of backpropagation described in section 6.5.6 below, which can accommodate any computational graph by explicitly manipulating a data structure for representing symbolic\ncomputation.\nAlgorithm 6.3 Forward propagation through a typical deep neural network and\nthe computation of the cost function. The loss L(y\u02c6, y) depends on the output\ny\u02c6 and on the target y (see section 6.2.1.1 for examples of loss functions). To\nobtain the total cost J, the loss may be added to a regularizer \u2126(\u03b8), where \u03b8\ncontains all the parameters (weights and biases). Algorithm 6.4 shows how to\ncompute gradients of J with respect to parameters W and b. For simplicity, this\ndemonstration uses only a single input example x. Practical applications should\nuse a minibatch. See section 6.5.7 for a more realistic demonstration.\nRequire: Network depth, l\nRequire: W(i)\n,i \u2208 {1, . . . , l}, the weight matrices of the model\nRequire: b(i) ,i \u2208 {1, . . . , l}, the bias parameters of the model\nRequire: x, the input to process\nRequire: y, the target output\nh(0) = x\nfor k = 1, . . . , l do\na(k) = b(k) + W(k)h(k\u22121)\nh(k) = f(a(k)\n)\nend for\ny\u02c6 = h(l)\nJ = L(y\u02c6, y) + \u03bb\u2126(\u03b8)\n6.5.5 Symbol-to-Symbol Derivatives\nAlgebraic expressions and computational graphs both operate on symbols, or\nvariables that do not have specific values. These algebraic and graph-based\nrepresentations are called symbolic representations. When we actually use or\ntrain a neural network, we must assign specific values to these symbols. We\nreplace a symbolic input to the network x with a specific numeric value, such as\n[1.2, 3.765, \u22121.8]\ue03e.\n212\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.4 Backward computation for the deep neural network of algorithm 6.3, which uses in addition to the input x a target y. This computation\nyields the gradients on the activations a(k) for each layer k, starting from the\noutput layer and going backwards to the first hidden layer. From these gradients,\nwhich can be interpreted as an indication of how each layer\u2019s output should change\nto reduce error, one can obtain the gradient on the parameters of each layer. The\ngradients on weights and biases can be immediately used as part of a stochastic gradient update (performing the update right after the gradients have been\ncomputed) or used with other gradient-based optimization methods.\nAfter the forward computation, compute the gradient on the output layer:\ng \u2190 \u2207y\u02c6J = \u2207y\u02c6L(y\u02c6, y)\nfor k = l, l \u2212 1, . . . , 1 do\nConvert the gradient on the layer\u2019s output into a gradient into the prenonlinearity activation (element-wise multiplication if f is element-wise):\ng \u2190 \u2207a(k)J = g \ue00c f\ue030\n(a(k)\n)\nCompute gradients on weights and biases (including the regularization term,\nwhere needed):\n\u2207b(k)J = g + \u03bb\u2207b(k)\u2126(\u03b8)\n\u2207W (k)J = g h(k\u22121)\ue03e + \u03bb\u2207W (k)\u2126(\u03b8)\nPropagate the gradients w.r.t. the next lower-level hidden layer\u2019s activations:\ng \u2190 \u2207h(k\u22121) J = W(k)\ue03e g\nend for\n213\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nz\nx\ny\nw\nf\nf\nf\nz\nx\ny\nw\nf\nf\nf\ndz\ndy\nf \ue021\ndy\ndx\nf \ue021\ndz\ndx\n\u00d7\ndx\ndw\nf \ue021\ndz\ndw\n\u00d7\nFigure 6.10: An example of the symbol-to-symbol approach to computing derivatives. In\nthis approach, the back-propagation algorithm does not need to ever access any actual\nspecific numeric values. Instead, it adds nodes to a computational graph describing how\nto compute these derivatives. A generic graph evaluation engine can later compute the\nderivatives for any specific numeric values. (Left)In this example, we begin with a graph\nrepresenting z = f (f(f (w))). (Right)We run the back-propagation algorithm, instructing\nit to construct the graph for the expression corresponding to dz\ndw . In this example, we do\nnot explain how the back-propagation algorithm works. The purpose is only to illustrate\nwhat the desired result is: a computational graph with a symbolic description of the\nderivative.\nSome approaches to back-propagation take a computational graph and a set\nof numerical values for the inputs to the graph, then return a set of numerical\nvalues describing the gradient at those input values. We call this approach \u201csymbolto-number\u201d differentiation. This is the approach used by libraries such as Torch\n(Collobert et al., 2011b) and Caffe (Jia, 2013).\nAnother approach is to take a computational graph and add additional nodes\nto the graph that provide a symbolic description of the desired derivatives. This\nis the approach taken by Theano (Bergstra et al., 2010; Bastien et al., 2012)\nand TensorFlow (Abadi et al., 2015). An example of how this approach works\nis illustrated in figure 6.10. The primary advantage of this approach is that\nthe derivatives are described in the same language as the original expression.\nBecause the derivatives are just another computational graph, it is possible to run\nback-propagation again, differentiating the derivatives in order to obtain higher\nderivatives. Computation of higher-order derivatives is described in section 6.5.10.\nWe will use the latter approach and describe the back-propagation algorithm in\n214\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nterms of constructing a computational graph for the derivatives. Any subset of the\ngraph may then be evaluated using specific numerical values at a later time. This\nallows us to avoid specifying exactly when each operation should be computed.\nInstead, a generic graph evaluation engine can evaluate every node as soon as its\nparents\u2019 values are available.\nThe description of the symbol-to-symbol based approach subsumes the symbolto-number approach. The symbol-to-number approach can be understood as\nperforming exactly the same computations as are done in the graph built by the\nsymbol-to-symbol approach. The key difference is that the symbol-to-number\napproach does not expose the graph.\n6.5.6 General Back-Propagation\nThe back-propagation algorithm is very simple. To compute the gradient of some\nscalar z with respect to one of its ancestors x in the graph, we begin by observing\nthat the gradient with respect to z is given by dz\ndz = 1. We can then compute\nthe gradient with respect to each parent of z in the graph by multiplying the\ncurrent gradient by the Jacobian of the operation that produced z. We continue\nmultiplying by Jacobians traveling backwards through the graph in this way until\nwe reach x. For any node that may be reached by going backwards from z through\ntwo or more paths, we simply sum the gradients arriving from different paths at\nthat node.\nMore formally, each node in the graph G corresponds to a variable. To achieve\nmaximum generality, we describe this variable as being a tensor V. Tensor can\nin general have any number of dimensions. They subsume scalars, vectors, and\nmatrices.\nWe assume that each variable V is associated with the following subroutines:\n\u2022 get_operation(V): This returns the operation that computes V, represented by the edges coming into V in the computational graph. For example,\nthere may be a Python or C++ class representing the matrix multiplication\noperation, and the get_operation function. Suppose we have a variable that\nis created by matrix multiplication, C = AB. Then get_operation(V)\nreturns a pointer to an instance of the corresponding C++ class.\n\u2022 get_consumers(V, G): This returns the list of variables that are children of\nV in the computational graph G.\n\u2022 get_inputs(V, G): This returns the list of variables that are parents of V\nin the computational graph G.\n215\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nEach operation op is also associated with a bprop operation. This bprop\noperation can compute a Jacobian-vector product as described by equation 6.47.\nThis is how the back-propagation algorithm is able to achieve great generality.\nEach operation is responsible for knowing how to back-propagate through the\nedges in the graph that it participates in. For example, we might use a matrix\nmultiplication operation to create a variable C = AB. Suppose that the gradient\nof a scalar z with respect to C is given by G. The matrix multiplication operation\nis responsible for defining two back-propagation rules, one for each of its input\narguments. If we call the bprop method to request the gradient with respect to\nA given that the gradient on the output is G, then the bprop method of the\nmatrix multiplication operation must state that the gradient with respect to A\nis given by GB\ue03e. Likewise, if we call the bprop method to request the gradient\nwith respect to B, then the matrix operation is responsible for implementing the\nbprop method and specifying that the desired gradient is given by A\ue03eG. The\nback-propagation algorithm itself does not need to know any differentiation rules. It\nonly needs to call each operation\u2019s bprop rules with the right arguments. Formally,\nop.bprop(inputs, X, G) must return\n\ue058\ni\n(\u2207 Xop.f(inputs)i) Gi, (6.54)\nwhich is just an implementation of the chain rule as expressed in equation 6.47.\nHere, inputs is a list of inputs that are supplied to the operation, op.f is the\nmathematical function that the operation implements,X is the input whose gradient\nwe wish to compute, and G is the gradient on the output of the operation.\nThe op.bprop method should always pretend that all of its inputs are distinct\nfrom each other, even if they are not. For example, if the mul operator is passed\ntwo copies of x to compute x2, the op.bprop method should still return x as the\nderivative with respect to both inputs. The back-propagation algorithm will later\nadd both of these arguments together to obtain 2x, which is the correct total\nderivative on x.\nSoftware implementations of back-propagation usually provide both the operations and their bprop methods, so that users of deep learning software libraries are\nable to back-propagate through graphs built using common operations like matrix\nmultiplication, exponents, logarithms, and so on. Software engineers who build a\nnew implementation of back-propagation or advanced users who need to add their\nown operation to an existing library must usually derive the op.bprop method for\nany new operations manually.\nThe back-propagation algorithm is formally described in algorithm 6.5.\n216\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.5 The outermost skeleton of the back-propagation algorithm. This\nportion does simple setup and cleanup work. Most of the important work happens\nin the build_grad subroutine of algorithm 6.6 .\nRequire: T, the target set of variables whose gradients must be computed.\nRequire: G, the computational graph\nRequire: z, the variable to be differentiated\nLet G\ue030 be G pruned to contain only nodes that are ancestors of z and descendents\nof nodes in T.\nInitialize grad_table, a data structure associating tensors to their gradients\ngrad_table[z] \u2190 1\nfor V in T do\nbuild_grad(V, G, G\ue030 , grad_table)\nend for\nReturn grad_table restricted to T\nIn section 6.5.2, we explained that back-propagation was developed in order to\navoid computing the same subexpression in the chain rule multiple times. The naive\nalgorithm could have exponential runtime due to these repeated subexpressions.\nNow that we have specified the back-propagation algorithm, we can understand its\ncomputational cost. If we assume that each operation evaluation has roughly the\nsame cost, then we may analyze the computational cost in terms of the number\nof operations executed. Keep in mind here that we refer to an operation as the\nfundamental unit of our computational graph, which might actually consist of very\nmany arithmetic operations (for example, we might have a graph that treats matrix\nmultiplication as a single operation). Computing a gradient in a graph with n nodes\nwill never execute more than O(n2) operations or store the output of more than\nO(n2) operations. Here we are counting operations in the computational graph, not\nindividual operations executed by the underlying hardware, so it is important to\nremember that the runtime of each operation may be highly variable. For example,\nmultiplying two matrices that each contain millions of entries might correspond to\na single operation in the graph. We can see that computing the gradient requires as\nmost O(n2) operations because the forward propagation stage will at worst execute\nall n nodes in the original graph (depending on which values we want to compute,\nwe may not need to execute the entire graph). The back-propagation algorithm\nadds one Jacobian-vector product, which should be expressed with O(1) nodes, per\nedge in the original graph. Because the computational graph is a directed acyclic\ngraph it has at most O(n2 ) edges. For the kinds of graphs that are commonly used\nin practice, the situation is even better. Most neural network cost functions are\n217\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nAlgorithm 6.6 The inner loop subroutine build_grad(V, G, G\ue030\n, grad_table) of\nthe back-propagation algorithm, called by the back-propagation algorithm defined\nin algorithm 6.5.\nRequire: V, the variable whose gradient should be added to G and grad_table.\nRequire: G, the graph to modify.\nRequire: G\ue030\n, the restriction of G to nodes that participate in the gradient.\nRequire: grad_table, a data structure mapping nodes to their gradients\nif V is in grad_table then\nReturn grad_table[V]\nend if\ni \u2190 1\nfor C in get_consumers(V, G\ue030\n) do\nop \u2190 get_operation\u00a9\nD \u2190 build_grad(C, G,G\ue030\n, grad_table)\nG(i) \u2190 op.bprop(get_inputs(C, G\ue030\n), V, D)\ni \u2190 i + 1\nend for\nG \u2190\ue050\ni G(i)\ngrad_table[V] = G\nInsert G and the operations creating it into G\nReturn G\nroughly chain-structured, causing back-propagation to have O(n) cost. This is far\nbetter than the naive approach, which might need to execute exponentially many\nnodes. This potentially exponential cost can be seen by expanding and rewriting\nthe recursive chain rule (equation 6.49) non-recursively:\n\u2202u(n)\n\u2202u(j) = \ue058\npath (u(\u03c01),u(\u03c02) ,\u2026,u(\u03c0t) ),\nfrom \u03c01=j to \u03c0t=n\n\ue059\nt\nk=2\n\u2202u(\u03c0k)\n\u2202u(\u03c0k\u22121 )\n. (6.55)\nSince the number of paths from node j to node n can grow exponentially in the\nlength of these paths, the number of terms in the above sum, which is the number\nof such paths, can grow exponentially with the depth of the forward propagation\ngraph. This large cost would be incurred because the same computation for\n\u2202u(i)\n\u2202u(j) would be redone many times. To avoid such recomputation, we can think\nof back-propagation as a table-filling algorithm that takes advantage of storing\nintermediate results \u2202u(n)\n\u2202u(i) . Each node in the graph has a corresponding slot in a\ntable to store the gradient for that node. By filling in these table entries in order,\n218\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nback-propagation avoids repeating many common subexpressions. This table-filling\nstrategy is sometimes called dynamic programming.\n6.5.7 Example: Back-Propagation for MLP Training\nAs an example, we walk through the back-propagation algorithm as it is used to\ntrain a multilayer perceptron.\nHere we develop a very simple multilayer perception with a single hidden\nlayer. To train this model, we will use minibatch stochastic gradient descent.\nThe back-propagation algorithm is used to compute the gradient of the cost on a\nsingle minibatch. Specifically, we use a minibatch of examples from the training\nset formatted as a design matrix X and a vector of associated class labels y.\nThe network computes a layer of hidden features H = max{0, XW(1)}. To\nsimplify the presentation we do not use biases in this model. We assume that our\ngraph language includes a relu operation that can compute max{0, Z} elementwise. The predictions of the unnormalized log probabilities over classes are then\ngiven by HW (2). We assume that our graph language includes a cross_entropy\noperation that computes the cross-entropy between the targets y and the probability\ndistribution defined by these unnormalized log probabilities. The resulting crossentropy defines the cost JMLE. Minimizing this cross-entropy performs maximum\nlikelihood estimation of the classifier. However, to make this example more realistic,\nwe also include a regularization term. The total cost\nJ = JMLE + \u03bb\n\uf8eb\n\uf8ed\ue058\ni,j\n\ue010\nW (1)\ni,j \ue0112\n+ \ue058\ni,j\n\ue010\nW(2)\ni,j \ue0112\n\uf8f6\n\uf8f8 (6.56)\nconsists of the cross-entropy and a weight decay term with coefficient \u03bb. The\ncomputational graph is illustrated in figure 6.11.\nThe computational graph for the gradient of this example is large enough that\nit would be tedious to draw or to read. This demonstrates one of the benefits\nof the back-propagation algorithm, which is that it can automatically generate\ngradients that would be straightforward but tedious for a software engineer to\nderive manually.\nWe can roughly trace out the behavior of the back-propagation algorithm\nby looking at the forward propagation graph in figure 6.11. To train, we wish\nto compute both \u2207W (1)J and \u2207W (2)J. There are two different paths leading\nbackward from J to the weights: one through the cross-entropy cost, and one\nthrough the weight decay cost. The weight decay cost is relatively simple; it will\nalways contribute 2\u03bbW(i) to the gradient on W(i) .\n219\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nX W (1)\nU (1)\nmatmul\nH\nrelu\nU (3)\nsqr u(4)\nsum\nW (2) u(7) \u03bb\nU (2)\nmatmul\ny\nJMLE\ncross_entropy\nU (5)\nsqr u(6)\nsum\nu(8)\nJ\n+\n\u00d7\n+\nFigure 6.11: The computational graph used to compute the cost used to train our example\nof a single-layer MLP using the cross-entropy loss and weight decay.\nThe other path through the cross-entropy cost is slightly more complicated.\nLet G be the gradient on the unnormalized log probabilities U(2) provided by\nthe cross_entropy operation. The back-propagation algorithm now needs to\nexplore two different branches. On the shorter branch, it adds H \ue03eG to the\ngradient on W(2) , using the back-propagation rule for the second argument to\nthe matrix multiplication operation. The other branch corresponds to the longer\nchain descending further along the network. First, the back-propagation algorithm\ncomputes \u2207HJ = GW(2)\ue03e using the back-propagation rule for the first argument\nto the matrix multiplication operation. Next, the relu operation uses its backpropagation rule to zero out components of the gradient corresponding to entries\nof U(1) that were less than 0. Let the result be called G\ue030 . The last step of the\nback-propagation algorithm is to use the back-propagation rule for the second\nargument of the matmul operation to add X\ue03eG\ue030 to the gradient on W(1).\nAfter these gradients have been computed, it is the responsibility of the gradient\ndescent algorithm, or another optimization algorithm, to use these gradients to\nupdate the parameters.\nFor the MLP, the computational cost is dominated by the cost of matrix\nmultiplication. During the forward propagation stage, we multiply by each weight\n220\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\nmatrix, resulting in O(w) multiply-adds, where w is the number of weights. During\nthe backward propagation stage, we multiply by the transpose of each weight\nmatrix, which has the same computational cost. The main memory cost of the\nalgorithm is that we need to store the input to the nonlinearity of the hidden layer.\nThis value is stored from the time it is computed until the backward pass has\nreturned to the same point. The memory cost is thus O(mnh), where m is the\nnumber of examples in the minibatch and nh is the number of hidden units.\n\u201d\nSummarize the above basic principle of back-propagation for neural network training with more equations, try to use your own words, do not directly copy the sentence from the above paper."}, {"role": "assistant", "content": "Back-propagation is a fundamental algorithm used for training neural networks, primarily used for optimizing the weights of the network by minimizing the cost or loss function. Here's a concise summary, focusing more on the mathematical formulation using my own wording:\n\nIn a feedforward neural network, information progresses from the input layer \\(x\\) through one or more hidden layers, culminating in an output \\( \\hat{y} \\), through a process known as forward propagation. During training, forward propagation is used to calculate a loss, \\(J(\\theta)\\), which quantifies the difference between the network's prediction \\( \\hat{y} \\) and the true output \\(y\\).\n\nBack-propagation, introduced by Rumelhart et al. in 1986, is then employed to calculate the gradients of this loss function with respect to the network's weights \\( \\theta \\). Despite common misinterpretation, back-propagation solely pertains to the computation of gradients, whereas the adjustment of weights involves another algorithm, typically stochastic gradient descent (SGD).\n\nThe back-propagation employs the chain rule from calculus to efficiently compute these gradients. For functions of a real variable, the chain rule is expressed as:\n\n\\[\n\\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx}\n\\]\n\nThis concept extends to multivariable functions, where derivatives are represented as Jacobian matrices. If we have a function mapping from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), followed by a scalar-valued function, the gradient of the scalar with respect to the input vector can be computed using the product of Jacobians.\n\nCentral to back-propagation is the computational graph, which represents the operations carried out by the neural network. Each node in this graph corresponds to a variable (which could be a scalar, vector, matrix, etc.), and edges represent operations on these variables. By traversing this graph in reverse, starting from the output back to the inputs, back-propagation applies the chain rule to compute gradients efficiently, avoiding redundant calculations.\n\nConsider a simple multi-layer perceptron (MLP) with one hidden layer, for example. Forward propagation for this MLP might look like \\(H = max(0, XW^{(1)})\\), which denotes the activation of the hidden layer after applying a ReLU nonlinearity to the weighted input \\(XW^{(1)}\\). The final output might be computed as \\(U^{(2)} = HW^{(2)}\\), with \\(W^{(1)}\\) and \\(W^{(2)}\\) representing the weights of the first and second layers, respectively. To minimize a loss function \\(J\\), which could include a cross-entropy term for classification and a regularization term for weight decay, back-propagation computes gradients \\( \\nabla_{W^{(1)}} J \\) and \\( \\nabla_{W^{(2)}} J \\) to update weights in the direction that minimizes \\(J\\).\n\nThe process of back-propagation efficiently computes these gradients by systematically applying the chain rule through the computational graph, using local gradients of operations (represented by Jacobian matrices) and accumulating contributions from different paths due to the connectivity of the network. It requires storing intermediate values from the forward pass to use in the backward pass, and the computational cost is primarily driven by the cost of these forward operations, such as matrix multiplications, along with the additional steps to compute the Jacobian-vector products during back-propagation.\n\nIn sum, back-propagation is a powerful and efficient algorithm that leverages the structure of the neural network and the chain rule to compute gradients, which are then used by another algorithm, like SGD, to adjust the weights and minimize the loss function during training."}, {"role": "user", "content": "Briefly tell some advantage of back-propagation"}, {"role": "assistant", "content": "Back-propagation offers several key advantages that have made it a central algorithm in the training of neural networks:\n\n1. **Efficiency**: It computes the gradient of the loss function with respect to each weight by efficiently using the chain rule, avoiding redundant calculations through its systematic backward pass. This makes it computationally efficient compared to naive approaches that might involve recalculating intermediate gradients repeatedly.\n\n2. **Scalability**: The algorithm scales well with the size and complexity of the network. Regardless of the network's depth or the number of parameters, back-propagation can compute gradients in a time that is linearly proportional to the number of edges in the computational graph, making it practical for large neural networks.\n\n3. **Applicability**: Back-propagation is versatile and can be used to optimize any network architecture, including feedforward neural networks, convolutional neural networks (CNNs), and recurrent neural networks (RNNs), as long as the network functions are differentiable.\n\n4. **Automation of Gradient Computation**: It automates the differentiation process, eliminating the need for manual computation of gradients. This is particularly advantageous for complex models, where manually deriving gradients can be error-prone and cumbersome.\n\n5. **Foundation for Advanced Techniques**: Back-propagation forms the cornerstone for more sophisticated optimization techniques in deep learning, such as Momentum, RMSProp, and Adam, which further enhance the convergence and performance of neural network training.\n\n6. **Flexibility and Generalization**: It can be applied to a wide range of loss functions and models, allowing for custom architectures and objectives tailored to specific tasks. This flexibility empowers researchers and practitioners to innovate and experiment with new models and applications.\n\nIn summary, the efficiency, scalability, versatility, and automation of back-propagation have been central to its pivotal role in the advancement and success of neural networks in machine learning.\n"}, {"role": "user", "content": "In few sentences"}, {"role": "assistant", "content": "Back-propagation is a highly efficient algorithm pivotal for training neural networks, enabling the computation of gradients efficiently across complex architectures. It scales well with network size, automates the gradient computation process, and supports a wide range of models and loss functions. Its versatility and scalability have made it foundational in the development and optimization of advanced neural network models, facilitating innovations across various domains in machine learning."}]}]}, {"summary": "The user requested summaries of technical documents on neural network concepts like ReLU, CNNs, convolution, stride, pooling, and k-NN, with a focus on mathematical explanations and avoiding direct text copying. The responses provided concise, original explanations of key principles and equations without reproducing the source material", "name": "Summarized technical documents on neural network concepts with mathematical clarity", "children": [], "numConvs": 13, "conversations": [{"facetValue": "The user asked about the k-NN approach for classification and provided a detailed text. The response involved summarizing the k-NN method in a clear and concise manner.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the k-NN approach for classification and provided a detailed text. The response involved summarizing the k-NN method in a clear and concise manner."}], "conversation": [{"role": "user", "content": "\"\n[1]. A common rule-based method is template matching, which compares test images to reference templates using metrics like normalized cross-correlation. Template matching can effectively detect systematic defects but struggles with mixed or random patterns. Thresholding techniques are also rule-based, where global or local thresholds on metrics like pixel intensity are used to segment defects from backgrounds. Rule-based classification approaches for AOI have several limitations. The thresholds and boundaries used in the rules must be carefully predefined, which can limit flexibility and require reprogramming if new defect types are encountered. Feature selection is also critical, as redundant or uninformative features may reduce accuracy. Rule-based classifiers also struggle to generalize well when defects occur in nonspecific or overlapping patterns. Additionally, manual feature engineering is typically required rather than learning features directly from data. The accuracy of rule-based classifiers is highly dependent on consistent image quality and may be reduced by noise, lighting variations or component shifts between teaching and test images. While simple to implement, the limitations of rule-based AOI suggest machine learning techniques could provide benefits through self-learning features and greater robustness to variability in defect occurrence and image conditions.\nFig. 4.1 (a) The 1-NN decision rule: the point ? is assigned to the class on the left; (b) the k-NN\ndecision rule, with k = 4: the point ? is assigned to the class on the left as well.\nonly one known sample; in Figure 4.1(b) more than one known sample is used. In the\nlast case, the parameter k is set to 4, so that the closest four samples are considered\nfor classifying the unknown one. Three of them belong to the same class, whereas\nonly one belongs to the other class. In both cases, the unknown sample is classified as\nbelonging to the class on the left. Figure 4.2 provides a sketch of the k-NN algorithm.\nThe distance function plays a crucial role in the success of the classification, as is\nthe case in many data mining techniques. Indeed, the most desirable distance function\nis the one for which a smaller distance among samples implies a greater likelihood\nfor samples to belong to the same class. The choice of this function may not be trivial.\nAnother important factor is the choice of the value for the parameter k. This is the\nmain parameter of the method, since it represents the number of nearest neighbors\nconsidered for classifying an unknown sample. Usually it is fixed beforehand, but\nselecting an appropriate value for k may not be trivial. If k is too large, classes with\na great number of classified samples can overwhelm small ones and the results will\nbe biased. On the other hand, if k is too small, the advantage of using many samples\nin the training set is not exploited. Usually, the k value is optimized by trials on\nthe training and validation sets (see Chapter 8). Moreover, assigning a classification\non the basis of the majority of the k \u201cvotes\u2019\u2019 of the nearest neighbors could not be\naccurate in some particular cases. For example, if the nearest neighbors vary widely\nin their distance, then an unknown sample may be classified considering sample\n\"\nSummarize the k-nn approach for classification in paragraph form , try not to directly copy the sentences from the paragraphs, use your own words ."}, {"role": "assistant", "content": "The k-Nearest Neighbors (k-NN) algorithm is a fundamental method used for classification tasks, where the goal is to assign a class to an unknown sample based on its similarity to known samples. The process involves comparing the unknown sample to a set of pre-classified samples to determine its class based on the closest neighbors. The \"k\" in k-NN represents the number of nearest neighbors used in this decision-making process. To classify an unknown sample, the algorithm looks at the 'k' most similar (or nearest) samples from the known dataset. If 'k' is set to 1, the class of the single nearest neighbor is assigned to the unknown sample. For larger values of 'k', the algorithm considers more neighbors and assigns the class based on the majority vote among these neighbors, with each contributing one vote. The success of k-NN in accurately classifying samples heavily relies on selecting an appropriate distance function to measure similarities and the optimal value of 'k'. The distance function should ensure that samples closer together are more likely to belong to the same class. Meanwhile, the choice of 'k' affects the algorithm's sensitivity and bias; too high a value and the classification may overlook minority classes, too low and it may not fully leverage the information available in the dataset. Finding the right balance for 'k' often involves trial and error through testing with training and validation datasets. The k-NN method is direct and effective, but its accuracy can be influenced by the distribution of samples, the chosen distance function, and the selection of 'k'."}]}, {"facetValue": "The user provided a detailed explanation of decision trees and requested a summary. The response focused on accurately capturing the key concepts without copying directly from the text.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed explanation of decision trees and requested a summary. The response focused on accurately capturing the key concepts without copying directly from the text."}], "conversation": [{"role": "user", "content": "\"Figure 1 illustrates a simple decision tree model that includes a single binary target variable Y (0 or 1) and two continuous variables, x1 and x2, that range from 0 to 1. The main components of a decision tree model are nodes and branches and the most important steps in building a model are splitting, stopping, and pruning.\n\nAn external file that holds a picture, illustration, etc.\nObject name is sap-27-02-130-g002.jpg\nFigure 1.\nSample decision tree based on binary target variable Y\nNodes. There are three types of nodes. (a) A root node, also called a decision node, represents a choice that will result in the subdivision of all records into two or more mutually exclusive subsets. (b) Internal nodes, also called chance nodes, represent one of the possible choices available at that point in the tree structure; the top edge of the node is connected to its parent node and the bottom edge is connected to its child nodes or leaf nodes. (c) Leaf nodes, also called end nodes, represent the final result of a combination of decisions or events.\n\nBranches. Branches represent chance outcomes or occurrences that emanate from root nodes and internal nodes. A decision tree model is formed using a hierarchy of branches. Each path from the root node through internal nodes to a leaf node represents a classification decision rule. These decision tree pathways can also be represented as \u2018if-then\u2019 rules. For example, \u201cif condition 1 and condition 2 and condition \u2026 and condition k occur, then outcome j occurs. \u201d\n\nSplitting. Only input variables related to the target variable are used to split parent nodes into purer child nodes of the target variable. Both discrete input variables and continuous input variables (which are collapsed into two or more categories) can be used. When building the model one must first identify the most important input variables, and then split records at the root node and at subsequent internal nodes into two or more categories or \u2018bins\u2019 based on the status of these variables. Characteristics that are related to the degree of \u2018purity\u2019 of the resultant child nodes (i. e. , the proportion with the target condition) are used to choose between different potential input variables; these characteristics include entropy, Gini index, classification error, information gain, gain ratio, and twoing criteria. [3] This splitting procedure continues until pre-determined homogeneity or stopping criteria are met. In most cases, not all potential input variables will be used to build the decision tree model and in some cases a specific input variable may be used multiple times at different levels of the decision tree.\n\nStopping. Complexity and robustness are competing characteristics of models that need to be simultaneously considered whenever building a statistical model. The more complex a model is, the less reliable it will be when used to predict future records. An extreme situation is to build a very complex decision tree model that spreads wide enough to make the records in each leaf node 100% pure (i. e. , all records have the target outcome). Such a decision tree would be overly fitted to the existing observations and have few records in each leaf, so it could not reliably predict future cases and, thus, would have poor generalizability (i. e. , lack robustness). To prevent this from happening, stopping rules must be applied when building a decision tree to prevent the model from becoming overly complex. Common parameters used in stopping rules include: (a) the minimum number of records in a leaf; (b) the minimum number of records in a node prior to splitting; and (c) the depth (i. e. , number of steps) of any leaf from the root node. Stopping parameters must be selected based on the goal of the analysis and the characteristics of the dataset being used. As a rule-of-thumb, Berry and Linoff [4] recommend avoiding overfitting and underfitting by setting the target proportion of records in a leaf node to be between 0. 25 and 1. 00% of the full training data set.\n\nPruning. In some situations, stopping rules do not work well. An alternative way to build a decision tree model is to grow a large tree first, and then prune it to optimal size by removing nodes that provide less additional information. [5] A common method of selecting the best possible sub-tree from several candidates is to consider the proportion of records with error prediction (i. e. , the proportion in which the predicted occurrence of the target is incorrect). Other methods of selecting the best alternative is to use a validation dataset (i. e. , dividing the sample in two and testing the model developed on the training dataset on the validation dataset), or, for small samples, crossvalidation (i. e. , dividing the sample in 10 groups or \u2018folds\u2019, and testing the model developed from 9 folds on the 10th fold, repeated for all ten combinations, and averaging the rates or erroneous predictions). There are two types of pruning, pre-pruning (forward pruning) and post-pruning (backward pruning). Pre-pruning uses Chi-square tests [6] or multiple-comparison adjustment methods to prevent the generation of non-significant branches. Post-pruning is used after generating a full decision tree to remove branches in a manner that improves the accuracy of the overall classification when applied to the validation dataset.\n\nDecision trees can also be illustrated as segmented space, as shown in Figure 2. The sample space is subdivided into mutually exclusive (and collectively exhaustive) segments, where each segment corresponds to a leaf node (that is, the final outcome of the serial decision rules). Each record is allocated to a single segment (leaf node). Decision tree analysis aims to identify the best model for subdividing all records into different segments.\n\n\"\nSummarize the above explanation of the decision tree, try not to copy from the above paragraph directly, use your own words."}, {"role": "assistant", "content": "Decision trees are a type of statistical model used to make predictions or decisions based on input data. They work by splitting data into branches at decision points (nodes), ultimately leading to outcomes represented by leaf nodes. The key components of a decision tree include the root node (where the decision process starts), internal nodes (where choices are made), and leaf nodes (which indicate final outcomes). The process of building a decision tree involves three main steps: splitting, stopping, and pruning.\n\nSplitting is the process of dividing data at various nodes based on certain conditions to make the subsets more homogenous or pure in terms of the target variable. This is done using measures like entropy, Gini index, and information gain among others. The aim is to use the most relevant input variables for splits to achieve purer child nodes.\n\nStopping criteria are applied to prevent the tree from becoming overly complex and to ensure it remains reliable for future predictions. This involves setting limits on things like the minimum number of records in nodes or leaves and the maximum depth of the tree. The goal here is to strike a balance between making the model simple enough to be robust, but detailed enough to be accurate.\n\nPruning is used to remove less informative parts of the tree either by stopping growth early (pre-pruning) or by reducing the size of a fully grown tree (post-pruning). This can enhance the model's performance by eliminating segments that do not significantly contribute to predicting outcomes. Methods include error rate comparison, using validation datasets, and Chi-square tests.\n\nIn essence, decision tree modeling is about finding the most effective way to divide a dataset into segments that are as pure as possible, making it easier to predict outcomes. This requires careful consideration of how and when to split, stop, and prune the tree for optimal accuracy and reliability."}]}, {"facetValue": "The user provided a detailed academic text about the weaknesses of SVM and related techniques. The response involved understanding and summarizing the content without any personal or sensitive information.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed academic text about the weaknesses of SVM and related techniques. The response involved understanding and summarizing the content without any personal or sensitive information."}], "conversation": [{"role": "user", "content": "\"3. Weaknesses of SVM\nDespite the generalization capacity and many advantages of the\nSVM, they have some very marked weaknesses, among which are:\nthe selection of parameters, algorithmic complexity that affects the\ntraining time of the classifier in large data sets, development of\noptimal classifiers for multi-class problems and the performance\nof SVMs in unbalanced data sets.\n3.1. Algorithmic complexity\nMaybe the principal disadvantage of SVM is due to its excessive\ncomputational cost in large data sets, because the training kernel\nmatrix grows in quadratic form with the size of the data set, which\nprovokes that training of SVM on large data sets is a very slow process. Support Vector Machines (SVM) have demonstrated highly\ncompetitive performance in many real-world applications. However, despite its good theoretical foundations and generalization\nperformance, SVM is not suitable for large data set classification.\nTraining an SVM is usually posed as a quadratic programming\n(QP) problem to find a separation hyperplane which implicates a\nmatrix of density n \b n , where the n is the number of points in\nthe data set. This needs huge quantities of computational time\nand memory for large data sets, so the training complexity of\nSVM is highly dependent on the size of a data set [61,62].\nAccording to the strategy used, the training methods for SVM\ncan be categorized into data selection, decomposition, geometric,\nparallel implementations and heuristics. Their core ideas and the\nmost representative algorithms are presented in this section.\nData selection methods for SVM intent to decrease the size of\ndata sets by removing the instances that do not contribute to the\ndefinition of the optimal separating hyperplane. The latter depends\ncompletely on instances which are located closest to the separation\nboundary [63], and correspond to those whose Lagrange multipliers are greater than zero in the Karush\u2013Kuhn\u2013Tucker conditions\nJ. Cervantes et al. / Neurocomputing 408 (2020) 189\u2013215 195\n(60). These instances are called support vectors (SVs). Generally,\nthe number of SVs is a small portion compared with the size of\ntraining sets.\nai \u00bc 0 ) yi\u00f0h i x; xi \u00fe b\u00de P 1; ni \u00bc 0\n0 < ai < C ) yi\u00f0h i x; xi \u00fe b\u00de \u00bc 1; ni \u00bc 0\nalphai \u00bc C ) yi\n\u00f0h i x; xi \u00fe b\u00de 6 1; ni P 0\n\u00f060\u00de\nSimple random sampling (SRS) is probably the most basic strategy to reduce the size of training sets. It consists in choosing a\nnumber of instances and then training a SVM with them. The\nworks presented in [64\u201366] show that uniform random sampling\nis the optimal robust selection scheme in terms of several statistical criteria. However, although SRS is computationally cheap, the\nstandard deviation of classification accuracy is large in most cases\n[66].\nA more sophisticated form of this type of sampling consists in\nassigning to each instance a probability to be chosen. Once a number of instances is randomly selected, a SVM is trained with them.\nAfter this, the probabilities are updated, increasing those whose\ninstances have been miss-classified [67\u201369]. This process is\nrepeated several times.\nSome data selection methods have been developed by computing the distance between the instances and the optimal hyperplane. Several metrics for measuring distance have been used in\nprevious works: These measures include the Euclidean [70], Mahalanobis [71] and Hausdorff [72,73] distances. Most of the current\ndistance-based methods are inspired on two observations: (1)\nthe instances closest to those ones with opposite label have high\nchances to be SVs [72] and (2) instances far from hyperplane do\nnot contribute to the definition of the decision boundary [74]. A\nproblem with naive implementations that require to compute all\ndistances between objects is that this task has a temporal and a\nspatial complexity of O\u00f0n2\u00de.\nThe Condensed Nearest Neighbor (CNN) [75] chooses instances\nnear to class frontiers, reducing the size of training sets. However,\nCNN is not noise tolerant. Reduced Nearest Neighbor (RNN) [76],\nSelective Nearest Neighbor (SNN) [77] and Minimal Consistent\nSet (MCS) are methods based on CNN, and therefore, they have also\nproblems with noisy data sets. RNN, SNN and MCS are more costly\nthan CNN.\nNeighborhood properties of SVs have also been exploited to\nreduce size of training sets. Wang and Kwong [78] used neighborhood entropy, while in [79] only the patterns in the overlap region\naround the decision boundary are selected. The method presented\nin [80] follows this trend but use fuzzy C-mean clustering to select\nsamples on the boundaries of class distribution, whereas [72] uses\nhyper spheres. Clustering has been proved to be an effective\nmethod to collaborate with SVM on classifying large data sets.\nFor example, hierarchical clustering [81,82], k-means [83] and parallel clustering [84]. Clustering-based methods can reduce the\ncomputations burden of SVM, however, the clustering algorithms\nthemselves are still complicated for large data set. Rocchio bundling is a statistics-based data reduction method [85]. The Bayesian\ncommittee machine is also reported to be used to train SVM on\nlarge data sets, where the large data set is divided into m subsets\nof the same size, and m models are derived from the individual sets\n[86]. But, it has higher error rate than normal SVM and the sparse\nproperty does not hold.\nThe basis for decomposition methods lies in the fact that the\ntraining time can be reduced if only the active constraints of the\nQP problem are taken into account [87]. A similar idea to active\nset methods for optimization is applied in decomposition methods.\nIn the active set approach, two sets are used: the working set and\nthe set of fixed variables. The optimization is made only on the\nworking set. For the case of SVM, the working set is usually composed of instances that violate the Karush\u2013Kuhn\u2013Tucker conditions. Apart of the proved convergence [88], a clear advantage of\ndecomposition is that memory requirement is linear in the number\nof training examples; but on the other hand, because only a fraction of variables is being considered in each iteration, it is time\nconsuming [89,90] if elements in the active set are not carefully\nselected. One of the first decomposition methods was Chunking\n[74]. It consists in repetitively obtaining the maximum margin\nhyperplane from an amount of instances (called the chunk) and\nthen forming a new chunk with the SVs from the previous solution\nand some new instances. Probably the most famous decomposing\nalgorithm is the SMO [15]. Sequential minimal optimization\n(SMO) is a fast method to train SVM [91,84]. Training SVM requires\nthe solution of the QP optimization problem. SMO breaks this large\nQP problem into a series of smallest possible QP problems. It considers the smallest size working set: only two training samples,\nand it is faster than the projected conjugate gradient (PCG) chunking algorithm. Dong et al. [61] introduced a parallel optimization\nstep where block diagonal matrices are used to approximate the\noriginal kernel matrix so that SVM classification can be split into\nhundreds of sub-problems. A recursive and computational superior\nmechanism referred as adaptive recursive partitioning was proposed in [92], where the data are recursively subdivided into smaller subsets. Genetic programming is able to deal with large data\nsets that do not fit in main memory [93]. Neural networks technique can also be applied for SVM to simplify the training process\n[94]. LibSVM [95] is an algorithm based on SMO with the improvement of a more advanced mechanism of selection of the working\nset by using the second order information method previously\nshown in [96]. The SVMlight [97] is another important state-ofthe-art decomposition method.\nVariants of SVM speed up the training time of SVM at expense\nof loosing accuracy [89]. These methods work by changing the\noriginal QP problem formulation. Most of the variants methods\nconclude with a system of linear equations solved efficiently if\nthe number of features is moderate, i.e., around 100. A representative method in this category is the least square SVM (LS-SVM) [98]\nwhich changes the original QP problem by using a linear system of\nequations that can be solved explicitly or by using a conjugate gradient method. Other important methods are the PSVM (Proximal\nSVM) [99] and reduced SVM (RSVM) [100].\nParallel implementation of the QP problem is difficult because\nthere is a strong dependence between data [101]. Most parallel\nmethods for training SVM divide the training set into independent\nsubsets to train SVM in different processors, as in [101\u2013103]. In\n[61], the kernel matrix of SVM is approximated by block diagonal\nmatrices so that the original optimization problem can be decomposed into hundreds of sub problems, which are easy to solve in a\nparallel fashion. Other parallel implementations can be found in\n[104\u2013108].\nGeometric methods for SVM are based on that computing the\noptimal separating hyperplane is equivalent to find the closest pair\nof points belonging to convex hulls [63,109,110]. Recent advances\non geometric methods can be found in [111\u2013115].\nAmong all heuristic methods, the alpha seeding [116] consists\nof providing initial estimates of the ai values for the starting of\nthe QP problem. Alpha seeding seems to be a practical method to\nimprove training time of SVM. Recently, an improvement of this\nmethod has been proposed in [117].\nAccording to the reviewed literature, there are currently just\nfew methods that combine decision tree (DT) for instance selection\nin a similar way to the presented in this research. In [118], the patterns by ordered projections (POP) algorithm was presented. It\nuses projections of instances on the axis of attributes to find the\nminimal number of elements to represent hyper-rectangles which\n196 J. Cervantes et al. / Neurocomputing 408 (2020) 189\u2013215\ncontain instances of the same class (entropy zero). A disadvantage\nof POP is that the reduction of the size of data sets is very low\n[119].\nIn [120], a method that approximates the decision boundary of\nSVM using a DT to speed up SVM in its testing phase was proposed.\nIn [120], an SVM is used in some leaves of a DT. The idea is to\nreduce the number of test data points that require SVM\u2019s decision.\nRecently, in [121,26], the combination of a DT and SVM was\nproposed. The underlying idea is to train an SVM first, and then\nuse the predictions of the model obtained to modify the class of\nexamples in the training set. A DT is afterward trained using the\nmodified set. The SVM is used as a pre-processor for improving\nthe performance of DT, when dealing with the problem of\nimbalance.\n3.2. Development of optimal classifiers for multi-class problems\nSupport Vector Machines were originally designed to solve binary classification problems [122]. The problem of multiclassification for SVM, does not present an easy solution. In order\nto apply SVM to multi-classification problems, it is necessary to\nchange the problem to multiple binary classification problem.\nThere are two basic types of algorithms to solve the multiclass\nclassification based in SVM.\n1. \u2018\u2018one versus one\u201d method (OVO). This method constructs k\u00f0k1\u00de\n2\nhyperplanes for k-class problem, where each one is trained with\njust two-class data sets. Thus given n training data\n\u00f0x1; y1\u00de; ... ;\u00f0xn; yn\u00de, where xi 2 Rd\n; i \u00bc 1; ... ; n and\nyi 2 f g 1; ... ; k is the class of xi, the i\nth SVM solves the problem:\nmin\nwij;bij;nij\n1\n2 \u00f0wij\u00de\nT\nwij \u00fe C\nXl\nj\u00bc1\nnij\nt\n\u00f0wij\u00de\nT\n/\u00f0xt\u00de \u00fe bij P 1  nij\nt ; ifo\nyt \u00bc i\n\u00f0wi\n\u00de\nT\n/\u00f0xj\u00de \u00fe bi\n6 1 \u00fe nij\nt ; ifo\nyt \u00bc j\nnij\nt P 0; j \u00bc 1; ... ; n\n\u00f061\u00de\nAt the prediction phase, a voting scheme is applied to get the class\nof x. The K\u00f0K1\u00de=2 classifiers are applied to an unseen sample x. If the\nk\nth classifier says that x belongs to the i class, then one vote for the i\nclass is added by one and the label of x is predicted in the class that\ngot the highest number of votes. The principal disadvantage of OVO\nis that some times the ambiguities in some regions of its input\nspace can provoke that two regions receive the same number of\nvotes.\n2. \u2018\u2018one versus all\u201d method (OVA). This algorithm constructs k\nhyperplanes for k-class problem. The i-th hyperplane is trained\nwith the samples between the i-th class and the rest data. Thus\ngiven n training data \u00f0x1; y1\u00de; ... ;\u00f0xn; yn\u00de, where\nxi 2 Rd\n; i \u00bc 1; ... ; n and yi 2 f g 1; ... ; k is the class of xi, the i\nth\nSVM solves the problem:\nmin\nwp;bp;np\n1\n2 \u00f0wp\u00de\nT\nwp \u00fe C\nXl\nj\u00bc1\nnp\ni\n\u00f0wp\u00de\nT\n/\u00f0xi\u00de \u00fe bp P 1  np\ni ; ifo\nyi \u00bc p\n\u00f0wp\u00de\nT\n/\u00f0xi\u00de \u00fe bp 6 1 \u00fe np\ni ; ifo\nyi \u2013 p\nnp\ni P 0; i \u00bc 1; ... ; n\n\u00f062\u00de\nwhere xi are mapped to high dimensional space by means of /,\nwhere /is a Kernel that satisfies the Mercer\u2019s condition [14], and C\nis a penalty parameter. The k decision functions are given by\n\u00f0w1\u00de\nT\n/\u00f0x\u00de \u00fe b1\n\u00f0w2\u00de\nT\n/\u00f0x\u00de \u00fe b2\n.\n.\n.\n\u00f0wk\u00de\nT\n/\u00f0x\u00de \u00fe bk\n\u00f063\u00de\nGiven a sample x to classify, the label of the class that has the largest value of the decision function is chosen as:\nclass of x \u00bc arg max i\u00bc1;2;...;k\n\u00f0\u00f0wp\u00de\nT\n/\u00f0x\u00de \u00fe bp\n\u00de \u00f064\u00de\nwhere wi and bi depict the hyperplane of the ith SVM.\nCurrently, most of methods use one-against-all or one-againstrest approaches to facing multi-class problems with SVM. However, there are numerous researchers who have faced the problem\nand have developed algorithms to try to solve the problem [123\u2013\n131].\n3.3. Performance of SVMs in imbalanced datasets\nIn imbalanced data sets, the correct classification of minority\nclass objects is a challenging problem. Normal classification methods, such as support vector machines, do not work well for these\nskewed data sets because is difficult to get the optimal separation\nhyperplane for an SVM trained with imbalanced data.\nThe imbalance in data sets affects considerably the performance\nof most classifiers. In general, the model extracted from this type of\ndata sets is biased towards the minority class. As a result, the accuracy on the minority classes is hampered. The imbalance in data\nsets is a recurrent problem in many domains, some examples\nare: fraud detection problems [132], classification of protein\nsequences [133\u2013135], medical diagnosis of rare and dangerous diseases [136], intrusion detection and text classification [137,138],\ndiscrimination between earthquakes and nuclear explosions\n[139]. Support Vector Machines were introduced by Vapnik [13]\nas a kernel based machine learning model for classification and\nregression tasks. The generalization capabilities and discriminative\npower of SVM have attracted the attention of practitioners and\ntheorists in last years. SVM has strong theoretical foundations,\nand, in general, it presents high classification accuracy in realworld applications. However, recent experiments [140\u2013142] show\nthat the performance of SVM is severely affected when it is applied\non imbalanced data sets. This is more evident when the ratio\nbetween the majority and the minority class is large. The first disadvantage of SVM on imbalanced data sets is due to the margin\nobtained is biased towards the minority class.\nThere are several solutions of SVM classification for imbalanced\ndata [143]. The techniques used to minimize the negative effect of\nimbalanced data sets on classifiers can be categorized as external\nand internal. The first techniques balance the data sets before\ntraining a classifier [144,145,142]. The second techniques modify\nthe model or architecture of classification methods [146\u2013148].\nPrincipal external techniques are under sampling and over sampling. In general, under sampling consists in selecting, randomly,\na small number of objects from majority class [146]. Over sampling\ntechniques generate artificial examples of the minority class. Other\nmethods use evolutionary algorithms to balance the data sets\n[143,149,150]. However, to add artificial data points to the minority class is a promising technique to tackle the problem of imbalance. Chawla et al. [140] proposed Synthetic Minority Over\nsampling Technique (SMOTE), which generates artificial objects\nto be included as members of the minority class. The minority class\nis over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any or\nall of the k minority class nearest neighbors. It does not cause\nJ. Cervantes et al. / Neurocomputing 408 (2020) 189\u2013215 197\nany information loss and could potentially find hidden minority\nregions. The disadvantage of this method is that it creates noise\nfor the classifiers which could result in a loss of performance\nbecause SMOTE makes the assumption that the instance between\na positive class instance and its nearest neighbors is also positive\n[151].\nClassification for imbalanced data sets has been studied by\nmachine learning community since the last decade [142]. There\nare many methods that are applied to imbalanced data sets in\norder to improve the performance of classifiers [135]. Generally,\nthese methods are divided into two categories: external methods\nand internal methods. External methods involve a pre processing\nof training data sets in order to make them balanced. Internal\nmethods deal with modifications of the learning algorithms in\norder to reduce their sensitiveness to class imbalance. In other\nwords, external methods attempt to balance the data sets by considering the number of examples for each class, whereas internal\nmethods consider the costs associated with misclassification and\ninclude these costs in the model. The most popular external methods are under-sampling, over-sampling and SMOTE. The under\nsampling and over sampling method [146] balances the data sets\nby randomly selecting small number of objects from majority class,\nand doubling the objects in the minority class. The main drawback\nis that some important points, such as support vectors, may be\nneglected by the random algorithm. [141] pointed out that the\nunder sampling strategy is not a good choice for SVM, and that\nthe over sampling cannot improve the final accuracy. The synthetic\nminority over sampling technique [140] generates artificial data in\nthe minority class by multiplying a random number in each original object. [152] showed that SMOTE is better than under sampling\nand over sampling. Several proposals inspired in SMOTE have been\nproposed, see, for example, [153\u2013155]. In [146] an SVM with different costs and SMOTE is used. That method introduces a scheme to\npenalize classification errors. The majority class is assigned a high\ncost and the minority class is assigned a low cost. This combination\nmakes denser the distribution of the minority class, and pushes the\nseparating hyperplane to the minority class. In [135], a penalization criterion is used to produce a similar effect on the separating\nhyper plane. In [142], a kernelized version of SMOTE is proposed.\nOther kernel-modification methods have been proposed in\n[142,156,157]. Other proposals can be found in [158,159].\nAlthough sampling methods and cost-sensitive learning methods seem to dominate the current research efforts in imbalanced\nlearning, Genetic Algorithm (GA)-based approaches have also been\npursued by the community. These algorithms use GAs in order to\nbalanced data sets. [149] used a GA for under-sampling the majority class, the algorithm tackles the difficulties of SVM learning on\nlarge data sets, because the method significantly reduce the size\nof the training set without loss of performance. Batuwita and\nPalade [160] applied Fuzzy SVM (FSVM) to improve the performance on imbalanced data sets to handle the problem of outliers\nand noise by assigning different fuzzy-membership values based\non their importance. In [161], the authors proposed a efficient\nresampling method selecting the most informative data examples\nlocated closer to the class boundary region by using the separating\nhyperplane found by training an SVM model on the original imbalanced data set, and then use only those examples in resampling. An\nexcellent survey about classification on imbalanced data sets can\nbe found in [162]. In [143], a GA is used to balance skewed data\nsets. That method produces better results than simple random\nsampling. Bazi and Melgani [163] used PSO algorithm in order to\nimprove the performance of SVM. That method was applied for\nthe classification of electrocardiogram signals and parameters estimation. In [159], the authors propose a classification system in\norder to detect the most important rules, and the rules which perturb the performance of classifier. That system uses hierarchical\nfuzzy rules and a GA. Garcia et al. [150] implemented an algorithm\nwhich performs an optimized selection of examples from data sets.\nThe learning algorithm is based on the nested generalized exemplar method and GA to generate and select the best suitable data\nto enhance the classification performance over imbalanced\ndomains. GA is used to guide the search process. Although generating new instances in the minority class can improve the performance in SVM classification [144], this process could introduce\nnoise to the data set, and to select randomly instances that helps\nto improve the performance in this area is almost impossible without using a genetic algorithm because the search space is huge\"\nSummarize the disadvantages of SVM in the paper above, try use your own words, do not directly copy from the paragraphs above.\n"}, {"role": "assistant", "content": "The paper outlines several key disadvantages of Support Vector Machines (SVM), particularly when dealing with large or complex datasets:\n\n1. **Algorithmic Complexity and Computational Cost**: One of the main drawbacks of SVM is its high computational demand, especially with large datasets. The training kernel matrix's size increases quadratically with the dataset, leading to slow training processes. This complexity makes SVM less suitable for classifying large datasets due to the immense computational time and memory requirements.\n\n2. **Difficulty in Parameter Selection**: Choosing the right parameters for SVM, such as the penalty parameter and the kernel type, is crucial but can be challenging. Incorrect parameter settings can negatively affect the performance of the SVM.\n\n3. **Handling Multi-Class Problems**: SVM is inherently designed for binary classification tasks, making multi-classification problems cumbersome. Techniques like \"one vs. one\" or \"one vs. all\" are used to adapt SVM for multi-class problems, but they come with their own sets of challenges, such as increased computational cost and complexity.\n\n4. **Performance in Unbalanced Datasets**: SVM struggles with datasets where one class significantly outnumbers the other(s). In such cases, the decision boundary may be biased towards the majority class, resulting in poorer classification performance of the minority class. Though techniques like under-sampling, over-sampling, and synthetic data generation (SMOTE) exist to mitigate this, they can introduce new problems, such as noise and loss of valuable data.\n\n5. **Difficulty with Large Feature Spaces**: When datasets have a very large number of features, the training time and computational resources required for SVM can become impracticable. This limitation can hinder the application of SVM in fields like genomics or image processing, where high-dimensional data is common.\n\n6. **Data Pre-processing Requirements**: Effective use of SVM often requires significant pre-processing of the data, such as feature scaling and normalization. This preprocessing step adds to the overall time and complexity of deploying SVM models.\n\nIn summary, while SVMs are a powerful tool for classification tasks with strong theoretical foundations and generally good performance, their practical application is hampered by issues related to computational efficiency, parameter tuning, and the handling of complex data scenarios like multi-classification and unbalanced datasets."}]}, {"facetValue": "The user asked about the principles of ReLU activation functions in neural networks, including their mathematical formulations and generalizations. The user sought a clear and concise explanation without direct copying of sentences from the provided text.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the principles of ReLU activation functions in neural networks, including their mathematical formulations and generalizations. The user sought a clear and concise explanation without direct copying of sentences from the provided text."}], "conversation": [{"role": "user", "content": "\"6.3 Hidden Units\nSo far we have focused our discussion on design choices for neural networks that\nare common to most parametric machine learning models trained with gradientbased optimization. Now we turn to an issue that is unique to feedforward neural\nnetworks: how to choose the type of hidden unit to use in the hidden layers of the\nmodel.\nThe design of hidden units is an extremely active area of research and does not\nyet have many definitive guiding theoretical principles.\nRectified linear units are an excellent default choice of hidden unit. Many other\ntypes of hidden units are available. It can be difficult to determine when to use\nwhich kind (though rectified linear units are usually an acceptable choice). We\n191\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\ndescribe here some of the basic intuitions motivating each type of hidden units.\nThese intuitions can help decide when to try out each of these units. It is usually\nimpossible to predict in advance which will work best. The design process consists\nof trial and error, intuiting that a kind of hidden unit may work well, and then\ntraining a network with that kind of hidden unit and evaluating its performance\non a validation set.\nSome of the hidden units included in this list are not actually differentiable at\nall input points. For example, the rectified linear function g (z) = max{0, z} is not\ndifferentiable at z = 0. This may seem like it invalidates g for use with a gradientbased learning algorithm. In practice, gradient descent still performs well enough\nfor these models to be used for machine learning tasks. This is in part because\nneural network training algorithms do not usually arrive at a local minimum of\nthe cost function, but instead merely reduce its value significantly, as shown in\nfigure 4.3. These ideas will be described further in chapter 8. Because we do not\nexpect training to actually reach a point where the gradient is 0, it is acceptable\nfor the minima of the cost function to correspond to points with undefined gradient.\nHidden units that are not differentiable are usually non-differentiable at only a\nsmall number of points. In general, a function g(z) has a left derivative defined\nby the slope of the function immediately to the left of z and a right derivative\ndefined by the slope of the function immediately to the right of z. A function\nis differentiable at z only if both the left derivative and the right derivative are\ndefined and equal to each other. The functions used in the context of neural\nnetworks usually have defined left derivatives and defined right derivatives. In the\ncase of g(z) = max{0, z}, the left derivative at z = 0 is 0 and the right derivative\nis 1. Software implementations of neural network training usually return one of\nthe one-sided derivatives rather than reporting that the derivative is undefined or\nraising an error. This may be heuristically justified by observing that gradientbased optimization on a digital computer is subject to numerical error anyway.\nWhen a function is asked to evaluate g(0), it is very unlikely that the underlying\nvalue truly was 0. Instead, it was likely to be some small value \ue00f that was rounded\nto 0. In some contexts, more theoretically pleasing justifications are available, but\nthese usually do not apply to neural network training. The important point is that\nin practice one can safely disregard the non-differentiability of the hidden unit\nactivation functions described below.\nUnless indicated otherwise, most hidden units can be described as accepting\na vector of inputs x, computing an affine transformation z = W \ue03ex + b, and\nthen applying an element-wise nonlinear function g(z). Most hidden units are\ndistinguished from each other only by the choice of the form of the activation\nfunction g(z).\n192\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\n6.3.1 Rectified Linear Units and Their Generalizations\nRectified linear units use the activation function g(z) = max{0, z}.\nRectified linear units are easy to optimize because they are so similar to linear\nunits. The only difference between a linear unit and a rectified linear unit is\nthat a rectified linear unit outputs zero across half its domain. This makes the\nderivatives through a rectified linear unit remain large whenever the unit is active.\nThe gradients are not only large but also consistent. The second derivative of the\nrectifying operation is 0 almost everywhere, and the derivative of the rectifying\noperation is 1 everywhere that the unit is active. This means that the gradient\ndirection is far more useful for learning than it would be with activation functions\nthat introduce second-order effects.\nRectified linear units are typically used on top of an affine transformation:\nh = g(W\ue03ex + b). (6.36)\nWhen initializing the parameters of the affine transformation, it can be a good\npractice to set all elements of b to a small, positive value, such as 0.1. This makes\nit very likely that the rectified linear units will be initially active for most inputs\nin the training set and allow the derivatives to pass through.\nSeveral generalizations of rectified linear units exist. Most of these generalizations perform comparably to rectified linear units and occasionally perform\nbetter.\nOne drawback to rectified linear units is that they cannot learn via gradientbased methods on examples for which their activation is zero. A variety of\ngeneralizations of rectified linear units guarantee that they receive gradient everywhere.\nThree generalizations of rectified linear units are based on using a non-zero\nslope \u03b1i when zi < 0: hi = g(z, \u03b1)i = max(0, zi) + \u03b1i min(0, zi). Absolute value\nrectification fixes \u03b1i = \u22121 to obtain g(z) = |z|. It is used for object recognition\nfrom images (Jarrett et al., 2009), where it makes sense to seek features that are\ninvariant under a polarity reversal of the input illumination. Other generalizations\nof rectified linear units are more broadly applicable. A leaky ReLU (Maas et al.,\n2013) fixes \u03b1i to a small value like 0.01 while a parametric ReLU or PReLU\ntreats \u03b1i as a learnable parameter (He et al., 2015).\nMaxout units (Goodfellow et al., 2013a) generalize rectified linear units\nfurther. Instead of applying an element-wise function g(z ), maxout units divide z\ninto groups of k values. Each maxout unit then outputs the maximum element of\n193\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\none of these groups:\ng(z)i = max\nj\u2208G(i)\nzj (6.37)\nwhere G(i) is the set of indices into the inputs for group i, {(i \u2212 1)k + 1, . . . ,ik}.\nThis provides a way of learning a piecewise linear function that responds to multiple\ndirections in the input x space.\nA maxout unit can learn a piecewise linear, convex function with up to k pieces.\nMaxout units can thus be seen as learning the activation function itself rather\nthan just the relationship between units. With large enough k, a maxout unit can\nlearn to approximate any convex function with arbitrary fidelity. In particular,\na maxout layer with two pieces can learn to implement the same function of the\ninput x as a traditional layer using the rectified linear activation function, absolute\nvalue rectification function, or the leaky or parametric ReLU, or can learn to\nimplement a totally different function altogether. The maxout layer will of course\nbe parametrized differently from any of these other layer types, so the learning\ndynamics will be different even in the cases where maxout learns to implement the\nsame function of x as one of the other layer types.\nEach maxout unit is now parametrized by k weight vectors instead of just one,\nso maxout units typically need more regularization than rectified linear units. They\ncan work well without regularization if the training set is large and the number of\npieces per unit is kept low (Cai et al., 2013).\nMaxout units have a few other benefits. In some cases, one can gain some statistical and computational advantages by requiring fewer parameters. Specifically,\nif the features captured by n different linear filters can be summarized without\nlosing information by taking the max over each group of k features, then the next\nlayer can get by with k times fewer weights.\nBecause each unit is driven by multiple filters, maxout units have some redundancy that helps them to resist a phenomenon called catastrophic forgetting\nin which neural networks forget how to perform tasks that they were trained on in\nthe past (Goodfellow et al., 2014a).\nRectified linear units and all of these generalizations of them are based on the\nprinciple that models are easier to optimize if their behavior is closer to linear.\nThis same general principle of using linear behavior to obtain easier optimization\nalso applies in other contexts besides deep linear networks. Recurrent networks can\nlearn from sequences and produce a sequence of states and outputs. When training\nthem, one needs to propagate information through several time steps, which is much\neasier when some linear computations (with some directional derivatives being of\nmagnitude near 1) are involved. One of the best-performing recurrent network\n194\nCHAPTER 6. DEEP FEEDFORWARD NETWORKS\narchitectures, the LSTM, propagates information through time via summation\u2014a\nparticular straightforward kind of such linear activation. This is discussed further\nin section\"\nSummarize the above basic principle of ReLU actication function for neural networks with equations, try to use your own words, do not directly copy the sentence from the above paper.\n"}, {"role": "assistant", "content": "The Rectified Linear Unit (ReLU) activation function, represented as \\(g(z) = \\max(0, z)\\), plays a critical role in the design of neural networks, particularly due to its simplicity and effectiveness in facilitating the optimization process. The primary appeal of ReLUs lies in their near-linear characteristics, which allow gradients to flow through the network without being heavily dampened as can be the case with nonlinear activation functions. This is especially beneficial during the backpropagation process where gradients are used to update the network's weights. \n\nWhen the input \\(z\\) to a ReLU is positive, the gradient remains large and consistent, specifically, the derivative of \\(g(z)\\) with respect to \\(z\\) is 1 for \\(z > 0\\), allowing for efficient learning during these phases. On the contrary, for negative inputs where \\(z \\leq 0\\), the output is fixed at 0, leading to a gradient of 0 as well. This simple switch between a linear behavior (for positive inputs) and zero output (for negative inputs) facilitates an efficient optimization process, avoiding the vanishing or exploding gradient problems that are common with purely linear or highly nonlinear activation functions.\n\nFurther development led to the generalization of ReLUs to address certain limitations, such as the inability to learn from negative inputs due to the zero-gradient problem. Variants like Leaky ReLU and Parametric ReLU (PReLU) introduce a non-zero gradient for \\(z < 0\\), allowing learning in this regime as well. Specifically, for these variants, the equation becomes \\(g(z, \\alpha) = \\max(0, z) + \\alpha \\min(0, z)\\), where \\(\\alpha\\) is a small constant or learnable parameter that ensures a non-zero gradient for negative inputs.\n\nAnother advanced variant, the Maxout unit, goes further by learning a piecewise linear function that can adapt to multiple directions in the input space. It divides the input \\(z\\) into groups and outputs the maximum value within each group. Thus, Maxout can approximate any convex function with sufficient pieces, making it a highly flexible choice for modeling nonlinear phenomena.\n\nIn summary, the core principle behind the ReLU and its generalizations is to facilitate network optimization by maintaining linear or near-linear behavior as much as possible. This is achieved by allowing large and consistent gradients for active units, and by introducing mechanisms to avoid dead neurons (units that stop learning because of consistent non-positive inputs) through variants that can handle negative inputs more effectively."}]}, {"facetValue": "The user provided a detailed technical document about convolutional neural networks and requested a summary with equations. The response involved interpreting and condensing the content into a clear explanation without copying text directly.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed technical document about convolutional neural networks and requested a summary with equations. The response involved interpreting and condensing the content into a clear explanation without copying text directly."}], "conversation": [{"role": "user", "content": "\"A. Convolution\nLet\u2019s assume that the input of our neural network has the\npresented shape in Fig. 2, It can be an image (e.g. color image\nof a CIFAR-10 dataset with a width and height of 32\u00d732\npixels, and a depth of 3 which RGB channel) or a video (gray\nscale video whose height and width are the resolution, and the\ndepth are the frames) or even an experimental video, which\nhas width and height of (L \u00d7 L) sensor values, and the depths\nare associated with different time frames, as in [2,10,15].\nWhy convolution ? . Let's assume that the network receives\nraw pixels as input. Therefore, to connect the input layer to\nonly one neuron (e.g. in the hidden layer in the Multi- Layer\nperceptron), as an example, there should be 32\u00d732\u00d73 weight\nconnections for the CIFAR-10 dataset.\nFig. 1. Learned features from a Convolutional Neural Network\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\nFig. 2. Three dimensional Input representation of CNN\nIf we add one more neuron into the hidden layer, then we will\nneed another 32\u00d732\u00d73 weight connection, which will become\nin total, 32\u00d732\u00d73\u00d72 parameters. To make it clearer, more than\n6000 weight parameters are used to connect the input to just\nonly two nodes. It may be thought that two neurons might not\nbe enough for any useful processing for an image\nclassification application. To make it more efficient, we can\nconnect the input image to the neurons in the next layer with\nexactly the same values for the height and width. It can be\nassumed this network is applied for the type of processing\nsuch as the edge in the image. However, the mentioned\nnetwork needs 32\u00d732\u00d73 by 32\u00d732 weight connections, which\nare (3,145,728), as in [4,5,14].\nTherefore, looking for a more efficient method, it emerged\nthat instead of a full connection, it is a good idea to look for\nlocal regions in the picture instead of in the whole image.\nFig. 3, shows a regional connection for the next layer. In other\nwords, the hidden neurons in the next layer only get inputs\nfrom the corresponding part of the previous layer. For\nexample, it can only be connected to 5\u00d75 neurons. Thus, if we\nwant to have 32\u00d732 neurons in the next layer, then we will\nhave 5\u00d75\u00d73 by 32x32 connections which is 76,800\nconnections (compared to 3,145,728 for full connectivity), as\nin [1,9,13,14,17].\nAlthough the size of connection drastically dropped, it still\nleaves so many parameters to solve. Another assumption for\nsimplification, is to keep the local connection weights fixed\nfor the entire neurons of the next layer. This will connect the\nneighbor neurons in the next layer with exactly the same\nweight to the local region of the previous layer. Therefore, it\nagain drops many extra parameters, and reduces the number of\nweights to only 5\u00d75\u00d73=75 to connect 32\u00d732\u00d73 neurons to\n32\u00d732 in the next layer [5,8,11].\nThere are many benefits to these simple assumptions. Firstly,\nthe number of connections decrease from around 3\nmillion to only 75 connections in the presented example.\nSecondly, and a more interesting concept, is that fixing the\nweights for the local connections is similar to sliding a\nwindow of 5\u00d75\u00d73 in the input neurons and mapping the\ngenerated output to the corresponding place. It provides an\nopportunity to detect and recognize features regardless of their\npositions in the image. This is the reason why they are called\nconvolutions [6,7,16].\nFig. 3. Convolution as alternative for fully connected network.\nTo show the astounding effect of the convolution matrix,\nFig.4, depicts what will happen if we manually pick the\nconnection weight in a 3\u00d73 window.\nAs we can see Fig.4, the matrix can be set to detect edges\nin the image. These matrices are also called a filter because\nthey act like the classic filters in the image processing.\nHowever, in the convolutional neural network these filters are\ninitialized, followed by the training procedure shape filters,\nwhich are more suitable for the given task.\nTo make this method more beneficial, it is possible to add\nmore layers after the input layer. Each layer can be associated\nwith different filters. Therefore, we can extract different\nfeatures from the given image. Fig. 5, shows how they are\nconnected to the different layers. Each layer has its own filter\nand therefore extracts different features from the input. The\nneurons shown in Fig. 5, use a different filter, but look at the\nsame part of the input image. [6,8,15,17]\n\nFig.4. Effects of different convolution matrix.\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\n\nFig. 5. Multiple layers which each of them correspond to different filter\nbut looking at the same region in the given image\nB. Stride\n In fact, CNN has more options which provide a lot of\nopportunities to even decrease the parameters more and more,\nand at the same time reduce some of the side effects. One of\nthese options is stride. In the above mentioned example, it is\nsimply assumed that the next layer\u2019s node has lots of overlaps\nwith their neighbors by looking at the regions. We can\nmanipulate the overlap by controlling the stride. Fig. 6, shows\na given 7\u00d77 image. If we move the filter one node every time,\nwe can have a 5x5 output only. Note that the output of the\nthree left matrices in Fig. 6, have an overlap (and three middle\nones together and three right ones also). However, if we move\nand make every stride 2, then the output will be 3x3. Put\nsimply, not only overlap, but also the size of the output will be\nreduced. [5,12,16].\nEquation (1), formalize this, given the image N\u00d7N\ndimension and the filter size of the F\u00d7F, the output size O as\nshown in Fig. 7.\n (1)\nWhere N is the input size, F is the filter size, and S is the\nstride size.\nC. Padding\nOne of the drawbacks of the convolution step is the loss of\ninformation that might exist on the border of the image.\nBecause they are only captured when the filter slides, they\nnever have the chance to be seen. A very simple, yet efficient\nmethod to resolve the issue, is to use zero-padding. The other\nbenefit of zero padding is to manage the output size. For\nexample, in Fig. 6, with N=7 and F=3 and stride 1, the output\nwill be 5\u00d75 (which shrinks from a 7\u00d77 input).\nFig. 7. The effect of stride in the output.\nHowever, by adding one zero-padding, the output will be\n7\u00d77, which is exactly the same as the original input (The\nactual N now becomes 9, use the formula (1). The modified\nformula including zero-padding is formula (2).\n\n (2)\nWhere P is the number of the layers of the zero-padding\n(e.g. P=1 in Fig. 8), This padding idea helps us to prevent\nnetwork output size from shrinking with depth. Therefore, it is\npossible to have any number of deep convolutional networks.\n[2,12].\nD. Feature of CNNs\nThe weight sharing brings invariance translations to the\nmodel. It helps to filter the learn feature regardless of the\nspatial properties. By starting random values for the filters,\nthey will learn to detect the edge (such as in Fig. 4) if it\nimproves the performance. It is important to remember that if\nwe need to know that something is spatially important in the\ngiven input, then it is an extremely bad idea to use a shared\nweight.\nFig. 8. Zero-padding\nFig. 6. Stride 1, the filter window moves only one time for each connection\n0 0 0 0 0 0 0 0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0 0 0 0 0 0 0 0\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\nFigure (9). Visualizing Convolutional deep neural network layers\n This concept can be extended to different dimensions\nalso. For example, if it is sequential data such as an audio,\nthen it can employ a one dimensional audio. If it is an\nimage, as it is shown, two dimensional convolutions can be\napplied. And for videos, or 3D images, a three dimensional\nconvolution can be used. This simple idea beat all the\nclassic object recognition methods in computer vision in the\n2012 ImageNet challenge as shown in Fig. 9, [5,14,18]\nE. Convolutional Formula\nThe convolution for one pixel in the next layer is\ncalculated according to the formula (3) .\nWhere is the output in the next layer, is the\ninput image and is the kernel or filter matrix and is the\nconvolution operation. Fig. 10, shows how the convolution\nworks. As can be seen, the element by element product of\nthe input and kernel is aggregated, and then represents the\ncorresponding point in the next layer. [4,9].\nIII. NONLINEARITY\nThe next layer after the convolution is non-linearity. The\nnon-linearity can be used to adjust or cut-off the generated\noutput. This layer is applied in order to saturate the output or\nlimiting the generated output.\n\nFig. 10. Details on Convolution layer\nFor many years, sigmoid and tanh were the most popular\nnon-linearity. Fig. 11, shows the common types of\nnonlinearity. However, recently, the Rectified Linear Unit\n(ReLU) has been used more often for the following reasons.\n1. ReLU has simpler definitions in both function and\ngradient.\n (4)\n (5)\n2. The saturated function such as sigmoid and tanh\ncause problems in the back propagation. As the\nneural network design is deeper, the gradient signal\nbegins to\nvanish, which is called the \u201cvanishing gradient\u201d.\nThis happens since the gradient of those functions is\nvery\nclose to zero almost everywhere but the center.\nHowever, the ReLU has a constant gradient for the\npositive input. Although the function is not\ndifferentiable, it can be ignored in the actual\nimplementation.\n3. The ReLU creates a sparser representation. because\nthe zero in the gradient leads to obtaining a\ncomplete zero. However, sigmoid and tanh always\nhave non-zero results from the gradient, which\nmight not be in favor for training. [2,5,13].\nFig. 11. Common types of nonlinearity.\n(3)\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\nIV. POOLING\nThe main idea of pooling is down-sampling in order to\nreduce the complexity for further layers. In the image\nprocessing domain, it can be considered as similar to\nreducing the resolution. Pooling does not affect the number\nof filters. Max-pooling is one of the most common types of\npooling methods. It partitions the image to sub-region\nrectangles, and it only returns the maximum value of the\ninside of that sub-region. One of the most common sizes\nused in max-pooling is 2\u00d72. As can see in Fig. 12, when\npooling is performed in the top-left 2\u00d72 blocks (pink area),\nit moves 2 and focus on top-right part. This means that\nstride 2 is used in pooling. To avoid down-sampling, stride\n1 can be used, which is not common. It should be\nconsidered that down-sampling does not preserve the\nposition of the information. Therefore, it should be applied\nonly when the presence of information is important (rather\nthan spatial information). Moreover, pooling can be used\nwith non-equal filters and strides to improve the efficiency.\nFor example, a 3x3 max-pooling with stride 2 keeps some\noverlaps between the areas. [5,10,16].\nV. FULLY-CONNECTED LAYER\nThe fully-connected layer is a similar to the way that neurons\nare arranged in a traditional neural network. Therefore, each node\nin a fully-connected layer is directly connected to every node in\nboth the previous and in the next layer as shown in Fig. 13, From\nthis figure we can note that each of the nodes in the last frames in\nthe pooling layer are connected as a vector to the first layer from\nthe fully-connected layer. These are the most parameters used with\nthe CNN within these layers, and take a long time in training [3,8].\nThe major drawback of a fully-connected layer, is that it\nincludes a lot of parameters that need complex computational in\ntraining examples. Therefore, we try to eliminate the number of\nnodes and connections. The removed nodes and connection can be\nsatisfied by using the dropout technique. For example, LeNet and\nAlexNet designed a deep and wide network while keeping the\ncomputational complex constant [4,6,9].\nThe essence of the CNN network, which is the\nconvolution, is when the nonlinearity and pooling layer are\nintroduced. Most common architecture uses three of them as\"\nFrom the above paper, explain and summarize the principle of CNN in detail with equations. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "Convolutional Neural Networks (CNNs) are a class of deep learning architectures that are particularly powerful for tasks involving image recognition, but they can also be applied to other types of data such as audio or time-series data. The core principle behind CNNs is their ability to automatically and adaptively learn spatial hierarchies of features from input data. Here's a detailed explanation of the principles and components of CNNs, articulated using fresh expressions:\n\n### Convolution Layer\n\nThe convolution layer is the backbone of a CNN, employing filters (also known as kernels) to capture spatial features such as edges or textures within an input image. For instance, consider an image represented as a 32x32x3 matrix (for a 32x32 pixel image with three color channels). The convolution operation involves sliding a smaller matrix (filter) across the image and computing the dot product of the filter with local regions of the input. This process generates a feature map that accentuates features detected by the filter.\n\nThis method reduces the number of parameters significantly compared to fully connected networks. If a filter is of size 5x5x3, instead of connecting each input pixel to every neuron in the next layer, each neuron in the ensuing layer is connected to a small region of the input through shared weights (75 weights in this example). This weight sharing mechanism drastically cuts down the number of free parameters, making the model computationally efficient and less prone to overfitting.\n\n### Stride and Padding\n\nStride controls how the filter convolves around the input. For example, a stride of 1 moves the filter one pixel at a time, while a stride of 2 moves it two pixels. Adjusting the stride affects the size of the feature maps produced.\n\nPadding involves adding layers of zeros around the input to allow the filter to slide across border pixels, thus maintaining the spatial size of the output feature map closer to the input.\n\n### Non-Linearity (Activation function)\n\nAfter each convolution operation, a non-linear function is applied to the feature maps. This is crucial since most real-world data we want the network to learn from is non-linear. ReLU (Rectified Linear Unit) is the most common activation function used in CNNs due to its computational efficiency and its ability to help mitigate the vanishing gradient problem, thereby promoting faster convergence.\n\n### Pooling\n\nPooling layers subsample (downsample) the feature map, reducing its dimensionality. This contributes to making the detection of features invariant to scale and orientation changes and also improves computational efficiency. Max pooling is a frequent choice, which takes the maximum value in each sub-region of the feature map.\n\n### Fully Connected Layers\n\nTowards the end of the network, fully connected layers are used where neurons between adjacent layers are fully connected. This part is responsible for mapping the pooled features into the final output, such as class scores in classification tasks. It\u2019s common to integrate dropout in these layers to prevent overfitting by randomly \"dropping out\" a number of neuron outputs during training.\n\nIn essence, a CNN learns to recognize patterns and features by applying filters, pooling, and non-linear operations, extracting increasingly complex features in each subsequent layer and using those features to perform tasks such as image classification. The powerful aspect of CNN is its ability to learn the filters\u2019 weights automatically through backpropagation, optimizing its internal parameters to minimize the error between the predicted and true labels."}]}, {"facetValue": "The user provided a detailed technical document about convolutional neural networks and requested a summary with equations. The response focused on explaining the principles of CNNs, including convolution, non-linearity, pooling, and fully connected layers, while avoiding direct copying of the original text.", "allFacetValues": [{"facet": "Journal", "value": "The user provided a detailed technical document about convolutional neural networks and requested a summary with equations. The response focused on explaining the principles of CNNs, including convolution, non-linearity, pooling, and fully connected layers, while avoiding direct copying of the original text."}], "conversation": [{"role": "user", "content": "\"A. Convolution\nLet\u2019s assume that the input of our neural network has the\npresented shape in Fig. 2, It can be an image (e.g. color image\nof a CIFAR-10 dataset with a width and height of 32\u00d732\npixels, and a depth of 3 which RGB channel) or a video (gray\nscale video whose height and width are the resolution, and the\ndepth are the frames) or even an experimental video, which\nhas width and height of (L \u00d7 L) sensor values, and the depths\nare associated with different time frames, as in [2,10,15].\nWhy convolution ? . Let's assume that the network receives\nraw pixels as input. Therefore, to connect the input layer to\nonly one neuron (e.g. in the hidden layer in the Multi- Layer\nperceptron), as an example, there should be 32\u00d732\u00d73 weight\nconnections for the CIFAR-10 dataset.\nFig. 1. Learned features from a Convolutional Neural Network\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\nFig. 2. Three dimensional Input representation of CNN\nIf we add one more neuron into the hidden layer, then we will\nneed another 32\u00d732\u00d73 weight connection, which will become\nin total, 32\u00d732\u00d73\u00d72 parameters. To make it clearer, more than\n6000 weight parameters are used to connect the input to just\nonly two nodes. It may be thought that two neurons might not\nbe enough for any useful processing for an image\nclassification application. To make it more efficient, we can\nconnect the input image to the neurons in the next layer with\nexactly the same values for the height and width. It can be\nassumed this network is applied for the type of processing\nsuch as the edge in the image. However, the mentioned\nnetwork needs 32\u00d732\u00d73 by 32\u00d732 weight connections, which\nare (3,145,728), as in [4,5,14].\nTherefore, looking for a more efficient method, it emerged\nthat instead of a full connection, it is a good idea to look for\nlocal regions in the picture instead of in the whole image.\nFig. 3, shows a regional connection for the next layer. In other\nwords, the hidden neurons in the next layer only get inputs\nfrom the corresponding part of the previous layer. For\nexample, it can only be connected to 5\u00d75 neurons. Thus, if we\nwant to have 32\u00d732 neurons in the next layer, then we will\nhave 5\u00d75\u00d73 by 32x32 connections which is 76,800\nconnections (compared to 3,145,728 for full connectivity), as\nin [1,9,13,14,17].\nAlthough the size of connection drastically dropped, it still\nleaves so many parameters to solve. Another assumption for\nsimplification, is to keep the local connection weights fixed\nfor the entire neurons of the next layer. This will connect the\nneighbor neurons in the next layer with exactly the same\nweight to the local region of the previous layer. Therefore, it\nagain drops many extra parameters, and reduces the number of\nweights to only 5\u00d75\u00d73=75 to connect 32\u00d732\u00d73 neurons to\n32\u00d732 in the next layer [5,8,11].\nThere are many benefits to these simple assumptions. Firstly,\nthe number of connections decrease from around 3\nmillion to only 75 connections in the presented example.\nSecondly, and a more interesting concept, is that fixing the\nweights for the local connections is similar to sliding a\nwindow of 5\u00d75\u00d73 in the input neurons and mapping the\ngenerated output to the corresponding place. It provides an\nopportunity to detect and recognize features regardless of their\npositions in the image. This is the reason why they are called\nconvolutions [6,7,16].\nFig. 3. Convolution as alternative for fully connected network.\nTo show the astounding effect of the convolution matrix,\nFig.4, depicts what will happen if we manually pick the\nconnection weight in a 3\u00d73 window.\nAs we can see Fig.4, the matrix can be set to detect edges\nin the image. These matrices are also called a filter because\nthey act like the classic filters in the image processing.\nHowever, in the convolutional neural network these filters are\ninitialized, followed by the training procedure shape filters,\nwhich are more suitable for the given task.\nTo make this method more beneficial, it is possible to add\nmore layers after the input layer. Each layer can be associated\nwith different filters. Therefore, we can extract different\nfeatures from the given image. Fig. 5, shows how they are\nconnected to the different layers. Each layer has its own filter\nand therefore extracts different features from the input. The\nneurons shown in Fig. 5, use a different filter, but look at the\nsame part of the input image. [6,8,15,17]\n\nFig.4. Effects of different convolution matrix.\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\n\nFig. 5. Multiple layers which each of them correspond to different filter\nbut looking at the same region in the given image\nB. Stride\n In fact, CNN has more options which provide a lot of\nopportunities to even decrease the parameters more and more,\nand at the same time reduce some of the side effects. One of\nthese options is stride. In the above mentioned example, it is\nsimply assumed that the next layer\u2019s node has lots of overlaps\nwith their neighbors by looking at the regions. We can\nmanipulate the overlap by controlling the stride. Fig. 6, shows\na given 7\u00d77 image. If we move the filter one node every time,\nwe can have a 5x5 output only. Note that the output of the\nthree left matrices in Fig. 6, have an overlap (and three middle\nones together and three right ones also). However, if we move\nand make every stride 2, then the output will be 3x3. Put\nsimply, not only overlap, but also the size of the output will be\nreduced. [5,12,16].\nEquation (1), formalize this, given the image N\u00d7N\ndimension and the filter size of the F\u00d7F, the output size O as\nshown in Fig. 7.\n (1)\nWhere N is the input size, F is the filter size, and S is the\nstride size.\nC. Padding\nOne of the drawbacks of the convolution step is the loss of\ninformation that might exist on the border of the image.\nBecause they are only captured when the filter slides, they\nnever have the chance to be seen. A very simple, yet efficient\nmethod to resolve the issue, is to use zero-padding. The other\nbenefit of zero padding is to manage the output size. For\nexample, in Fig. 6, with N=7 and F=3 and stride 1, the output\nwill be 5\u00d75 (which shrinks from a 7\u00d77 input).\nFig. 7. The effect of stride in the output.\nHowever, by adding one zero-padding, the output will be\n7\u00d77, which is exactly the same as the original input (The\nactual N now becomes 9, use the formula (1). The modified\nformula including zero-padding is formula (2).\n\n (2)\nWhere P is the number of the layers of the zero-padding\n(e.g. P=1 in Fig. 8), This padding idea helps us to prevent\nnetwork output size from shrinking with depth. Therefore, it is\npossible to have any number of deep convolutional networks.\n[2,12].\nD. Feature of CNNs\nThe weight sharing brings invariance translations to the\nmodel. It helps to filter the learn feature regardless of the\nspatial properties. By starting random values for the filters,\nthey will learn to detect the edge (such as in Fig. 4) if it\nimproves the performance. It is important to remember that if\nwe need to know that something is spatially important in the\ngiven input, then it is an extremely bad idea to use a shared\nweight.\nFig. 8. Zero-padding\nFig. 6. Stride 1, the filter window moves only one time for each connection\n0 0 0 0 0 0 0 0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0\n0 0 0 0 0 0 0 0 0\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\nFigure (9). Visualizing Convolutional deep neural network layers\n This concept can be extended to different dimensions\nalso. For example, if it is sequential data such as an audio,\nthen it can employ a one dimensional audio. If it is an\nimage, as it is shown, two dimensional convolutions can be\napplied. And for videos, or 3D images, a three dimensional\nconvolution can be used. This simple idea beat all the\nclassic object recognition methods in computer vision in the\n2012 ImageNet challenge as shown in Fig. 9, [5,14,18]\nE. Convolutional Formula\nThe convolution for one pixel in the next layer is\ncalculated according to the formula (3) .\nWhere is the output in the next layer, is the\ninput image and is the kernel or filter matrix and is the\nconvolution operation. Fig. 10, shows how the convolution\nworks. As can be seen, the element by element product of\nthe input and kernel is aggregated, and then represents the\ncorresponding point in the next layer. [4,9].\nIII. NONLINEARITY\nThe next layer after the convolution is non-linearity. The\nnon-linearity can be used to adjust or cut-off the generated\noutput. This layer is applied in order to saturate the output or\nlimiting the generated output.\n\nFig. 10. Details on Convolution layer\nFor many years, sigmoid and tanh were the most popular\nnon-linearity. Fig. 11, shows the common types of\nnonlinearity. However, recently, the Rectified Linear Unit\n(ReLU) has been used more often for the following reasons.\n1. ReLU has simpler definitions in both function and\ngradient.\n (4)\n (5)\n2. The saturated function such as sigmoid and tanh\ncause problems in the back propagation. As the\nneural network design is deeper, the gradient signal\nbegins to\nvanish, which is called the \u201cvanishing gradient\u201d.\nThis happens since the gradient of those functions is\nvery\nclose to zero almost everywhere but the center.\nHowever, the ReLU has a constant gradient for the\npositive input. Although the function is not\ndifferentiable, it can be ignored in the actual\nimplementation.\n3. The ReLU creates a sparser representation. because\nthe zero in the gradient leads to obtaining a\ncomplete zero. However, sigmoid and tanh always\nhave non-zero results from the gradient, which\nmight not be in favor for training. [2,5,13].\nFig. 11. Common types of nonlinearity.\n(3)\nAuthorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply.\nICET2017, Antalya, Turkey\n978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE\nIV. POOLING\nThe main idea of pooling is down-sampling in order to\nreduce the complexity for further layers. In the image\nprocessing domain, it can be considered as similar to\nreducing the resolution. Pooling does not affect the number\nof filters. Max-pooling is one of the most common types of\npooling methods. It partitions the image to sub-region\nrectangles, and it only returns the maximum value of the\ninside of that sub-region. One of the most common sizes\nused in max-pooling is 2\u00d72. As can see in Fig. 12, when\npooling is performed in the top-left 2\u00d72 blocks (pink area),\nit moves 2 and focus on top-right part. This means that\nstride 2 is used in pooling. To avoid down-sampling, stride\n1 can be used, which is not common. It should be\nconsidered that down-sampling does not preserve the\nposition of the information. Therefore, it should be applied\nonly when the presence of information is important (rather\nthan spatial information). Moreover, pooling can be used\nwith non-equal filters and strides to improve the efficiency.\nFor example, a 3x3 max-pooling with stride 2 keeps some\noverlaps between the areas. [5,10,16].\nV. FULLY-CONNECTED LAYER\nThe fully-connected layer is a similar to the way that neurons\nare arranged in a traditional neural network. Therefore, each node\nin a fully-connected layer is directly connected to every node in\nboth the previous and in the next layer as shown in Fig. 13, From\nthis figure we can note that each of the nodes in the last frames in\nthe pooling layer are connected as a vector to the first layer from\nthe fully-connected layer. These are the most parameters used with\nthe CNN within these layers, and take a long time in training [3,8].\nThe major drawback of a fully-connected layer, is that it\nincludes a lot of parameters that need complex computational in\ntraining examples. Therefore, we try to eliminate the number of\nnodes and connections. The removed nodes and connection can be\nsatisfied by using the dropout technique. For example, LeNet and\nAlexNet designed a deep and wide network while keeping the\ncomputational complex constant [4,6,9].\nThe essence of the CNN network, which is the\nconvolution, is when the nonlinearity and pooling layer are\nintroduced. Most common architecture uses three of them as\"\nFrom the above paper, explain and summarize the principle of CNN in detail with equations. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "Convolutional Neural Networks (CNNs) are a class of deep learning models primarily used for processing structured grid data such as images. At the heart of CNNs is the ability to automatically and adaptively learn spatial hierarchies of features from input data. The architecture of CNNs enables them to efficiently handle the high dimensionality of raw data through a combination of convolutional layers, non-linearity, pooling, and fully connected layers. Let's break down these components and their principles.\n\n### A. Convolution\n\nThe convolutional layer is the core building block of a CNN. It applies a series of learnable filters to the input data. Each filter slides over the input data and performs an element-wise multiplication with the part of the image it is currently on, and sums up the results into a single output pixel. This process is mathematically described as:\n\n\\[ O_{ij} = K * I \\]\n\nwhere \\(O_{ij}\\) is the output of the filter applied at position \\((i, j)\\) on the input image \\(I\\), \\(K\\) is the kernel or filter, and \\(*\\) denotes the convolution operation.\n\nThe convolution operation has several advantages: it significantly reduces the number of parameters by sharing weights (a single filter is used across the entire input), it maintains the spatial relationship between pixels, and it allows the network to learn filters that activate when they see specific types of features at some spatial position in the input.\n\n### B. Non-Linearity (Activation Function)\n\nAfter each convolution operation, a non-linear activation function is applied to the output. This is necessary because the combination of linear operations (such as convolution) can only learn linear transformations, and many real-world data distributions are highly non-linear. The most commonly used activation function in modern CNNs is the Rectified Linear Unit (ReLU), described by:\n\n\\[ f(x) = \\max(0, x) \\]\n\nReLU introduces non-linearity while being computationally efficient, allowing models to learn faster and achieve better performance.\n\n### C. Pooling (Subsampling or Downsampling)\n\nPooling layers are used to reduce the dimensionality of the feature maps. This not only reduces the computational load and the number of parameters but also provides an abstracted form of the features, making the network more robust to variations in the position of features in the input. A common pooling operation is max pooling, where the maximum value from a group of neurons in the feature map is selected:\n\n\\[ P = \\max (X_{subregion}) \\]\n\nThis operation is applied separately for each feature map and reduces its size by a factor, depending on the size of the pooling window.\n\n### D. Fully-Connected Layers\n\nAfter several convolutional and pooling layers, the high-level reasoning in the network is done through fully connected layers. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular neural networks. The output from the last pooling or convolutional layer, which is a high-level feature representation of the input, is flattened and fed into these fully-connected layers to perform classification or regression.\n\nFully-connected layers are often followed by a softmax function if the task is classification, which converts the output scores from the network into probabilities.\n\nThe architecture and components of CNNs allow them to learn powerful hierarchical feature representations from raw data, making them especially suited for tasks like image recognition, where spatial hierarchies are key. Through a combination of convolutional layers for feature extraction, non-linearity for introducing complex patterns recognition, pooling for dimensionality reduction, and fully connected layers for classification, CNNs can effectively learn to make sense of spatial data."}]}, {"facetValue": "The user asked about the principles of convolutional neural networks and provided a detailed text for explanation. The response involved summarizing the content and explaining the key concepts of CNNs, including convolution, stride, padding, nonlinearity, pooling, and fully-connected layers.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the principles of convolutional neural networks and provided a detailed text for explanation. The response involved summarizing the content and explaining the key concepts of CNNs, including convolution, stride, padding, nonlinearity, pooling, and fully-connected layers."}], "conversation": [{"role": "user", "content": "\"A. Convolution Let\u2019s assume that the input of our neural network has the presented shape in Fig. 2, It can be an image (e.g. color image of a CIFAR-10 dataset with a width and height of 32\u00d732 pixels, and a depth of 3 which RGB channel) or a video (gray scale video whose height and width are the resolution, and the depth are the frames) or even an experimental video, which has width and height of (L \u00d7 L) sensor values, and the depths are associated with different time frames, as in [2,10,15]. Why convolution ? . Let's assume that the network receives raw pixels as input. Therefore, to connect the input layer to only one neuron (e.g. in the hidden layer in the Multi- Layer perceptron), as an example, there should be 32\u00d732\u00d73 weight connections for the CIFAR-10 dataset. Fig. 1. Learned features from a Convolutional Neural Network Authorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply. ICET2017, Antalya, Turkey 978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE Fig. 2. Three dimensional Input representation of CNN If we add one more neuron into the hidden layer, then we will need another 32\u00d732\u00d73 weight connection, which will become in total, 32\u00d732\u00d73\u00d72 parameters. To make it clearer, more than 6000 weight parameters are used to connect the input to just only two nodes. It may be thought that two neurons might not be enough for any useful processing for an image classification application. To make it more efficient, we can connect the input image to the neurons in the next layer with exactly the same values for the height and width. It can be assumed this network is applied for the type of processing such as the edge in the image. However, the mentioned network needs 32\u00d732\u00d73 by 32\u00d732 weight connections, which are (3,145,728), as in [4,5,14]. Therefore, looking for a more efficient method, it emerged that instead of a full connection, it is a good idea to look for local regions in the picture instead of in the whole image. Fig. 3, shows a regional connection for the next layer. In other words, the hidden neurons in the next layer only get inputs from the corresponding part of the previous layer. For example, it can only be connected to 5\u00d75 neurons. Thus, if we want to have 32\u00d732 neurons in the next layer, then we will have 5\u00d75\u00d73 by 32x32 connections which is 76,800 connections (compared to 3,145,728 for full connectivity), as in [1,9,13,14,17]. Although the size of connection drastically dropped, it still leaves so many parameters to solve. Another assumption for simplification, is to keep the local connection weights fixed for the entire neurons of the next layer. This will connect the neighbor neurons in the next layer with exactly the same weight to the local region of the previous layer. Therefore, it again drops many extra parameters, and reduces the number of weights to only 5\u00d75\u00d73=75 to connect 32\u00d732\u00d73 neurons to 32\u00d732 in the next layer [5,8,11]. There are many benefits to these simple assumptions. Firstly, the number of connections decrease from around 3 million to only 75 connections in the presented example. Secondly, and a more interesting concept, is that fixing the weights for the local connections is similar to sliding a window of 5\u00d75\u00d73 in the input neurons and mapping the generated output to the corresponding place. It provides an opportunity to detect and recognize features regardless of their positions in the image. This is the reason why they are called convolutions [6,7,16]. Fig. 3. Convolution as alternative for fully connected network. To show the astounding effect of the convolution matrix, Fig.4, depicts what will happen if we manually pick the connection weight in a 3\u00d73 window. As we can see Fig.4, the matrix can be set to detect edges in the image. These matrices are also called a filter because they act like the classic filters in the image processing. However, in the convolutional neural network these filters are initialized, followed by the training procedure shape filters, which are more suitable for the given task. To make this method more beneficial, it is possible to add more layers after the input layer. Each layer can be associated with different filters. Therefore, we can extract different features from the given image. Fig. 5, shows how they are connected to the different layers. Each layer has its own filter and therefore extracts different features from the input. The neurons shown in Fig. 5, use a different filter, but look at the same part of the input image. [6,8,15,17] Fig.4. Effects of different convolution matrix. Authorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply. ICET2017, Antalya, Turkey 978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE Fig. 5. Multiple layers which each of them correspond to different filter but looking at the same region in the given image B. Stride In fact, CNN has more options which provide a lot of opportunities to even decrease the parameters more and more, and at the same time reduce some of the side effects. One of these options is stride. In the above mentioned example, it is simply assumed that the next layer\u2019s node has lots of overlaps with their neighbors by looking at the regions. We can manipulate the overlap by controlling the stride. Fig. 6, shows a given 7\u00d77 image. If we move the filter one node every time, we can have a 5x5 output only. Note that the output of the three left matrices in Fig. 6, have an overlap (and three middle ones together and three right ones also). However, if we move and make every stride 2, then the output will be 3x3. Put simply, not only overlap, but also the size of the output will be reduced. [5,12,16]. Equation (1), formalize this, given the image N\u00d7N dimension and the filter size of the F\u00d7F, the output size O as shown in Fig. 7. (1) Where N is the input size, F is the filter size, and S is the stride size. C. Padding One of the drawbacks of the convolution step is the loss of information that might exist on the border of the image. Because they are only captured when the filter slides, they never have the chance to be seen. A very simple, yet efficient method to resolve the issue, is to use zero-padding. The other benefit of zero padding is to manage the output size. For example, in Fig. 6, with N=7 and F=3 and stride 1, the output will be 5\u00d75 (which shrinks from a 7\u00d77 input). Fig. 7. The effect of stride in the output. However, by adding one zero-padding, the output will be 7\u00d77, which is exactly the same as the original input (The actual N now becomes 9, use the formula (1). The modified formula including zero-padding is formula (2). (2) Where P is the number of the layers of the zero-padding (e.g. P=1 in Fig. 8), This padding idea helps us to prevent network output size from shrinking with depth. Therefore, it is possible to have any number of deep convolutional networks. [2,12]. D. Feature of CNNs The weight sharing brings invariance translations to the model. It helps to filter the learn feature regardless of the spatial properties. By starting random values for the filters, they will learn to detect the edge (such as in Fig. 4) if it improves the performance. It is important to remember that if we need to know that something is spatially important in the given input, then it is an extremely bad idea to use a shared weight. Fig. 8. Zero-padding Fig. 6. Stride 1, the filter window moves only one time for each connection 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Authorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply. ICET2017, Antalya, Turkey 978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE Figure (9). Visualizing Convolutional deep neural network layers This concept can be extended to different dimensions also. For example, if it is sequential data such as an audio, then it can employ a one dimensional audio. If it is an image, as it is shown, two dimensional convolutions can be applied. And for videos, or 3D images, a three dimensional convolution can be used. This simple idea beat all the classic object recognition methods in computer vision in the 2012 ImageNet challenge as shown in Fig. 9, [5,14,18] E. Convolutional Formula The convolution for one pixel in the next layer is calculated according to the formula (3) . Where is the output in the next layer, is the input image and is the kernel or filter matrix and is the convolution operation. Fig. 10, shows how the convolution works. As can be seen, the element by element product of the input and kernel is aggregated, and then represents the corresponding point in the next layer. [4,9]. III. NONLINEARITY The next layer after the convolution is non-linearity. The non-linearity can be used to adjust or cut-off the generated output. This layer is applied in order to saturate the output or limiting the generated output. Fig. 10. Details on Convolution layer For many years, sigmoid and tanh were the most popular non-linearity. Fig. 11, shows the common types of nonlinearity. However, recently, the Rectified Linear Unit (ReLU) has been used more often for the following reasons. 1. ReLU has simpler definitions in both function and gradient. (4) (5) 2. The saturated function such as sigmoid and tanh cause problems in the back propagation. As the neural network design is deeper, the gradient signal begins to vanish, which is called the \u201cvanishing gradient\u201d. This happens since the gradient of those functions is very close to zero almost everywhere but the center. However, the ReLU has a constant gradient for the positive input. Although the function is not differentiable, it can be ignored in the actual implementation. 3. The ReLU creates a sparser representation. because the zero in the gradient leads to obtaining a complete zero. However, sigmoid and tanh always have non-zero results from the gradient, which might not be in favor for training. [2,5,13]. Fig. 11. Common types of nonlinearity. (3) Authorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply. ICET2017, Antalya, Turkey 978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE IV. POOLING The main idea of pooling is down-sampling in order to reduce the complexity for further layers. In the image processing domain, it can be considered as similar to reducing the resolution. Pooling does not affect the number of filters. Max-pooling is one of the most common types of pooling methods. It partitions the image to sub-region rectangles, and it only returns the maximum value of the inside of that sub-region. One of the most common sizes used in max-pooling is 2\u00d72. As can see in Fig. 12, when pooling is performed in the top-left 2\u00d72 blocks (pink area), it moves 2 and focus on top-right part. This means that stride 2 is used in pooling. To avoid down-sampling, stride 1 can be used, which is not common. It should be considered that down-sampling does not preserve the position of the information. Therefore, it should be applied only when the presence of information is important (rather than spatial information). Moreover, pooling can be used with non-equal filters and strides to improve the efficiency. For example, a 3x3 max-pooling with stride 2 keeps some overlaps between the areas. [5,10,16]. V. FULLY-CONNECTED LAYER The fully-connected layer is a similar to the way that neurons are arranged in a traditional neural network. Therefore, each node in a fully-connected layer is directly connected to every node in both the previous and in the next layer as shown in Fig. 13, From this figure we can note that each of the nodes in the last frames in the pooling layer are connected as a vector to the first layer from the fully-connected layer. These are the most parameters used with the CNN within these layers, and take a long time in training [3,8]. The major drawback of a fully-connected layer, is that it includes a lot of parameters that need complex computational in training examples. Therefore, we try to eliminate the number of nodes and connections. The removed nodes and connection can be satisfied by using the dropout technique. For example, LeNet and AlexNet designed a deep and wide network while keeping the computational complex constant [4,6,9]. The essence of the CNN network, which is the convolution, is when the nonlinearity and pooling layer are introduced. Most common architecture uses three of them as\" From the above paper, explain and summarize the principle of CNN in detail with equations. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "Convolutional Neural Networks (CNNs) are a class of deep neural networks highly effective for image and video recognition, amongst other uses. The foundational concept of CNNs lies in their ability to automatically and adaptively learn spatial hierarchies of features from input images. Unlike traditional neural networks that fully connect each neuron in one layer to all neurons in the next layer, CNNs utilize a more structured and efficient approach.\n\n### Convolution Operation\n\nThe core operation in a CNN is the convolution operation. It involves sliding a filter (or kernel) over the input image and computing the dot product of the filter and local regions of the image. This operation is mathematically represented as \\(O = I * K\\), where \\(O\\) is the output feature map, \\(I\\) is the input image, and \\(K\\) is the kernel. This process can significantly reduce the number of parameters in the network. For example, instead of connecting each input pixel to every neuron in the next layer, we only connect local regions (say, 5x5 pixels) to a neuron, and we reuse the same weights for each of these local connections across the entire input, reducing the parameters from potentially millions to just the size of the filter (in this case, \\(5 \\times 5 \\times \\text{Depth of the Input}\\)).\n\n### Stride and Padding\n\n- **Stride**: The stride dictates how much the filter moves across the input image. A stride of 1 means the filter moves one pixel at a time, while a stride of 2 means it moves two pixels. Adjusting the stride changes the output size, effectively reducing it as the stride increases.\n- **Padding**: Padding involves adding a border of zeros (or other values) around the input image to ensure that the convolution operation output size can be adjusted to meet the requirement (e.g., preserving the input size in the output).\n\n### Nonlinearity\n\nAfter the convolution operation, a nonlinear activation function is applied. This is usually the Rectified Linear Unit (ReLU), defined as \\(f(x) = max(0, x)\\). The introduction of nonlinearity enables the network to learn complex patterns. ReLU is preferred due to its computational efficiency and ability to help alleviate the vanishing gradient problem.\n\n### Pooling\n\nPooling is a down-sampling operation aimed at reducing the spatial dimensions (height and width) of the input volume for the next convolution layer. The most common pooling method is max pooling, which partitions the input image into a set of non-overlapping rectangles and outputs the maximum for each sub-region. This operation reduces the computational complexity and the number of parameters.\n\n### Fully-Connected Layers\n\nAfter several convolutional and pooling layers, CNN architectures typically conclude with one or more fully-connected layers. In these layers, neurons are fully connected to all activations in the previous layer, as seen in traditional Multi-Layer Perceptrons. It is in these layers where the final classification or regression decision is made based on the high-level features extracted by the convolutional and pooling layers.\n\n### Dropout\n\nTo avoid overfitting, especially in fully-connected layers that contain the majority of network parameters, dropout is often employed. Dropout randomly \"drops\" a proportion of the nodes (i.e., sets their outputs to zero) during training, which helps to prevent the network from becoming too finely tuned to the training data.\n\nIn summary, CNNs transform the input image layer by layer from raw pixel values to the final output (e.g., class scores) through a series of convolutional layers, nonlinearity activation functions, pooling layers, and fully connected layers, significantly reducing the number of parameters through weight sharing and sparse connections, thus enabling the efficient processing and analysis of images."}]}, {"facetValue": "The user asked about the principle of convolution in neural networks, providing a detailed academic text. The response involved summarizing and explaining the concept in clear, concise terms without copying the original text.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the principle of convolution in neural networks, providing a detailed academic text. The response involved summarizing and explaining the concept in clear, concise terms without copying the original text."}], "conversation": [{"role": "user", "content": "\u201cA. Convolution Let\u2019s assume that the input of our neural network has the presented shape in Fig. 2, It can be an image (e.g. color image of a CIFAR-10 dataset with a width and height of 32\u00d732 pixels, and a depth of 3 which RGB channel) or a video (gray scale video whose height and width are the resolution, and the depth are the frames) or even an experimental video, which has width and height of (L \u00d7 L) sensor values, and the depths are associated with different time frames, as in [2,10,15]. Why convolution ? . Let\u2019s assume that the network receives raw pixels as input. Therefore, to connect the input layer to only one neuron (e.g. in the hidden layer in the Multi- Layer perceptron), as an example, there should be 32\u00d732\u00d73 weight connections for the CIFAR-10 dataset. Fig. 1. Learned features from a Convolutional Neural Network Authorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply. ICET2017, Antalya, Turkey 978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE Fig. 2. Three dimensional Input representation of CNN If we add one more neuron into the hidden layer, then we will need another 32\u00d732\u00d73 weight connection, which will become in total, 32\u00d732\u00d73\u00d72 parameters. To make it clearer, more than 6000 weight parameters are used to connect the input to just only two nodes. It may be thought that two neurons might not be enough for any useful processing for an image classification application. To make it more efficient, we can connect the input image to the neurons in the next layer with exactly the same values for the height and width. It can be assumed this network is applied for the type of processing such as the edge in the image. However, the mentioned network needs 32\u00d732\u00d73 by 32\u00d732 weight connections, which are (3,145,728), as in [4,5,14]. Therefore, looking for a more efficient method, it emerged that instead of a full connection, it is a good idea to look for local regions in the picture instead of in the whole image. Fig. 3, shows a regional connection for the next layer. In other words, the hidden neurons in the next layer only get inputs from the corresponding part of the previous layer. For example, it can only be connected to 5\u00d75 neurons. Thus, if we want to have 32\u00d732 neurons in the next layer, then we will have 5\u00d75\u00d73 by 32x32 connections which is 76,800 connections (compared to 3,145,728 for full connectivity), as in [1,9,13,14,17]. Although the size of connection drastically dropped, it still leaves so many parameters to solve. Another assumption for simplification, is to keep the local connection weights fixed for the entire neurons of the next layer. This will connect the neighbor neurons in the next layer with exactly the same weight to the local region of the previous layer. Therefore, it again drops many extra parameters, and reduces the number of weights to only 5\u00d75\u00d73=75 to connect 32\u00d732\u00d73 neurons to 32\u00d732 in the next layer [5,8,11]. There are many benefits to these simple assumptions. Firstly, the number of connections decrease from around 3 million to only 75 connections in the presented example. Secondly, and a more interesting concept, is that fixing the weights for the local connections is similar to sliding a window of 5\u00d75\u00d73 in the input neurons and mapping the generated output to the corresponding place. It provides an opportunity to detect and recognize features regardless of their positions in the image. This is the reason why they are called convolutions [6,7,16]. Fig. 3. Convolution as alternative for fully connected network. To show the astounding effect of the convolution matrix, Fig.4, depicts what will happen if we manually pick the connection weight in a 3\u00d73 window. As we can see Fig.4, the matrix can be set to detect edges in the image. These matrices are also called a filter because they act like the classic filters in the image processing. However, in the convolutional neural network these filters are initialized, followed by the training procedure shape filters, which are more suitable for the given task. To make this method more beneficial, it is possible to add more layers after the input layer. Each layer can be associated with different filters. Therefore, we can extract different features from the given image. Fig. 5, shows how they are connected to the different layers. Each layer has its own filter and therefore extracts different features from the input. The neurons shown in Fig. 5, use a different filter, but look at the same part of the input image. [6,8,15,17]\u201d From the above paper, explain and summarize the principle of Convolution in detail. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "The principle of convolution in the context of neural networks, particularly Convolutional Neural Networks (CNNs), is an efficient and powerful way of processing images, videos, or any data that has a spatial structure. The core idea behind convolution is to reduce the complexity and computational cost associated with fully connected layers, where every input unit is connected to every unit in the next layer, which can result in an enormous number of parameters to learn.\n\nFor example, given an image from the CIFAR-10 dataset, which is a 32x32 pixels color image with three channels (RGB), connecting the image to just one neuron in a fully connected manner would require 32x32x3 (3072) weight connections. This number rises linearly with more neurons added to the layer, quickly leading to a vast quantity of parameters even for small networks.\n\nThe convolution approach simplifies this by applying two key concepts:\n1. **Local connectivity**: Instead of connecting each input unit to every neuron in the next layer, each neuron is only connected to a local, spatially contiguous patch of units in the input. This is based on the observation that in tasks like image processing, local groups of inputs (pixels) often hold the most immediate relevance to each other. For instance, to detect an edge in an image, only a small window of pixels around the edge is relevant at any one moment.\n\n2. **Shared weights (parameter sharing)**: Instead of learning a unique set of weights for every such local connection, the same set of weights (a filter or kernel) is applied across all locations of the input. This drastically reduces the number of free parameters. For a 5x5 filter operating over a 32x32x3 image, instead of needing a separate 5x5x3 set of weights for each possible position in the input, we only need a single 5x5x3 set of weights, totaling 75 weights. This filter is slid or \"convolved\" across the input image, processing local regions one at a time, and producing a feature map that highlights specific features detected by that filter, such as edges.\n\nThe convolution operation enables networks to learn filters that autonomously discover useful features in the input with far fewer parameters, thereby facilitating deeper and more efficient networks. Each filter in a convolutional layer can detect a specific feature, and as the input passes through successive layers, more complex features can be detected. Multiple filters can be applied in parallel to detect various features at each level of the network.\n\nOne of the significant advantages of this approach is translational invariance: once a filter has learned to recognize a feature, it can detect that feature anywhere in the input image. This is particularly useful for tasks such as image recognition, where the position of an object in an image should not affect the network's ability to recognize the object.\n\nIn summary, convolution leverages local connectivity and shared weights to efficiently process spatial data, significantly reducing the number of parameters required and enabling the network to learn to detect and compose hierarchical features from the data, which is key for tasks like image and video recognition."}]}, {"facetValue": "The user asked about the principle of convolution in neural networks, providing a detailed academic text. The response involved explaining the concept with equations and principles, focusing on local connectivity, weight sharing, and feature detection.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the principle of convolution in neural networks, providing a detailed academic text. The response involved explaining the concept with equations and principles, focusing on local connectivity, weight sharing, and feature detection."}], "conversation": [{"role": "user", "content": "\u201cA. Convolution Let\u2019s assume that the input of our neural network has the presented shape in Fig. 2, It can be an image (e.g. color image of a CIFAR-10 dataset with a width and height of 32\u00d732 pixels, and a depth of 3 which RGB channel) or a video (gray scale video whose height and width are the resolution, and the depth are the frames) or even an experimental video, which has width and height of (L \u00d7 L) sensor values, and the depths are associated with different time frames, as in [2,10,15]. Why convolution ? . Let\u2019s assume that the network receives raw pixels as input. Therefore, to connect the input layer to only one neuron (e.g. in the hidden layer in the Multi- Layer perceptron), as an example, there should be 32\u00d732\u00d73 weight connections for the CIFAR-10 dataset. Fig. 1. Learned features from a Convolutional Neural Network Authorized licensed use limited to: Hong Kong Polytechnic University. Downloaded on April 08,2024 at 07:52:10 UTC from IEEE Xplore. Restrictions apply. ICET2017, Antalya, Turkey 978-1-5386-1949-0/17/$31.00 \u00a92017 IEEE Fig. 2. Three dimensional Input representation of CNN If we add one more neuron into the hidden layer, then we will need another 32\u00d732\u00d73 weight connection, which will become in total, 32\u00d732\u00d73\u00d72 parameters. To make it clearer, more than 6000 weight parameters are used to connect the input to just only two nodes. It may be thought that two neurons might not be enough for any useful processing for an image classification application. To make it more efficient, we can connect the input image to the neurons in the next layer with exactly the same values for the height and width. It can be assumed this network is applied for the type of processing such as the edge in the image. However, the mentioned network needs 32\u00d732\u00d73 by 32\u00d732 weight connections, which are (3,145,728), as in [4,5,14]. Therefore, looking for a more efficient method, it emerged that instead of a full connection, it is a good idea to look for local regions in the picture instead of in the whole image. Fig. 3, shows a regional connection for the next layer. In other words, the hidden neurons in the next layer only get inputs from the corresponding part of the previous layer. For example, it can only be connected to 5\u00d75 neurons. Thus, if we want to have 32\u00d732 neurons in the next layer, then we will have 5\u00d75\u00d73 by 32x32 connections which is 76,800 connections (compared to 3,145,728 for full connectivity), as in [1,9,13,14,17]. Although the size of connection drastically dropped, it still leaves so many parameters to solve. Another assumption for simplification, is to keep the local connection weights fixed for the entire neurons of the next layer. This will connect the neighbor neurons in the next layer with exactly the same weight to the local region of the previous layer. Therefore, it again drops many extra parameters, and reduces the number of weights to only 5\u00d75\u00d73=75 to connect 32\u00d732\u00d73 neurons to 32\u00d732 in the next layer [5,8,11]. There are many benefits to these simple assumptions. Firstly, the number of connections decrease from around 3 million to only 75 connections in the presented example. Secondly, and a more interesting concept, is that fixing the weights for the local connections is similar to sliding a window of 5\u00d75\u00d73 in the input neurons and mapping the generated output to the corresponding place. It provides an opportunity to detect and recognize features regardless of their positions in the image. This is the reason why they are called convolutions [6,7,16]. Fig. 3. Convolution as alternative for fully connected network. To show the astounding effect of the convolution matrix, Fig.4, depicts what will happen if we manually pick the connection weight in a 3\u00d73 window. As we can see Fig.4, the matrix can be set to detect edges in the image. These matrices are also called a filter because they act like the classic filters in the image processing. However, in the convolutional neural network these filters are initialized, followed by the training procedure shape filters, which are more suitable for the given task. To make this method more beneficial, it is possible to add more layers after the input layer. Each layer can be associated with different filters. Therefore, we can extract different features from the given image. Fig. 5, shows how they are connected to the different layers. Each layer has its own filter and therefore extracts different features from the input. The neurons shown in Fig. 5, use a different filter, but look at the same part of the input image. [6,8,15,17]\u201d From the above paper, explain and summarize the principle of Convolution with equations in detail. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "The principle of convolution in the context of neural networks, particularly Convolutional Neural Networks (CNNs), revolves around the efficient processing of input data, such as images, to recognize patterns like edges, textures, or objects. The convolution operation significantly reduces the number of parameters required to learn from the input data by employing the following strategies:\n\n### 1. **Local Connectivity:**\n\nInstead of connecting every input pixel to every neuron in the next layer (which in fully connected layers leads to a massive number of parameters), the convolution operation connects each neuron to only a small, localized region of the input image. This localized region can be defined by a window or filter of size \\(F \\times F\\), where \\(F\\) is smaller than the dimension of the input image. For a \\(W \\times H \\times D\\) input (Width, Height, Depth), a kernel size of \\(F \\times F\\) means that each neuron in the convolutional layer looks at only \\(F \\times F \\times D\\) inputs.\n\n### 2. **Weight Sharing:**\n\nIn traditional neural networks, each connection between layers has a unique weight, leading to a large number of parameters. However, in CNNs, all connections associated with a single filter share the same weights. This is analogous to sliding the filter across the entire input space and applying the same transformation everywhere. This dramatically reduces the number of unique parameters.\n\n### 3. **Equation and Operation:**\n\nThe convolution operation involves sliding the filter over the input image. At each position, the filter performs an element-wise multiplication with the part of the image it covers, and the results are summed up to produce a single output pixel in the feature map. The mathematical expression for this operation (ignoring bias for simplicity) for a given position \\((x, y)\\) is:\n\n\\[O(x, y) = \\sum_{i=0}^{F-1}\\sum_{j=0}^{F-1}\\sum_{k=0}^{D-1} I(x+i, y+j, k) \\times W(i, j, k)\\]\n\nwhere \\(O(x, y)\\) is the output value at position \\((x, y)\\), \\(I(x+i, y+j, k)\\) represents the input value at the covered position with depth \\(k\\), and \\(W(i, j, k)\\) is the weight of the filter at \\((i, j, k)\\).\n\n### 4. **Feature Detection Capability:**\n\nBy using the same set of weights across the entire input, CNNs can detect features anywhere in the image. This is analogous to having a specialized tool (the filter) for finding specific patterns (like edges or corners) no matter where they appear in the input space. \n\n### 5. **Multiple Filters for Complex Feature Extraction:**\n\nCNNs usually employ multiple filters in each convolutional layer, allowing each filter to specialize in detecting different features. By stacking multiple convolutional layers, with each layer capable of recognizing more complex patterns built on the simpler features detected by preceding layers, CNNs can efficiently process and understand complex input data like images or videos.\n\nTo summarize, the convolution operation in CNNs reduces the model's complexity and enhances its ability to recognize patterns across the input data by using local connectivity and weight sharing, which leads to a significant reduction in the number of parameters and the model's capacity to detect features irrespective of their spatial location in the input."}]}, {"facetValue": "The user asked about the principle of stride in CNNs and provided a detailed explanation from a paper. The response involved summarizing the concept, mentioning figures, and explaining the mathematical formula.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the principle of stride in CNNs and provided a detailed explanation from a paper. The response involved summarizing the concept, mentioning figures, and explaining the mathematical formula."}], "conversation": [{"role": "user", "content": "\"B. Stride\nIn fact, CNN has more options which provide a lot of opportunities to even decrease the parameters more and more, and at the same time reduce some of the side effects. One of these options is stride. In the above mentioned example, it is simply assumed that the next layer's node has lots of overlaps with their neighbors by looking at the regions. We can manipulate the overlap by controlling the stride. Fig. 6, shows a given 7\u00d77 image. If we move the filter one node every time, we can have a 5\u00d75 output only. Note that the output of the three left matrices in Fig. 6, have an overlap (and three middle ones together and three right ones also). However, if we move and make every stride 2, then the output will be 3\u00d73. Put simply, not only overlap, but also the size of the output will be reduced. [5,12,16].\n\nFig. 6. - Stride 1, the filter window moves only one time for each connection\n\nEquation (1), formalize this, given the image N\u00d7N dimension and the filter size of the F\u00d7F, the output size O as shown in Fig. 7.\nO=1+N\u2212FS(1)\nView SourceRight-click on figure for MathML and additional features.Where N is the input size, F is the filter size, and S is the stride size.\"\nFrom the above paper, explain and summarize the principle of stride of CNN in detail.Mention the Figures. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "The principle of stride in Convolutional Neural Networks (CNN) is an essential concept that deals with how the network's filters (or kernels) scan across the input data, such as an image, to produce a transformed output layer. In simpler terms, stride refers to the step size the filter takes while moving across the input image. By adjusting the stride value, we can control how much the filter windows overlap each other and, in turn, influence the size of the output feature map.\n\nLet's consider an example to understand stride using an imaginary 7\u00d77 pixel image. When we apply a filter to this image to extract features, the way the filter moves across the image is critical. With a stride of 1, the filter moves one pixel at a time. As it scans the image, it systematically overlaps with the previously covered areas, ensuring no information is missed but producing a relatively larger output. For instance, if we use a 3x3 filter on our 7x7 image with a stride of 1, we'll end up with a 5x5 feature map because the filter moves across and down the image one pixel at a time, closely inspecting nearly every part of the image.\n\nHowever, if we change the stride to 2, the filter now skips every other pixel as it moves. This larger step size reduces overlap between the filter's positions and significantly decreases the size of the output feature map. In the same example of a 7x7 image, applying a 3x3 filter with a stride of 2 will produce a smaller 3x3 feature map. This is because, with fewer positions for the filter to occupy (due to larger steps), there's a drastic reduction in the output size.\n\nThe stride concept is mathematically represented by an equation that helps calculate the size of the output feature map based on the size of the input (N\u00d7N), the filter size (F\u00d7F), and the stride (S). The formula is as follows: O = 1 + (N\u2212F)/S, where 'O' stands for the output size. This equation demonstrates how varying the stride affects the dimensions of the output layer, offering a practical way to manage the network's computational load and the resolution of the feature maps it generates.\n\nIn summary, the stride in a CNN allows for flexible control over how densely the filters analyze the input and the size of the resultant feature maps. By adjusting the stride, one can find a balance between capturing detailed information (with smaller strides) and reducing computational requirements or achieving more abstract representations (with larger strides)."}]}, {"facetValue": "The user asked about the mathematical explanation of a convolutional formula in a CNN. The user requested a detailed summary of the formula using their own words without copying from the provided text.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the mathematical explanation of a convolutional formula in a CNN. The user requested a detailed summary of the formula using their own words without copying from the provided text."}], "conversation": [{"role": "user", "content": "\u201cE. Convolutional Formula\nThe convolution for one pixel in the next layer is calculated according to the formula (3).\n\n\nWhere is the output in the next layer, x is the input image and w is the kernel or filter matrix and * is the convolution operation. Fig. 10, shows how the convolution works. As can be seen, the element by element product of the input and kernel is aggregated, and then represents the corresponding point in the next layer. [4], [9].\n\n\u201d\nFrom the above paper, explain and summarize the formula mathematically of CNN in detail. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "In the context of Convolutional Neural Networks (CNNs), the process of convolution plays a central role in transforming input data (such as images) into a form that can be used to extract features, which are crucial for tasks like image classification, object detection, etc. The convolution operation, as described in the excerpt, involves a mathematical formula that we'll break down and summarize here.\n\nAt its core, the convolution process in a CNN involves taking an input image (denoted as \\(x\\)) and applying a filter or kernel (denoted as \\(w\\)) to it to produce an output (let's call it \\(o\\)) for the next layer. The kernel \\(w\\) is essentially a small matrix of weights that slides (convolves) across the input image to perform element-wise multiplications followed by an aggregation (typically a sum) of these products.\n\nThe step-by-step process is as follows:\n\n1. **Overlay the Kernel:** Position the kernel over a corresponding location on the input image. Initially, this is usually the top-left corner.\n\n2. **Element-wise Multiplication:** Multiply each element of the kernel with the corresponding element of the image it covers.\n\n3. **Aggregation:** Sum up all the results of these multiplications. This aggregated sum represents a single element in the output matrix (referred to as the feature map or convolved feature).\n\n4. **Sliding:** The kernel then slides across the entire image. The amount by which the kernel moves is determined by what is called the stride. A stride of 1 means the kernel moves one pixel at a time, a stride of 2 means two pixels, and so on.\n\n5. **Repeat:** Steps 1-4 are repeated for every position the kernel can fit over the input image. This process results in a feature map that represents a distilled information version of the input, emphasizing features that match the pattern of the kernel.\n\nMathematically, if we were to write out a basic formula for this operation (without considering edge cases, padding, or stride for simplicity), it would look something like:\n\n\\[\no(i,j) = \\sum_{m}\\sum_{n} x(m,n) \\cdot w(i-m,j-n)\n\\]\n\nwhere:\n- \\(o(i,j)\\) is the value of the output at position \\((i,j)\\),\n- \\(x(m,n)\\) represents the elements of the input image,\n- \\(w\\) is the kernel, and\n- the sums run over the dimensions of the kernel.\n\nIn practice, this convolution operation allows CNNs to learn increasingly complex patterns as the input data passes through successive layers, each with its own kernels that specialize in detecting specific features from simple edges in the initial layers to more complex patterns in deeper layers."}]}, {"facetValue": "The user asked about the concept of pooling in CNNs and provided a detailed explanation. The response focused on summarizing the key points of the conversation without copying directly from the text.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the concept of pooling in CNNs and provided a detailed explanation. The response focused on summarizing the key points of the conversation without copying directly from the text."}], "conversation": [{"role": "user", "content": "\"Pooling\nThe main idea of pooling is down-sampling in order to reduce the complexity for further layers. In the image processing domain, it can be considered as similar to reducing the resolution. Pooling does not affect the number of filters. Max-pooling is one of the most common types of pooling methods. It partitions the image to sub-region rectangles, and it only returns the maximum value of the inside of that sub-region. One of the most common sizes used in max-pooling is 2\u00d72. As can see in Fig. 12, when pooling is performed in the top-left 2\u00d72 blocks (pink area), it moves 2 and focus on top-right part. This means that stride 2 is used in pooling. To avoid down-sampling, stride 1 can be used, which is not common. It should be considered that down-sampling does not preserve the position of the information. Therefore, it should be applied only when the presence of information is important (rather than spatial information). Moreover, pooling can be used with non-equal filters and strides to improve the efficiency. For example, a 3\u00d73 max-pooling with stride 2 keeps some overlaps between the areas. [5,10,16].\n\nFig. 12. - Max-pooling is demonstrated. The max-pooling with 2\u00d72 filter and stride 2 lead to down-sampling of each 2\u00d72 blocks is mapped to 1 block (pixel).\"\nFrom the above paper, explain and summarize the pooling of CNN with the figures mentioned. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "Pooling in Convolutional Neural Networks (CNNs) is a technique used to reduce the spatial dimensions of an image, thereby simplifying the amount of information that future layers need to process. It contributes to making the computation more manageable and focuses on the most relevant features within a region of the image. This process can be likened to decreasing the image resolution without significantly losing critical information.\n\nAmong the various types of pooling, max-pooling is notably widespread. This method involves dividing the image into several rectangular regions or sub-regions and then selecting the maximum value from each of these areas to represent them. A common practice within max-pooling is to use a 2\u00d72 filter size. This means the image is split into squares of 2 pixels by 2 pixels, and from each square, the highest pixel value is taken to create a new, down-sampled image. \n\nAn essential aspect of pooling is the concept of stride, which refers to how far the pooling filter moves across the image. In a scenario with a stride of 2, as often used with a 2\u00d72 filter, the filter skips every other pixel horizontally and vertically, significantly reducing the image size. This is contrasted with a stride of 1, where the filter moves one pixel at a time, leading to less down-sampling and a larger output size. Stride 1 is less common in pooling operations since the main goal of pooling is to reduce dimensionality.\n\nIt's worth noting that pooling, especially down-sampling, can cause some loss of positional information. This is because, in prioritizing the presence of certain features (like edges or textures) over their exact locations, pooling overlooks the precise arrangement of pixels within each filtered region. However, this is typically acceptable in many applications because capturing the essence of features can be more crucial than knowing their exact positions.\n\nAdditionally, pooling operations can be varied in terms of filter sizes and strides to achieve different degrees of down-sampling and efficiency. For instance, using a 3\u00d73 filter with a stride of 2 allows for a bit of overlap between sequential pooling operations, capturing a broader array of information while still reducing the input size.\n\nIn summary, pooling in CNNs is a crucial step for managing computational resources and focusing on the most meaningful elements of the input data. Max-pooling, with its straightforward approach of selecting maximum values within predefined blocks, serves as a key method for efficiently condensing an image's information content while maintaining its core characteristics."}]}, {"facetValue": "The user asked about the concept of pooling in CNNs and provided a detailed explanation. The response involved summarizing the technical content while ensuring clarity and avoiding direct copying.", "allFacetValues": [{"facet": "Journal", "value": "The user asked about the concept of pooling in CNNs and provided a detailed explanation. The response involved summarizing the technical content while ensuring clarity and avoiding direct copying."}], "conversation": [{"role": "user", "content": "\u201cPooling\nThe main idea of pooling is down-sampling in order to reduce the complexity for further layers. In the image processing domain, it can be considered as similar to reducing the resolution. Pooling does not affect the number of filters. Max-pooling is one of the most common types of pooling methods. It partitions the image to sub-region rectangles, and it only returns the maximum value of the inside of that sub-region. One of the most common sizes used in max-pooling is 2\u00d72. As can see in Fig. 12, when pooling is performed in the top-left 2\u00d72 blocks (pink area), it moves 2 and focus on top-right part. This means that stride 2 is used in pooling. To avoid down-sampling, stride 1 can be used, which is not common. It should be considered that down-sampling does not preserve the position of the information. Therefore, it should be applied only when the presence of information is important (rather than spatial information). Moreover, pooling can be used with non-equal filters and strides to improve the efficiency. For example, a 3\u00d73 max-pooling with stride 2 keeps some overlaps between the areas. [5,10,16].\n\nFig. 12. - Max-pooling is demonstrated. The max-pooling with 2\u00d72 filter and stride 2 lead to down-sampling of each 2\u00d72 blocks is mapped to 1 block (pixel).\u201d\nFrom the above paper, Summarize the explanation of pooling of CNN with the figures mentioned. Try to use your own words, do not directly copy from the above sentences."}, {"role": "assistant", "content": "Pooling in Convolutional Neural Networks (CNNs) is a technique aimed at reducing the size or dimensionality of input data, which in turn minimizes computational complexity for layers that follow. This process can be likened to lowering the resolution of an image, where the overall structure is preserved but details may be lost. Despite the reduction in data size, the number of filters\u2014or feature detectors\u2014used in the network remains unchanged by pooling.\n\nA particularly widespread method of pooling is max-pooling. This technique involves dividing an image into smaller rectangular areas and selecting only the highest value from each area to represent that segment. A commonly used dimension for these segments is 2x2. An illustrative example is taking a 2x2 portion of an image (considered as a pink area for visualization) and simplifying it down to a single value that reflects the maximum pixel intensity found within that segment. Max-pooling often utilizes a \"stride\" of 2, meaning it moves two spaces over to select the next area, thereby reducing the image's dimensions by half in both width and height. While it is possible to use a stride of 1 to prevent any loss in image size, this approach is less common and negates the simplifying advantages of pooling.\n\nOne significant consideration with down-sampling through pooling is the trade-off between preserving the existence of features over their precise locations. Since pooling, especially with a stride greater than 1, doesn't maintain the exact positioning of features within an image, it's best applied in contexts where detecting the presence of certain characteristics matters more than knowing their exact spatial relations.\n\nTo enhance the efficiency of feature detection, variations in the size of the filters and strides can be utilized in pooling. For instance, employing a 3x3 filter with a stride of 2 allows for a slight overlap between the areas being condensed. This slight overlap can assist in maintaining a balance between reducing complexity and preserving useful information within the image."}]}]}], "numConvs": 19}], "numConvs": 19}